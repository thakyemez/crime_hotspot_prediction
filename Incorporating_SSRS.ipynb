{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thakyemez/crime_hotspot_prediction/blob/main/Incorporating_SSRS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OBb6UhJSovE",
        "outputId": "6dfbebe9-752a-4839-864b-9ac25fa6d3ad"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXSe_QlEqOzN"
      },
      "source": [
        "**PREPARING THE DATASETS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD2Jl1Kg4xdb"
      },
      "source": [
        "**Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k-fdL7E43Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95020abd-0b07-4e37-93f9-4f9de412f02a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "import torch.nn as nn\n",
        "from torch.nn import BatchNorm2d, Conv1d, Conv2d, ModuleList, Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from fastprogress import progress_bar\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install durbango\n",
        "!pip install funcy\n",
        "!pip install py3nvml\n",
        "from durbango import pickle_save\n",
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting durbango\n",
            "  Downloading durbango-0.23.tar.gz (27 kB)\n",
            "Building wheels for collected packages: durbango\n",
            "  Building wheel for durbango (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for durbango: filename=durbango-0.23-py3-none-any.whl size=31553 sha256=4df290b41cda002885912c3847ca1233ee7d70f1b4d2c67acc59e140c3996abe\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/ab/8e/d7b59feb6ed6eaa5320c3e05700ec20fc62e0c04fddda07501\n",
            "Successfully built durbango\n",
            "Installing collected packages: durbango\n",
            "Successfully installed durbango-0.23\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Installing collected packages: funcy\n",
            "Successfully installed funcy-1.17\n",
            "Collecting py3nvml\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: xmltodict, py3nvml\n",
            "Successfully installed py3nvml-0.2.7 xmltodict-0.12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/durbango/pd_utils.py:12: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  from pandas.util.testing import assert_frame_equal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geopandas\n",
            "  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 14.4 MB/s \n",
            "\u001b[?25hCollecting fiona>=1.8\n",
            "  Downloading Fiona-1.8.20-cp37-cp37m-manylinux1_x86_64.whl (15.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 85.2 MB/s \n",
            "\u001b[?25hCollecting pyproj>=2.2.0\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
            "Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.20 geopandas-0.10.2 munch-2.5.0 pyproj-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlooOLin41FN"
      },
      "source": [
        "**Preparing the Crime Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqjHnOUJc6RH"
      },
      "source": [
        "##Theft\n",
        "TheftTable = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/Transactions/ CenterTheftTable.csv')\n",
        "a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "TheftTable.index=a\n",
        "#TheftTable=TheftTable.drop(columns=['Time_ID'])\n",
        "TheftTable=TheftTable.drop(columns=['Unnamed: 0'])\n",
        "TheftTable_Smoothed=TheftTable.ewm(alpha=0.1).mean()\n",
        "##Aggregating at day level\n",
        "TheftTable_daily = TheftTable.resample('D').sum()\n",
        "TheftTable_daily.sum()\n",
        "TheftTable_daily.head()\n",
        "TheftTable_daily.to_csv('/content/gdrive/MyDrive/transactions.csv')\n",
        "TheftTable_daily_Smoothed=TheftTable_daily.ewm(alpha=0.05).mean()\n",
        "TheftTable_daily_Smoothed.head()\n",
        "##Robbery\n",
        "RobberyTable = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/Transactions/ CenterRobberyTable.csv')\n",
        "a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "RobberyTable.index=a\n",
        "RobberyTable=RobberyTable.drop(columns=['Time_ID'])\n",
        "RobberyTable=RobberyTable.drop(columns=['Unnamed: 0'])\n",
        "RobberyTable_Smoothed=RobberyTable.ewm(alpha=0.1).mean()\n",
        "##Aggregating at day level\n",
        "RobberyTable_daily = RobberyTable.resample('D').sum()\n",
        "RobberyTable_daily.sum()\n",
        "RobberyTable_daily.head()\n",
        "RobberyTable_daily.to_csv('/content/gdrive/MyDrive/transactions.csv')\n",
        "RobberyTable_daily_Smoothed=RobberyTable_daily.ewm(alpha=0.05).mean()\n",
        "RobberyTable_daily_Smoothed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-YYexBA1PSK"
      },
      "source": [
        "**Preparing the adjancency matrices for CS network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l5mxtONwpn7"
      },
      "source": [
        "##Preparing the Center graph\n",
        "import glob\n",
        "paths=(\"/content/gdrive/MyDrive/MainFiles/CenterSide/Graphs\")\n",
        "all_files = glob.glob(paths + \"/*.pickle\")\n",
        "for m in all_files:\n",
        "  Cgraph=pd.read_pickle(m)\n",
        "  nodescaC=np.array(TheftTable_daily_Smoothed.columns)\n",
        "  nodescaC_to_ind=np.arange(0,2459,1)\n",
        "  ntemp=nx.convert_matrix.to_numpy_matrix(Cgraph,nodelist=nodescaC,weight='weight')\n",
        "  name=str(m.split('/')[-1]).split('.')[0]\n",
        "  ext= '.pkl'\n",
        "  with open(\"/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix/%s%s\" %(name,ext),'wb') as f:\n",
        "    pickle.dump([nodescaC,nodescaC_to_ind,ntemp], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxjeDtybqdxG"
      },
      "source": [
        "**GENERATE TRAINING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ7wkNAID_A7",
        "outputId": "c3483b4f-8b9d-4df2-b65e-94927b1a8710"
      },
      "source": [
        "def generate_graph_seq2seq_io_data(\n",
        "        df, x_offsets, y_offsets, add_time_in_day=True, add_day_in_week=False, scaler=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate samples from\n",
        "    :param df:\n",
        "    :param x_offsets:\n",
        "    :param y_offsets:\n",
        "    :param add_time_in_day:\n",
        "    :param add_day_in_week:\n",
        "    :param scaler:\n",
        "    :return:\n",
        "    # x: (epoch_size, input_length, num_nodes, input_dim)\n",
        "    # y: (epoch_size, output_length, num_nodes, output_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples, num_nodes = df.shape\n",
        "    data = np.expand_dims(df.values, axis=-1)\n",
        "    print(\"shape_of_data: \", data.shape)\n",
        "    feature_list = [data]\n",
        "\n",
        "    if add_time_in_day:\n",
        "        time_ind = (df.index.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
        "        time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "        feature_list.append(time_in_day)\n",
        "    if add_day_in_week:\n",
        "        dow = df.index.dayofweek\n",
        "        #Trying Weekend/Weekday distinction\n",
        "        dow2=dow.copy()\n",
        "        dow2=[1 if (x==5 or x==6) else 0  for x in dow2]\n",
        "        dow=np.array(dow2)\n",
        "        dow_tiled = np.tile(dow, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "        feature_list.append(dow_tiled)\n",
        "\n",
        "    RobberyRSIS=RobberyRSIS_daily.values.reshape(num_samples, num_nodes,1)\n",
        "    feature_list.append(RobberyRSIS)\n",
        "    #Adding Normalized SSRS as an external variable\n",
        "    RobberyRSIS=RobberyRSISS_normalized.values.reshape(num_samples, num_nodes,1)\n",
        "    feature_list.append(RobberyRSIS)\n",
        "    feature_list.append(RSISRobbery)\n",
        "    data = np.concatenate(feature_list, axis=-1)\n",
        "    print(\"shape_of_data: \", data.shape)\n",
        "    x, y = [], []\n",
        "    min_t = abs(min(x_offsets))\n",
        "    print(\"min_t \", min_t)\n",
        "    max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
        "    print(\"max_t\", max_t)\n",
        "    for t in range(min_t, max_t):  # t is the index of the last observation. The loop iterates between 11 and 2180.\n",
        "        x.append(data[t + x_offsets, ...])\n",
        "        y.append(data[t + y_offsets, ...])\n",
        "    print(\"x before stack \", x[0].shape)\n",
        "    x = np.stack(x, axis=0)\n",
        "    print(\"shape of x \", x.shape)\n",
        "    print(\"y before stack \", y[0].shape)\n",
        "    y = np.stack(y, axis=0)\n",
        "    print(\"shape of y \", y.shape)\n",
        "    return x, y\n",
        "\n",
        "def generate_train_val_test(args):\n",
        "    seq_length_x, seq_length_y = args.seq_length_x, args.seq_length_y\n",
        "    df = args.traffic_df_filename\n",
        "    # 0 is the latest observed sample.\n",
        "    x_offsets = np.sort(np.concatenate((np.arange(-(seq_length_x - 1), 1, 1),)))\n",
        "    print(\"shape of x_offsets \", x_offsets.shape)\n",
        "    print(\"x_offsets \",x_offsets)\n",
        "    # Predict the next one hour\n",
        "    y_offsets = np.sort(np.arange(args.y_start, (seq_length_y + 1), 1))\n",
        "    print(\"shape of y_offsets \", y_offsets.shape)\n",
        "    print(\"y_offsets \",y_offsets)\n",
        "    # x: (num_samples, input_length, num_nodes, input_dim)\n",
        "    # y: (num_samples, output_length, num_nodes, output_dim)\n",
        "    x, y = generate_graph_seq2seq_io_data(\n",
        "        df,\n",
        "        x_offsets=x_offsets,\n",
        "        y_offsets=y_offsets,\n",
        "        add_time_in_day=False,\n",
        "        add_day_in_week=args.dow,\n",
        "    )\n",
        "    print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
        "    # Write the data into npz file.\n",
        "    num_samples = x.shape[0]\n",
        "    num_test = round(num_samples * 0.2)\n",
        "    num_train = round(num_samples * 0.7)\n",
        "    num_val = num_samples - num_test - num_train\n",
        "    x_train, y_train = x[:num_train], y[:num_train]\n",
        "    x_val, y_val = (\n",
        "        x[num_train: num_train + num_val],\n",
        "        y[num_train: num_train + num_val],\n",
        "    )\n",
        "    x_test, y_test = x[-num_test:], y[-num_test:]\n",
        "\n",
        "    for cat in [\"train\", \"val\", \"test\"]:\n",
        "        _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
        "        print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
        "        np.savez_compressed(\n",
        "            os.path.join(args.output_dir, f\"{cat}.npz\"),\n",
        "            x=_x,\n",
        "            y=_y,\n",
        "            x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
        "            y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
        "        )\n",
        "    print(\"shape of x_offsets \", x_offsets.shape)\n",
        "    print(\"shape of y_offsets \", y_offsets.shape)\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"/content/gdrive/MyDrive/Robbery_Daily_data/\", help=\"Output directory.\")\n",
        "    parser.add_argument(\"--traffic_df_filename\", type=str, default=RobberyTable_daily_Smoothed, help=\" Daily crime risk scores\",)\n",
        "    parser.add_argument(\"--seq_length_x\", type=int, default=42, help=\"Sequence Length.\",)\n",
        "    parser.add_argument(\"--seq_length_y\", type=int, default=1, help=\"Sequence Length.\",)\n",
        "    parser.add_argument(\"--y_start\", type=int, default=1, help=\"Y pred start\", )\n",
        "    parser.add_argument(\"--dow\", action='store_true',)\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    if os.path.exists(args.output_dir):\n",
        "        reply = str(input(f'{args.output_dir} exists. Do you want to overwrite it? (y/n)')).lower().strip()\n",
        "        if reply[0] != 'y': exit\n",
        "    else:\n",
        "        os.makedirs(args.output_dir)\n",
        "    generate_train_val_test(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Robbery_Daily_data/ exists. Do you want to overwrite it? (y/n)y\n",
            "shape of x_offsets  (42,)\n",
            "x_offsets  [-41 -40 -39 -38 -37 -36 -35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25 -24\n",
            " -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10  -9  -8  -7  -6\n",
            "  -5  -4  -3  -2  -1   0]\n",
            "shape of y_offsets  (1,)\n",
            "y_offsets  [1]\n",
            "shape_of_data:  (731, 2459, 1)\n",
            "shape_of_data:  (731, 2459, 1)\n",
            "min_t  41\n",
            "max_t 730\n",
            "x before stack  (42, 2459, 1)\n",
            "shape of x  (689, 42, 2459, 1)\n",
            "y before stack  (1, 2459, 1)\n",
            "shape of y  (689, 1, 2459, 1)\n",
            "x shape:  (689, 42, 2459, 1) , y shape:  (689, 1, 2459, 1)\n",
            "train x:  (482, 42, 2459, 1) y: (482, 1, 2459, 1)\n",
            "val x:  (69, 42, 2459, 1) y: (69, 1, 2459, 1)\n",
            "test x:  (138, 42, 2459, 1) y: (138, 1, 2459, 1)\n",
            "shape of x_offsets  (42,)\n",
            "shape of y_offsets  (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgw10fGuDvS"
      },
      "source": [
        "**TRAINING TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "w88fsxvYuIWK",
        "outputId": "443c90fe-bec6-4ee4-956a-bf666df3c3f3"
      },
      "source": [
        "hdir=\"/content/gdrive/MyDrive/\"\n",
        "best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "def main(args, **model_kwargs):\n",
        "    device = torch.device(args.device)\n",
        "    data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "    scaler = data['scaler']\n",
        "    aptinit, supports = make_graph_inputs(args, device)\n",
        "    torch.manual_seed(7)\n",
        "    model = GWNet.from_args(args, device, supports, aptinit, **model_kwargs)\n",
        "    if args.checkpoint:\n",
        "\n",
        "      model.load_checkpoint(torch.load(args.checkpoint))\n",
        "    model.to(device)\n",
        "    engine = Trainer.from_args(model, scaler, args)\n",
        "    metrics = []\n",
        "    best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "    lowest_mae_yet = 100  # high value, will get overwritten\n",
        "    mb = progress_bar(list(range(1, args.epochs + 1)))\n",
        "    epochs_since_best_mae = 0\n",
        "    for _ in mb:\n",
        "        train_loss, train_mape, train_rmse = [], [], []\n",
        "        data['train_loader'].shuffle()\n",
        "        for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\n",
        "          trainx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "          trainy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "          yspeed = trainy[:, 0, :, :]\n",
        "          if yspeed.max() == 0: continue\n",
        "              #print('traınx',trainx.shape)\n",
        "              #print('traıny',trainy.shape)\n",
        "              #print('yspeed',yspeed.shape)\n",
        "          mae, mape, rmse = engine.train(trainx, yspeed)\n",
        "          train_loss.append(mae)\n",
        "          train_mape.append(mape)\n",
        "          train_rmse.append(rmse)\n",
        "          if args.n_iters is not None and iter >= args.n_iters:\n",
        "              break\n",
        "        engine.scheduler.step()\n",
        "        _, valid_loss, valid_mape, valid_rmse = eval_(data['val_loader'], device, engine)\n",
        "        m = dict(train_loss=np.mean(train_loss), train_mape=np.mean(train_mape),\n",
        "                      train_rmse=np.mean(train_rmse), valid_loss=np.mean(valid_loss),\n",
        "                        valid_mape=np.mean(valid_mape), valid_rmse=np.mean(valid_rmse))\n",
        "\n",
        "        m = pd.Series(m)\n",
        "        metrics.append(m)\n",
        "        if m.valid_loss < lowest_mae_yet:\n",
        "            torch.save(engine.model.state_dict(), best_model_save_path)\n",
        "            lowest_mae_yet = m.valid_loss\n",
        "            epochs_since_best_mae = 0\n",
        "        else:\n",
        "            epochs_since_best_mae += 1\n",
        "        met_df = pd.DataFrame(metrics)\n",
        "        mb.comment = f'best val_loss: {met_df.valid_loss.min(): .3f}, current val_loss: {m.valid_loss:.3f}, current train loss: {m.train_loss: .3f}'\n",
        "        met_df.round(6).to_csv(f'{hdir}/metrics.csv')\n",
        "        if epochs_since_best_mae >= args.es_patience: break\n",
        "        # Metrics on test data\n",
        "    with open(best_model_save_path, 'rb') as f:\n",
        "      buffer = io.BytesIO(f.read())\n",
        "    engine.model.load_state_dict(torch.load(buffer))\n",
        "    realy = torch.Tensor(data['y_test']).transpose(1, 3)[:, 0, :, :].to(device)\n",
        "    test_met_df, yhat = calc_tstep_metrics(engine.model, device, data['test_loader'], scaler, realy, args.seq_length)\n",
        "    test_met_df.round(12).to_csv(os.path.join(hdir, 'test_metrics.csv'))\n",
        "\n",
        "def eval_(ds, device, engine):\n",
        "    \"\"\"Run validation.\"\"\"\n",
        "    valid_loss = []\n",
        "    valid_mape = []\n",
        "    valid_rmse = []\n",
        "    s1 = time.time()\n",
        "    for (x, y) in ds.get_iterator():\n",
        "        testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "        testy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "        metrics = engine.eval(testx, testy[:, 0, :, :])\n",
        "        valid_loss.append(metrics[0])\n",
        "        valid_mape.append(metrics[1])\n",
        "        valid_rmse.append(metrics[2])\n",
        "    total_time = time.time() - s1\n",
        "    return total_time, valid_loss, valid_mape, valid_rmse\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser =get_shared_arg_parser()\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='')\n",
        "    parser.add_argument('--clip', type=int, default=3, help='Gradient Clipping')\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.005, help='weight decay rate')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
        "    parser.add_argument('--lr_decay_rate', type=float, default=0.935, help='learning rate')\n",
        "    parser.add_argument('--save', type=str, default='Robbery_Daily_experiment', help='save path')\n",
        "    parser.add_argument('--n_iters', default=None, help='quit after this many iterations')\n",
        "    parser.add_argument('--es_patience', type=int, default=20, help='quit if no improvement after this many iterations')\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    t1 = time.time()\n",
        "    if not os.path.exists(hdir):\n",
        "        os.mkdir(hdir)\n",
        "    pickle_save(args,'/content/gdrive/MyDrive/args.pkl')\n",
        "    main(args)\n",
        "    t2 = time.time()\n",
        "    mins = (t2 - t1) / 60\n",
        "    print(f\"Total time spent: {mins:.2f} seconds\")\n",
        "    #pref=m.split('/')[-1]\n",
        "    #pp=p.split('/')[-1].split('.')[0].split('graph')[0].split('Center')[1]\n",
        "    #key=str(\"experiment:\"+\"-\"+ str(k)+\"-\"+str(l)+\"-\"+str(n)+\".csv\")\n",
        "    key=str(\"Robbery_Daily_experiment:\"+\".csv\")\n",
        "\n",
        "    epath=('/content/gdrive/MyDrive/Robbery_Daily_experiment/%s'%(key))\n",
        "def main(args, save_pred_path=epath, save_metrics_path='last_test_metrics_Robbery_preds_10day_dayca8_smoothingcoef05.csv', loader='test', **model_kwargs):\n",
        "    args.checkpoint= best_model_save_path\n",
        "    device = torch.device(args.device)\n",
        "    adjinit, supports = make_graph_inputs(args, device)\n",
        "    model = GWNet.from_args(args, device, supports, adjinit,**model_kwargs)\n",
        "    model.load_state_dict(torch.load(args.checkpoint))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print('model loaded successfully')\n",
        "    data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "    scaler = data['scaler']\n",
        "    realy = torch.Tensor(data[f'y_{loader}']).to(device)\n",
        "    realy = realy.transpose(1,3)[:,0,:,:]\n",
        "    met_df, yhat = calc_tstep_metrics(model, device, data[f'{loader}_loader'], scaler, realy, args.seq_length)\n",
        "    df2 = make_pred_df(realy, yhat, scaler, args.seq_length)\n",
        "    met_df.to_csv(save_metrics_path)\n",
        "    df2.to_csv(save_pred_path, index=False)\n",
        "    if args.plotheatmap:\n",
        "      plot_learned_adj_matrix(model)\n",
        "    return met_df, df2\n",
        "\n",
        "def plot_learned_adj_matrix(model):\n",
        "    adp = F.softmax(F.relu(torch.mm(model.nodevec1, model.nodevec2)), dim=1)\n",
        "    adp = adp.cpu().detach().numpy()\n",
        "    adp = adp / np.max(adp)\n",
        "    df = pd.DataFrame(adp)\n",
        "    sns.heatmap(df, cmap=\"RdYlBu\")\n",
        "    plt.savefig(\"/content/gdrive/MyDrive/heatmap.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = get_shared_arg_parser()\n",
        "    parser.add_argument('--checkpoints', type=str, help='')\n",
        "    parser.add_argument('--plotheatmap', action='store_true')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='20' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      20.00% [20/100 04:04<16:19 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 4.38 seconds\n",
            "model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBmTw98sqj8T"
      },
      "source": [
        "**UTIL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgX34OkAUa3w"
      },
      "source": [
        "DEFAULT_DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "        if pad_with_last_sample:\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        #print(\"number of batches\",self.num_batch)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                yield (x_i, y_i)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "\n",
        "class StandardScaler():\n",
        "\n",
        "    def __init__(self, mean, std, fill_zeroes=True):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.fill_zeroes = fill_zeroes\n",
        "\n",
        "    def transform(self, data):\n",
        "        if self.fill_zeroes:\n",
        "            mask = (data == 0)\n",
        "            data[mask] = self.mean\n",
        "        return (data - self.mean) / self.std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "\n",
        "\n",
        "def sym_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n",
        "\n",
        "def asym_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "def calculate_normalized_laplacian(adj):\n",
        "    \"\"\"\n",
        "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
        "    # D = diag(A 1)\n",
        "    :param adj:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    d = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "    return normalized_laplacian\n",
        "\n",
        "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
        "    if undirected:\n",
        "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
        "    L = calculate_normalized_laplacian(adj_mx)\n",
        "    if lambda_max is None:\n",
        "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
        "        lambda_max = lambda_max[0]\n",
        "    L = sp.csr_matrix(L)\n",
        "    M, _ = L.shape\n",
        "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
        "    L = (2 / lambda_max * L) - I\n",
        "    return L.astype(np.float32).todense()\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        #print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "ADJ_CHOICES = ['scalap', 'normlap', 'symnadj', 'transition', 'identity']\n",
        "def load_adj(pkl_filename, adjtype):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n",
        "    if adjtype == \"scalap\":\n",
        "        adj = [calculate_scaled_laplacian(adj_mx)]\n",
        "    elif adjtype == \"normlap\":\n",
        "        adj = [calculate_normalized_laplacian(adj_mx).astype(np.float32).todense()]\n",
        "    elif adjtype == \"symnadj\":\n",
        "        #print('sym')\n",
        "        adj = [sym_adj(adj_mx)]\n",
        "    elif adjtype == \"transition\":\n",
        "        adj = [asym_adj(adj_mx)]\n",
        "    elif adjtype == \"doubletransition\":\n",
        "        adj = [asym_adj(adj_mx), asym_adj(np.transpose(adj_mx))]\n",
        "    elif adjtype == \"identity\":\n",
        "        adj = [np.diag(np.ones(adj_mx.shape[0])).astype(np.float32)]\n",
        "    else:\n",
        "        error = 0\n",
        "        assert error, \"adj type not defined\"\n",
        "    #print(\"sensor_ids\",sensor_ids)\n",
        "    #print(\"sensor_id_to_ind\",sensor_id_to_ind)\n",
        "    return sensor_ids, sensor_id_to_ind, adj\n",
        "\n",
        "\n",
        "def load_dataset(dataset_dir, batch_size, valid_batch_size=None, test_batch_size=None, n_obs=None, fill_zeroes=True):\n",
        "    data = {}\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
        "        data['x_' + category] = cat_data['x']\n",
        "        data['y_' + category] = cat_data['y']\n",
        "        if n_obs is not None:\n",
        "            data['x_' + category] = data['x_' + category][:n_obs]\n",
        "            data['y_' + category] = data['y_' + category][:n_obs]\n",
        "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std(), fill_zeroes=fill_zeroes)\n",
        "    # Data format\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "    data['train_loader'] = DataLoader(data['x_train'], data['y_train'], batch_size)\n",
        "    data['val_loader'] = DataLoader(data['x_val'], data['y_val'], valid_batch_size)\n",
        "    data['test_loader'] = DataLoader(data['x_test'], data['y_test'], test_batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "def calc_metrics(preds, labels,device='cuda:0', null_val=0.):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels != null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean(mask)\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    #print(\"mask shape\",mask.shape)\n",
        "    mse = (preds - labels) ** 2\n",
        "    #print(\"preds.shape\",preds.shape)\n",
        "    #print(\"labels.shape\",preds.shape)\n",
        "    #print(\"mse shape\",mse.shape)\n",
        "    #print(\"mse\",mse)\n",
        "    #mae = (torch.abs(preds-labels)) # original mae\n",
        "    #print(\"mae shape before mask\",mae.shape)\n",
        "    #print(\"mae type before mask\", type(mae.shape))\n",
        "    ##adapted mae\n",
        "    if preds.shape[0]==args.batch_size:\n",
        "      fpreds=torch.flatten(preds).to(device).cpu().data.numpy()\n",
        "      flabels=torch.flatten(labels).to(device).cpu().data.numpy()\n",
        "      #print(\"flabels.shape\",fpreds.shape)\n",
        "      #c=[abs(x-y)*y if y >0 else abs(x-y)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Trying MSE  ##Trying MSE yielded inferior performance\n",
        "      ##c=[((x-y)**2)*y if y >0 else ((x-y)**2)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Quantile loss\n",
        "      c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds,flabels)]\n",
        "      a=np.array(c)\n",
        "      b=a.reshape(-1,2459,1)\n",
        "      mae=torch.tensor([b],requires_grad=True).to(device)\n",
        "    else:\n",
        "      d=preds.shape[0]\n",
        "      fpreds=torch.flatten(preds).to(device).cpu().data.numpy()\n",
        "      flabels=torch.flatten(labels).to(device).cpu().data.numpy()\n",
        "      #print(\"flabels.shape\",fpreds.shape)\n",
        "      #c=[abs(x-y)*y if y >0 else abs(x-y)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Trying MSE yielded inferior performance\n",
        "      #c=[((x-y)**2)*y if y >0 else ((x-y)**2)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Quantile loss\n",
        "      c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds,flabels)]\n",
        "      a=np.array(c)\n",
        "      b=a.reshape(d,2459)\n",
        "      mae=torch.tensor([b],requires_grad=True).to(device)\n",
        "    #print(\"mae train\",mae)\n",
        "    #print(\"preds shape in calc_metrics\",preds.shape)\n",
        "    #print(\"labels shape in calc_metrics\",labels.shape)\n",
        "    mape = mae / labels\n",
        "    #print(\"mae before mask\",mae)\n",
        "    #print(\"mae mean before mask\",mae.mean)\n",
        "    mae, mape, mse = [mask_and_fillna(l, mask) for l in [mae, mape, mse]]\n",
        "    #print(\"mae after mask\",mae)\n",
        "    rmse = torch.sqrt(mse)\n",
        "    return mae, mape, rmse\n",
        "\n",
        "\n",
        "def mask_and_fillna(loss, mask):\n",
        "    loss = loss * mask\n",
        "    #print(\"shape of loss\", loss.shape)\n",
        "    #print(\"shape of mask\", mask.shape)\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "\n",
        "def calc_tstep_metrics(model, device, test_loader, scaler, realy, seq_length) -> pd.DataFrame:\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for _, (x, __) in enumerate(test_loader.get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "        #print(\"testx.shape\",testx.shape)\n",
        "        with torch.no_grad():\n",
        "            preds = model(testx).transpose(1, 3)\n",
        "            #print(\"preds.shape\",preds.shape)\n",
        "        outputs.append(preds.squeeze(1))\n",
        "    yhat = torch.cat(outputs, dim=0)[:realy.size(0), ...]\n",
        "    #print(\"yhat.shape in calc tstep metrics\",yhat.shape)\n",
        "    test_met = []\n",
        "\n",
        "    for i in range(seq_length):\n",
        "        pred = scaler.inverse_transform(yhat[:, :, i])\n",
        "        pred = torch.clamp(pred, min=0., max=70.)\n",
        "        real = realy[:, :, i]\n",
        "        test_met.append([x.item() for x in calc_metrics(pred, real)])\n",
        "    test_met_df = pd.DataFrame(test_met, columns=['mae', 'mape', 'rmse']).rename_axis('t')\n",
        "    #print(\"test_met_df.shape\",test_met_df.shape)\n",
        "    return test_met_df, yhat\n",
        "\n",
        "\n",
        "def _to_ser(arr):\n",
        "    return pd.DataFrame(arr.cpu().detach().numpy()).stack().rename_axis(['obs', 'sensor_id'])\n",
        "\n",
        "\n",
        "def make_pred_df(realy, yhat, scaler, seq_length):\n",
        "    #print(\"realy.shape\",realy.shape)\n",
        "    #print(\"yhat.shape\",yhat.shape)\n",
        "    df = pd.DataFrame(dict(y_last=_to_ser(realy[:, :, seq_length - 1]),\n",
        "                           yhat_last=_to_ser(scaler.inverse_transform(yhat[:, :, seq_length - 1]))))\n",
        "                           #y_3=_to_ser(realy[:, :, 2]),\n",
        "                           #yhat_3=_to_ser(scaler.inverse_transform(yhat[:, :, 2]))))\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_graph_inputs(args, device):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_adj(args.adjdata, args.adjtype)\n",
        "    supports = [torch.tensor(i).to(device) for i in adj_mx]\n",
        "    aptinit = None if args.randomadj else supports[0]  # ignored without do_graph_conv and add_apt_adj\n",
        "    if args.aptonly:\n",
        "        if not args.addaptadj and args.do_graph_conv: raise ValueError(\n",
        "            'WARNING: not using adjacency matrix')\n",
        "        supports = None\n",
        "    return aptinit, supports\n",
        "def get_shared_arg_parser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--device', type=str, default='cuda:0' if torch.cuda.is_available() else 'cpu', help='')\n",
        "    parser.add_argument('--data', type=str, default=\"/content/gdrive/MyDrive/Robbery_Daily_data\", help='data path')\n",
        "    parser.add_argument('--adjdata', type=str, default='/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix/Center01graph.pkl',help='adj data path')\n",
        "    ##for grid search\n",
        "    #parser.add_argument('--data', type=str, default=m, help='data path')\n",
        "    #parser.add_argument('--adjdata', type=str, default=p,help='adj data path')\n",
        "    parser.add_argument('--adjtype', type=str, default='symnadj', help='adj type', choices=ADJ_CHOICES)\n",
        "    parser.add_argument('--do_graph_conv', action='store_true',\n",
        "                        help='whether to add graph convolution layer')\n",
        "    parser.add_argument('--aptonly', default=False, help='whether only adaptive adj')\n",
        "    parser.add_argument('--addaptadj', action='store_true', help='whether add adaptive adj')\n",
        "    parser.add_argument('--randomadj', action='store_true',\n",
        "                        help='whether random initialize adaptive adj')\n",
        "    parser.add_argument('--seq_length', type=int, default=1, help='')\n",
        "    parser.add_argument('--nhid', type=int, default=8, help='Number of channels for internal conv')\n",
        "    parser.add_argument('--in_dim', type=int, default=1, help='inputs dimension')\n",
        "    parser.add_argument('--num_nodes', type=int, default=2459, help='number of nodes')\n",
        "    parser.add_argument('--batch_size', type=int, default=16, help='batch size')\n",
        "    parser.add_argument('--dropout', type=float, default=0.3, help='dropout rate')\n",
        "    parser.add_argument('--n_obs', default=None, help='Only use this many observations. For unit testing.')\n",
        "    parser.add_argument('--apt_size', default=10, type=int)\n",
        "    parser.add_argument('--cat_feat_gc', action='store_true')\n",
        "    parser.add_argument('--fill_zeroes', action='store_true')\n",
        "    parser.add_argument('--checkpoint', type=str, help='')\n",
        "    return parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5l9QKP7rv5d"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8gmPuYtrznt"
      },
      "source": [
        "def nconv(x, A):\n",
        "    \"\"\"Multiply x by adjacency matrix along source node axis\"\"\"\n",
        "    return torch.einsum('ncvl,vw->ncwl', (x, A)).contiguous()\n",
        "\n",
        "class GraphConvNet(nn.Module):\n",
        "    def __init__(self, c_in, c_out, dropout, support_len=3, order=2):\n",
        "        super().__init__()\n",
        "        c_in = (order * support_len + 1) * c_in\n",
        "        self.final_conv = Conv2d(c_in, c_out, (1, 1), padding=(0, 0), stride=(1, 1), bias=True)\n",
        "        self.dropout = dropout\n",
        "        self.order = order\n",
        "\n",
        "    def forward(self, x, support: list):\n",
        "        out = [x]\n",
        "        #print(\"x shape in forward\",x.shape)\n",
        "        for a in support:\n",
        "            x1 = nconv(x, a)\n",
        "            out.append(x1)\n",
        "            for k in range(2, self.order + 1):\n",
        "                x2 = nconv(x1, a)\n",
        "                out.append(x2)\n",
        "                x1 = x2\n",
        "\n",
        "        h = torch.cat(out, dim=1)\n",
        "        h = self.final_conv(h)\n",
        "        h = F.dropout(h, self.dropout, training=self.training)\n",
        "        #print(\"hidden layer shape\",h.shape)\n",
        "        return h\n",
        "\n",
        "\n",
        "class GWNet(nn.Module):\n",
        "    def __init__(self, device, num_nodes,supports, dropout=0.3, do_graph_conv=True,\n",
        "                 addaptadj=True,aptinit=None, in_dim=1, out_dim=1,\n",
        "                 residual_channels=32, dilation_channels=32, cat_feat_gc=False,\n",
        "                 skip_channels=256, end_channels=512, kernel_size=2, blocks=7, layers=3,\n",
        "                 apt_size=10): #here we had to delete =none statement for supports and aptinit argument\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.blocks = blocks\n",
        "        self.layers = layers\n",
        "        self.do_graph_conv = do_graph_conv\n",
        "        self.cat_feat_gc = cat_feat_gc\n",
        "        self.addaptadj = addaptadj\n",
        "\n",
        "\n",
        "        if self.cat_feat_gc:\n",
        "            self.start_conv = nn.Conv2d(in_channels=1,  # hard code to avoid errors\n",
        "                                        out_channels=residual_channels,\n",
        "                                        kernel_size=(1, 1))\n",
        "\n",
        "            self.cat_feature_conv = nn.Conv2d(in_channels=in_dim - 1,\n",
        "                                              out_channels=residual_channels,\n",
        "                                              kernel_size=(1, 1))\n",
        "        else:\n",
        "            self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
        "                                        out_channels=residual_channels,\n",
        "                                        kernel_size=(1, 1))\n",
        "\n",
        "        self.fixed_supports = supports or []\n",
        "        receptive_field = 1\n",
        "\n",
        "        self.supports_len = len(self.fixed_supports)\n",
        "        if do_graph_conv and addaptadj:\n",
        "            if aptinit is None:\n",
        "                #print(\"randomnodevecs\")\n",
        "                nodevecs = torch.randn(num_nodes, apt_size), torch.randn(apt_size, num_nodes)\n",
        "            else:\n",
        "                #print(\"not random vecs\")\n",
        "                nodevecs = self.svd_init(apt_size, aptinit)\n",
        "            #print(\"shape nodevecs\",nodevecs.shape)\n",
        "            self.supports_len += 1\n",
        "            self.nodevec1, self.nodevec2 = [Parameter(n.to(device), requires_grad=True) for n in nodevecs]\n",
        "\n",
        "        depth = list(range(blocks * layers))\n",
        "\n",
        "        # 1x1 convolution for residual and skip connections (slightly different see docstring)\n",
        "        self.residual_convs = ModuleList([Conv1d(dilation_channels, residual_channels, (1, 1)) for _ in depth])\n",
        "        self.skip_convs = ModuleList([Conv1d(dilation_channels, skip_channels, (1, 1)) for _ in depth])\n",
        "        self.bn = ModuleList([BatchNorm2d(residual_channels) for _ in depth])\n",
        "        self.graph_convs = ModuleList([GraphConvNet(dilation_channels, residual_channels, dropout, support_len=self.supports_len)\n",
        "                                              for _ in depth])\n",
        "\n",
        "        self.filter_convs = ModuleList()\n",
        "        self.gate_convs = ModuleList()\n",
        "        for b in range(blocks):\n",
        "            additional_scope = kernel_size - 1\n",
        "            D = 1 # dilation\n",
        "            for i in range(layers):\n",
        "                # dilated convolutions\n",
        "                self.filter_convs.append(Conv2d(residual_channels, dilation_channels, (1, kernel_size), dilation=D))\n",
        "                self.gate_convs.append(Conv1d(residual_channels, dilation_channels, (1, kernel_size), dilation=D))\n",
        "                D *= 2\n",
        "                receptive_field += additional_scope\n",
        "                additional_scope *= 2\n",
        "        self.receptive_field = receptive_field\n",
        "        self.end_conv_1 = Conv2d(skip_channels, end_channels, (1, 1), bias=True)\n",
        "        self.end_conv_2 = Conv2d(end_channels, out_dim, (1, 1), bias=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def svd_init(apt_size, aptinit):\n",
        "        m, p, n = torch.linalg.svd(aptinit)\n",
        "        nodevec1 = torch.mm(m[:, :apt_size], torch.diag(p[:apt_size] ** 0.5))\n",
        "        nodevec2 = torch.mm(torch.diag(p[:apt_size] ** 0.5), n[:, :apt_size].t())\n",
        "        #print(\"nodevec1 shape\",nodevec1.shape)\n",
        "        #print(\"nodevec2 shape\",nodevec2.shape)\n",
        "        return nodevec1, nodevec2\n",
        "\n",
        "    @classmethod\n",
        "    def from_args(cls, args, device, supports, aptinit, **kwargs):\n",
        "        defaults = dict(dropout=args.dropout, supports=supports,\n",
        "                        do_graph_conv=args.do_graph_conv, addaptadj=args.addaptadj, aptinit=aptinit,\n",
        "                        in_dim=args.in_dim, apt_size=args.apt_size, out_dim=args.seq_length,\n",
        "                        residual_channels=args.nhid, dilation_channels=args.nhid,\n",
        "                        skip_channels=args.nhid * 8, end_channels=args.nhid * 16,\n",
        "                        cat_feat_gc=args.cat_feat_gc)\n",
        "        defaults.update(**kwargs)\n",
        "        model = cls(device, args.num_nodes, **defaults)\n",
        "        return model\n",
        "\n",
        "    def load_checkpoint(self, state_dict):\n",
        "        \"\"\"It is assumed that ckpt was trained to predict a subset of timesteps.\"\"\"\n",
        "        bk, wk = ['end_conv_2.bias', 'end_conv_2.weight']  # only weights that depend on seq_length\n",
        "        b, w = state_dict.pop(bk), state_dict.pop(wk)\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "        cur_state_dict = self.state_dict()\n",
        "        cur_state_dict[bk][:b.shape[0]] = b\n",
        "        cur_state_dict[wk][:w.shape[0]] = w\n",
        "        self.load_state_dict(cur_state_dict)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('forwardın bası', x.shape)\n",
        "        # Input shape is (bs, features, n_nodes, n_timesteps)\n",
        "        in_len = x.size(3)\n",
        "        if in_len < self.receptive_field:\n",
        "            x = nn.functional.pad(x, (self.receptive_field - in_len, 0, 0, 0))\n",
        "            #print('if receptive_field',x.shape)\n",
        "        if self.cat_feat_gc:\n",
        "            f1, f2 = x[:, [0]], x[:, 1:]\n",
        "            x1 = self.start_conv(f1)\n",
        "            x2 = F.leaky_relu(self.cat_feature_conv(f2))\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            x = self.start_conv(x)\n",
        "            #print('if start conv',x.shape)\n",
        "        skip = 0\n",
        "        adjacency_matrices = self.fixed_supports\n",
        "        # calculate the current adaptive adj matrix once per iteration\n",
        "        if self.addaptadj:\n",
        "            adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)\n",
        "            adjacency_matrices = self.fixed_supports + [adp]\n",
        "\n",
        "        # WaveNet layers\n",
        "        for i in range(self.blocks * self.layers):\n",
        "            # EACH BLOCK\n",
        "\n",
        "            #            |----------------------------------------|     *residual*\n",
        "            #            |                                        |\n",
        "            #            |   |-dil_conv -- tanh --|                |\n",
        "            #         ---|                  * ----|-- 1x1 -- + -->\t*x_in*\n",
        "            #                |-dil_conv -- sigm --|    |\n",
        "            #                                         1x1\n",
        "            #                                          |\n",
        "            # ---------------------------------------> + ------------->\t*skip*\n",
        "            residual = x\n",
        "            # dilated convolution\n",
        "            filter = torch.tanh(self.filter_convs[i](residual))\n",
        "            gate = torch.sigmoid(self.gate_convs[i](residual))\n",
        "            x = filter * gate\n",
        "            #print('filter*gate',x.shape)\n",
        "            # parametrized skip connection\n",
        "            s = self.skip_convs[i](x)  # what are we skipping??\n",
        "            try:  # if i > 0 this works\n",
        "                skip = skip[:, :, :,  -s.size(3):]  # TODO(SS): Mean/Max Pool?\n",
        "            except:\n",
        "                skip = 0\n",
        "            skip = s + skip\n",
        "            if i == (self.blocks * self.layers - 1):  # last X getting ignored anyway\n",
        "                break\n",
        "\n",
        "            if self.do_graph_conv:\n",
        "                graph_out = self.graph_convs[i](x, adjacency_matrices)\n",
        "                x = x + graph_out if self.cat_feat_gc else graph_out\n",
        "                #print('if do graph conv',x.shape)\n",
        "            else:\n",
        "                x = self.residual_convs[i](x)\n",
        "                #print('if residual convs',x.shape)\n",
        "            x = x + residual[:, :, :, -x.size(3):]  # TODO(SS): Mean/Max Pool?\n",
        "            #print('x + residual[:, :, :, -x.size(3):]',x.shape)\n",
        "            x = self.bn[i](x)\n",
        "            #print('self.bn[i](x)',x.shape)\n",
        "\n",
        "        x = F.relu(skip)  # ignore last X?\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        #print('relu and end.conv1',x.shape)\n",
        "        x = self.end_conv_2(x)  # downsample to (bs, seq_length, 1804, nfeatures)\n",
        "        #print(\"x in forward\",x.shape)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OIzqVZ7rb9y"
      },
      "source": [
        "**ENGINE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HCuMURWrj4L"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model: GWNet, scaler, lrate, wdecay, clip=3, lr_decay_rate=.97):\n",
        "        self.model = model\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "        self.scaler = scaler\n",
        "        self.clip = clip\n",
        "        self.scheduler = optim.lr_scheduler.LambdaLR(\n",
        "            self.optimizer, lr_lambda=lambda epoch: lr_decay_rate ** epoch)\n",
        "\n",
        "    @classmethod\n",
        "    def from_args(cls, model, scaler, args):\n",
        "        return cls(model, scaler, args.learning_rate, args.weight_decay, clip=args.clip,\n",
        "                   lr_decay_rate=args.lr_decay_rate)\n",
        "\n",
        "    def train(self, input, real_val):\n",
        "       # print('engıne traın bası',input.shape)\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        input = nn.functional.pad(input,(1,0,0,0))\n",
        "        #print(\"inputshape in engine.train\",input.shape)\n",
        "        output = self.model(input).transpose(1,3)  # now, output = [batch_size,1,num_nodes, seq_length]\n",
        "        #print(\"outputshape in engine.train\",output.shape)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        #print(\"predictshape in engine.train\",predict.shape)\n",
        "        assert predict.shape[1] == 1\n",
        "        mae, mape, rmse = calc_metrics(predict.squeeze(1), real_val, null_val=0.0)\n",
        "        #print(\"mae after calc_metrics\",mae)\n",
        "        #print(\"predict.squeeze shape\",predict.squeeze(1).shape)\n",
        "        #print(\"real val shape\",real_val.shape)\n",
        "        mae.backward()\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "        self.optimizer.step()\n",
        "        #print(\"mae.item().shape\",mae.item())\n",
        "        return mae.item(),mape.item(),rmse.item()\n",
        "\n",
        "    def eval(self, input, real_val):\n",
        "        self.model.eval()\n",
        "        input = nn.functional.pad(input,(1,0,0,0))\n",
        "        output = self.model(input).transpose(1,3) #  [batch_size,seq_length,num_nodes,1]\n",
        "       # print(\"outputshape in engine.eval\",output.shape)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        #print(\"real shape in engine.eval\",real.shape)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        predict = torch.clamp(predict, min=0., max=70.)\n",
        "        #print(\"predict shape in engine.eval\",predict.shape)\n",
        "        mae, mape, rmse = [x.item() for x in calc_metrics(predict, real, null_val=0.0)]\n",
        "        return mae, mape, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQlwya6Q1agt"
      },
      "source": [
        "**EXP RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xcdm3e1gQZ"
      },
      "source": [
        "def summary(d):\n",
        "    try:\n",
        "        tr_val = pd.read_csv(f'{d}/metrics.csv', index_col=0)\n",
        "        tr_ser = tr_val.loc[tr_val.valid_loss.idxmin()]\n",
        "        tr_ser['best_epoch'] = tr_val.valid_loss.idxmin()\n",
        "        tr_ser['min_train_loss'] = tr_val.train_loss.min()\n",
        "    except FileNotFoundError:\n",
        "        tr_ser = pd.Series()\n",
        "    try:\n",
        "        tmet = pd.read_csv(f'{d}/test_metrics.csv', index_col=0)\n",
        "        tmean = tmet.add_prefix('test_').mean()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        tmean = pd.Series()\n",
        "    tab = pd.concat([tr_ser, tmean]).round(3)\n",
        "    return tab\n",
        "\n",
        "def loss_curve(d):\n",
        "    if 'logs' not in d: d =  f'logs/{d}'\n",
        "    tr_val = pd.read_csv(f'{d}/metrics.csv', index_col=0)\n",
        "    return tr_val[['train_loss', 'valid_loss']]\n",
        "\n",
        "\n",
        "def plot_loss_curve(log_dir):\n",
        "    d = loss_curve(log_dir)\n",
        "    ax = d.plot()\n",
        "    plt.axhline(d.valid_loss.min())\n",
        "    print(d.valid_loss.idxmin())\n",
        "\n",
        "def make_results_table():\n",
        "    return pd.DataFrame({os.path.basename(c): summary(c) for c in glob('logs/*')}).T.sort_values('valid_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-qUeEDeINYk"
      },
      "source": [
        "**GRID SEARCH FOR HYPERPARAMETER TUNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "vwm7ZjT6IT7Z",
        "outputId": "64a95b14-faaf-4025-e4a2-8f13a63f3d32"
      },
      "source": [
        "#Iterating over generated train&test datasets\n",
        "import os\n",
        "import itertools\n",
        "directory_list = list()\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Robbery_Daily_data\", topdown=False):\n",
        "    for name in dirs:\n",
        "        directory_list.append(os.path.join(root, name))\n",
        "import glob\n",
        "paths=(\"/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix\")\n",
        "all_files = glob.glob(paths + \"/*.pkl\")\n",
        "##Separate parameters\n",
        "lr=[0.001,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]\n",
        "weight_decay=[0.0001,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01]\n",
        "lr_decay_rate=[0.97,0.94,0.9375,0.9350,0.9325,0.93,0.9275,0.9250,0.9225,0.92,0.9175]\n",
        "for k,l,n in zip(lr,weight_decay,lr_decay_rate):\n",
        "    hdir=\"/content/gdrive/MyDrive/\"\n",
        "    best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "    def main(args, **model_kwargs):\n",
        "        device = torch.device(args.device)\n",
        "        data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "        scaler = data['scaler']\n",
        "        aptinit, supports = make_graph_inputs(args, device)\n",
        "        torch.manual_seed(0)\n",
        "        model = GWNet.from_args(args, device, supports, aptinit, **model_kwargs)\n",
        "        if args.checkpoint:\n",
        "          model.load_checkpoint(torch.load(args.checkpoint))\n",
        "        model.to(device)\n",
        "        engine = Trainer.from_args(model, scaler, args)\n",
        "        metrics = []\n",
        "        best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "        lowest_mae_yet = 100  # high value, will get overwritten\n",
        "        mb = progress_bar(list(range(1, args.epochs + 1)))\n",
        "        epochs_since_best_mae = 0\n",
        "        for _ in mb:\n",
        "            train_loss, train_mape, train_rmse = [], [], []\n",
        "            data['train_loader'].shuffle()\n",
        "            for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\n",
        "              trainx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "              trainy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "              yspeed = trainy[:, 0, :, :]\n",
        "              if yspeed.max() == 0: continue\n",
        "                  #print('traınx',trainx.shape)\n",
        "                  #print('traıny',trainy.shape)\n",
        "                  #print('yspeed',yspeed.shape)\n",
        "              mae, mape, rmse = engine.train(trainx, yspeed)\n",
        "              train_loss.append(mae)\n",
        "              train_mape.append(mape)\n",
        "              train_rmse.append(rmse)\n",
        "              if args.n_iters is not None and iter >= args.n_iters:\n",
        "                  break\n",
        "            engine.scheduler.step()\n",
        "            _, valid_loss, valid_mape, valid_rmse = eval_(data['val_loader'], device, engine)\n",
        "            m = dict(train_loss=np.mean(train_loss), train_mape=np.mean(train_mape),\n",
        "                      train_rmse=np.mean(train_rmse), valid_loss=np.mean(valid_loss),\n",
        "                        valid_mape=np.mean(valid_mape), valid_rmse=np.mean(valid_rmse))\n",
        "\n",
        "            m = pd.Series(m)\n",
        "            metrics.append(m)\n",
        "            if m.valid_loss < lowest_mae_yet:\n",
        "                torch.save(engine.model.state_dict(), best_model_save_path)\n",
        "                lowest_mae_yet = m.valid_loss\n",
        "                epochs_since_best_mae = 0\n",
        "            else:\n",
        "                epochs_since_best_mae += 1\n",
        "            met_df = pd.DataFrame(metrics)\n",
        "            mb.comment = f'best val_loss: {met_df.valid_loss.min(): .3f}, current val_loss: {m.valid_loss:.3f}, current train loss: {m.train_loss: .3f}'\n",
        "            met_df.round(6).to_csv(f'{hdir}/metrics.csv')\n",
        "            if epochs_since_best_mae >= args.es_patience: break\n",
        "            # Metrics on test data\n",
        "        with open(best_model_save_path, 'rb') as f:\n",
        "          buffer = io.BytesIO(f.read())\n",
        "        engine.model.load_state_dict(torch.load(buffer))\n",
        "        realy = torch.Tensor(data['y_test']).transpose(1, 3)[:, 0, :, :].to(device)\n",
        "        test_met_df, yhat = calc_tstep_metrics(engine.model, device, data['test_loader'], scaler, realy, args.seq_length)\n",
        "        test_met_df.round(6).to_csv(os.path.join(hdir, 'test_metrics.csv'))\n",
        "\n",
        "    def eval_(ds, device, engine):\n",
        "        \"\"\"Run validation.\"\"\"\n",
        "        valid_loss = []\n",
        "        valid_mape = []\n",
        "        valid_rmse = []\n",
        "        s1 = time.time()\n",
        "        for (x, y) in ds.get_iterator():\n",
        "            testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "            testy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "            metrics = engine.eval(testx, testy[:, 0, :, :])\n",
        "            valid_loss.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "        total_time = time.time() - s1\n",
        "        return total_time, valid_loss, valid_mape, valid_rmse\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        parser =get_shared_arg_parser()\n",
        "        parser.add_argument('--epochs', type=int, default=100, help='')\n",
        "        parser.add_argument('--clip', type=int, default=3, help='Gradient Clipping')\n",
        "        parser.add_argument('--weight_decay', type=float, default=l, help='weight decay rate')\n",
        "        parser.add_argument('--learning_rate', type=float, default=k, help='learning rate')\n",
        "        parser.add_argument('--lr_decay_rate', type=float, default=n, help='learning rate')\n",
        "        parser.add_argument('--save', type=str, default='Robbery_Daily_experiment', help='save path')\n",
        "        parser.add_argument('--n_iters', default=None, help='quit after this many iterations')\n",
        "        parser.add_argument('--es_patience', type=int, default=25, help='quit if no improvement after this many iterations')\n",
        "\n",
        "        args, unknown = parser.parse_known_args()\n",
        "        t1 = time.time()\n",
        "        if not os.path.exists(hdir):\n",
        "            os.mkdir(hdir)\n",
        "        pickle_save(args,'/content/gdrive/MyDrive/args.pkl')\n",
        "        main(args)\n",
        "        t2 = time.time()\n",
        "        mins = (t2 - t1) / 60\n",
        "        print(f\"Total time spent: {mins:.2f} seconds\")\n",
        "        #pref=m.split('/')[-1]\n",
        "        #pp=p.split('/')[-1].split('.')[0].split('graph')[0].split('Center')[1]\n",
        "        key=str(\"experiment:\"+\"-\"+ str(k)+\"-\"+str(l)+\"-\"+str(n)+\".csv\")\n",
        "        epath=('/content/gdrive/MyDrive/Robbery_Daily_experiment/%s'%(key))\n",
        "    def main(args, save_pred_path=epath, save_metrics_path='last_test_metrics_Robbery_preds_10day_dayca8_smoothingcoef05.csv', loader='test', **model_kwargs):\n",
        "        args.checkpoint= best_model_save_path\n",
        "        device = torch.device(args.device)\n",
        "        adjinit, supports = make_graph_inputs(args, device)\n",
        "        model = GWNet.from_args(args, device, supports, adjinit,**model_kwargs)\n",
        "        model.load_state_dict(torch.load(args.checkpoint))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print('model loaded successfully')\n",
        "        data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "        scaler = data['scaler']\n",
        "        realy = torch.Tensor(data[f'y_{loader}']).to(device)\n",
        "        realy = realy.transpose(1,3)[:,0,:,:]\n",
        "        met_df, yhat = calc_tstep_metrics(model, device, data[f'{loader}_loader'], scaler, realy, args.seq_length)\n",
        "        df2 = make_pred_df(realy, yhat, scaler, args.seq_length)\n",
        "        met_df.to_csv(save_metrics_path)\n",
        "        df2.to_csv(save_pred_path, index=False)\n",
        "        if args.plotheatmap: plot_learned_adj_matrix(model)\n",
        "        return met_df, df2\n",
        "\n",
        "    def plot_learned_adj_matrix(model):\n",
        "        adp = F.softmax(F.relu(torch.mm(model.nodevec1, model.nodevec2)), dim=1)\n",
        "        adp = adp.cpu().detach().numpy()\n",
        "        adp = adp / np.max(adp)\n",
        "        df = pd.DataFrame(adp)\n",
        "        sns.heatmap(df, cmap=\"RdYlBu\")\n",
        "        plt.savefig(\"/content/gdrive/MyDrive/heatmap.png\")\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        parser = get_shared_arg_parser()\n",
        "        parser.add_argument('--checkpoints', type=str, help='')\n",
        "        parser.add_argument('--plotheatmap', action='store_true')\n",
        "        args, unknown = parser.parse_known_args()\n",
        "        main(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='46' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      46.00% [46/100 32:50<38:32 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:194: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 33.92 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='41' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      41.00% [41/100 29:11<42:01 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 30.09 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='31' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      31.00% [31/100 22:01<49:02 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 22.93 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='32' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      32.00% [32/100 22:47<48:25 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 23.67 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='53' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      53.00% [53/100 37:38<33:22 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 38.53 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='34' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      34.00% [34/100 24:13<47:01 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 25.11 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='40' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      40.00% [40/100 28:18<42:27 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 29.18 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='44' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      44.00% [44/100 31:14<39:45 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 32.12 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='44' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      44.00% [44/100 31:06<39:35 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 31.99 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='34' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      34.00% [34/100 24:04<46:44 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time spent: 24.96 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='21' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      21.00% [21/100 14:52<55:56 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='41' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      41.00% [41/100 29:02<41:47 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 29.93 seconds\n",
            "model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZAU-rBb_UlP"
      },
      "source": [
        "# **GRID SEARCH TRAIN-TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "28oYWDSV_ZXc",
        "outputId": "236e1dbb-da87-4709-8fc4-fd18cb696e3d"
      },
      "source": [
        "#Iterating over generated train&test datasets\n",
        "import os\n",
        "directory_list = list()\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Robbery_Daily_data\", topdown=False):\n",
        "    for name in dirs:\n",
        "        directory_list.append(os.path.join(root, name))\n",
        "import glob\n",
        "paths=(\"/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix\")\n",
        "all_files = glob.glob(paths + \"/*.pkl\")\n",
        "\n",
        "for m in directory_list:\n",
        "  for p in all_files:\n",
        "      import itertools\n",
        "      hdir=\"/content/gdrive/MyDrive/\"\n",
        "      best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "      def main(args, **model_kwargs):\n",
        "          device = torch.device(args.device)\n",
        "          data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "          scaler = data['scaler']\n",
        "          aptinit, supports = make_graph_inputs(args, device)\n",
        "          torch.manual_seed(0)\n",
        "          model = GWNet.from_args(args, device, supports, aptinit, **model_kwargs)\n",
        "          if args.checkpoint:\n",
        "            model.load_checkpoint(torch.load(args.checkpoint))\n",
        "          model.to(device)\n",
        "          engine = Trainer.from_args(model, scaler, args)\n",
        "          metrics = []\n",
        "          best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "          lowest_mae_yet = 100  # high value, will get overwritten\n",
        "          mb = progress_bar(list(range(1, args.epochs + 1)))\n",
        "          epochs_since_best_mae = 0\n",
        "          for _ in mb:\n",
        "              train_loss, train_mape, train_rmse = [], [], []\n",
        "              data['train_loader'].shuffle()\n",
        "              for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\n",
        "                trainx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "                trainy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "                yspeed = trainy[:, 0, :, :]\n",
        "                if yspeed.max() == 0: continue\n",
        "                    #print('traınx',trainx.shape)\n",
        "                    #print('traıny',trainy.shape)\n",
        "                    #print('yspeed',yspeed.shape)\n",
        "                mae, mape, rmse = engine.train(trainx, yspeed)\n",
        "                train_loss.append(mae)\n",
        "                train_mape.append(mape)\n",
        "                train_rmse.append(rmse)\n",
        "                if args.n_iters is not None and iter >= args.n_iters:\n",
        "                    break\n",
        "              engine.scheduler.step()\n",
        "              _, valid_loss, valid_mape, valid_rmse = eval_(data['val_loader'], device, engine)\n",
        "              m = dict(train_loss=np.mean(train_loss), train_mape=np.mean(train_mape),\n",
        "                        train_rmse=np.mean(train_rmse), valid_loss=np.mean(valid_loss),\n",
        "                          valid_mape=np.mean(valid_mape), valid_rmse=np.mean(valid_rmse))\n",
        "\n",
        "              m = pd.Series(m)\n",
        "              metrics.append(m)\n",
        "              if m.valid_loss < lowest_mae_yet:\n",
        "                  torch.save(engine.model.state_dict(), best_model_save_path)\n",
        "                  lowest_mae_yet = m.valid_loss\n",
        "                  epochs_since_best_mae = 0\n",
        "              else:\n",
        "                  epochs_since_best_mae += 1\n",
        "              met_df = pd.DataFrame(metrics)\n",
        "              mb.comment = f'best val_loss: {met_df.valid_loss.min(): .3f}, current val_loss: {m.valid_loss:.3f}, current train loss: {m.train_loss: .3f}'\n",
        "              met_df.round(6).to_csv(f'{hdir}/metrics.csv')\n",
        "              if epochs_since_best_mae >= args.es_patience: break\n",
        "              # Metrics on test data\n",
        "          with open(best_model_save_path, 'rb') as f:\n",
        "            buffer = io.BytesIO(f.read())\n",
        "          engine.model.load_state_dict(torch.load(buffer))\n",
        "          realy = torch.Tensor(data['y_test']).transpose(1, 3)[:, 0, :, :].to(device)\n",
        "          test_met_df, yhat = calc_tstep_metrics(engine.model, device, data['test_loader'], scaler, realy, args.seq_length)\n",
        "          test_met_df.round(6).to_csv(os.path.join(hdir, 'test_metrics.csv'))\n",
        "\n",
        "      def eval_(ds, device, engine):\n",
        "          \"\"\"Run validation.\"\"\"\n",
        "          valid_loss = []\n",
        "          valid_mape = []\n",
        "          valid_rmse = []\n",
        "          s1 = time.time()\n",
        "          for (x, y) in ds.get_iterator():\n",
        "              testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "              testy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "              metrics = engine.eval(testx, testy[:, 0, :, :])\n",
        "              valid_loss.append(metrics[0])\n",
        "              valid_mape.append(metrics[1])\n",
        "              valid_rmse.append(metrics[2])\n",
        "          total_time = time.time() - s1\n",
        "          return total_time, valid_loss, valid_mape, valid_rmse\n",
        "\n",
        "\n",
        "      if __name__ == \"__main__\":\n",
        "          parser =get_shared_arg_parser()\n",
        "          parser.add_argument('--epochs', type=int, default=100, help='')\n",
        "          parser.add_argument('--clip', type=int, default=3, help='Gradient Clipping')\n",
        "          parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay rate')\n",
        "          parser.add_argument('--learning_rate', type=float, default=0.01, help='learning rate')\n",
        "          parser.add_argument('--lr_decay_rate', type=float, default=0.97, help='learning rate')\n",
        "          parser.add_argument('--save', type=str, default='experiment', help='save path')\n",
        "          parser.add_argument('--n_iters', default=None, help='quit after this many iterations')\n",
        "          parser.add_argument('--es_patience', type=int, default=25, help='quit if no improvement after this many iterations')\n",
        "\n",
        "          args, unknown = parser.parse_known_args()\n",
        "          t1 = time.time()\n",
        "          if not os.path.exists(hdir):\n",
        "              os.mkdir(hdir)\n",
        "          pickle_save(args,'/content/gdrive/MyDrive/args.pkl')\n",
        "          main(args)\n",
        "          t2 = time.time()\n",
        "          mins = (t2 - t1) / 60\n",
        "          print(f\"Total time spent: {mins:.2f} seconds\")\n",
        "          pref=m.split('/')[-1]\n",
        "          pp=p.split('/')[-1].split('.')[0].split('graph')[0].split('Center')[1]\n",
        "          key=str(\"experiment:\"+pref+pp+\"-\"+\".csv\")\n",
        "          epath=('/content/gdrive/MyDrive/Robbery_Daily_experiment/%s'%(key))\n",
        "      def main(args, save_pred_path=epath, save_metrics_path='last_test_metrics_Robbery_preds_10day_dayca8_smoothingcoef05.csv', loader='test', **model_kwargs):\n",
        "          args.checkpoint= best_model_save_path\n",
        "          device = torch.device(args.device)\n",
        "          adjinit, supports = make_graph_inputs(args, device)\n",
        "          model = GWNet.from_args(args, device, supports, adjinit,**model_kwargs)\n",
        "          model.load_state_dict(torch.load(args.checkpoint))\n",
        "          model.to(device)\n",
        "          model.eval()\n",
        "          print('model loaded successfully')\n",
        "          data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "          scaler = data['scaler']\n",
        "          realy = torch.Tensor(data[f'y_{loader}']).to(device)\n",
        "          realy = realy.transpose(1,3)[:,0,:,:]\n",
        "          met_df, yhat = calc_tstep_metrics(model, device, data[f'{loader}_loader'], scaler, realy, args.seq_length)\n",
        "          df2 = make_pred_df(realy, yhat, scaler, args.seq_length)\n",
        "          met_df.to_csv(save_metrics_path)\n",
        "          df2.to_csv(save_pred_path, index=False)\n",
        "          if args.plotheatmap: plot_learned_adj_matrix(model)\n",
        "          return met_df, df2\n",
        "\n",
        "      def plot_learned_adj_matrix(model):\n",
        "          adp = F.softmax(F.relu(torch.mm(model.nodevec1, model.nodevec2)), dim=1)\n",
        "          adp = adp.cpu().detach().numpy()\n",
        "          adp = adp / np.max(adp)\n",
        "          df = pd.DataFrame(adp)\n",
        "          sns.heatmap(df, cmap=\"RdYlBu\")\n",
        "          plt.savefig(\"/content/gdrive/MyDrive/heatmap.png\")\n",
        "\n",
        "\n",
        "      if __name__ == \"__main__\":\n",
        "          parser = get_shared_arg_parser()\n",
        "          parser.add_argument('--checkpoints', type=str, help='')\n",
        "          parser.add_argument('--plotheatmap', action='store_true')\n",
        "          args, unknown = parser.parse_known_args()\n",
        "          main(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:03<54:09 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:194: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.52 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:00<54:00 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.13 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:02<54:06 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.16 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:01<54:05 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.24 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:04<54:13 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.18 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:03<54:09 best val_loss:  0.000, current val_loss: 0.000, current train loss:  0.000]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.16 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:04<54:12 best val_loss:  0.002, current val_loss: 0.002, current train loss:  0.003]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.25 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:02<54:08 best val_loss:  0.002, current val_loss: 0.002, current train loss:  0.003]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.14 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:01<54:03 best val_loss:  0.002, current val_loss: 0.002, current train loss:  0.003]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.11 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:01<54:04 best val_loss:  0.011, current val_loss: 0.011, current train loss:  0.018]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.20 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 17:59<53:58 best val_loss:  0.011, current val_loss: 0.011, current train loss:  0.018]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.08 seconds\n",
            "model loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      25.00% [25/100 18:01<54:03 best val_loss:  0.011, current val_loss: 0.011, current train loss:  0.018]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time spent: 19.11 seconds\n",
            "model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoOqtwVi63ja"
      },
      "source": [
        "**PREPARING THE EXTERNAL DATASETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuVEhyim66Rm"
      },
      "source": [
        "import geopandas as gpd\n",
        "cl_gchi_core_sn=pd.read_pickle(\"/content/gdrive/MyDrive/MainFiles/snappedcorecrimes.pkl\")\n",
        "##Subsetting the crime dataset\n",
        "Robbery=['312','313','031A','031B','320','325','326','330','331','334','337','033A','033B','340']\n",
        "Theft=['810','820','850','860','865','870','880','890','895']\n",
        "core20162018=cl_gchi_core_sn.loc[(cl_gchi_core_sn['Time_ID'] > 2191) & (cl_gchi_core_sn['Time_ID'] <= 4383)]\n",
        "Robbery20162018=core20162018.loc[(core20162018.IUCR.isin(Robbery))]\n",
        "Theft20162018=core20162018.loc[(core20162018.IUCR.isin(Theft))]\n",
        "##Retrieving the segments\n",
        "gchi_segments=pd.read_pickle(\"/content/gdrive/MyDrive/MainFiles/segments.pkl\")\n",
        "gchi_intersections=pd.read_pickle(\"/content/gdrive/MyDrive/MainFiles/gintersections.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ZgjjlE22KG"
      },
      "source": [
        "**RETRIEVING THE BUSINESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ZU87p-IB25qt",
        "outputId": "a5172324-4576-41de-e6cf-006d55926815"
      },
      "source": [
        "##Retrieving the business\n",
        "chi_bus= pd.read_csv(\"/content/gdrive/MyDrive/MainFiles/MatchedFeatures8.csv\")\n",
        "##Theft RSIS\n",
        "tchi_bus=chi_bus.drop(['Theft_RSIS_WD', 'Theft_RSIS_WE','Robbery_RSIS_WD', 'Robbery_RSIS_WE',\n",
        "                       'Theft_RSIS_FS','Theft_RSIS_SS','Theft_RSIS_TS' ], axis=1)\n",
        "tchi_bus.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-27096422b191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Retrieving the business\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchi_bus\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/MainFiles/MatchedFeatures8.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m##Theft RSIS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m tchi_bus=chi_bus.drop(['Theft_RSIS_WD', 'Theft_RSIS_WE','Robbery_RSIS_WD', 'Robbery_RSIS_WE',\n\u001b[1;32m      5\u001b[0m                        'Theft_RSIS_FS','Theft_RSIS_SS','Theft_RSIS_TS' ], axis=1)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/MainFiles/MatchedFeatures8.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKMTQ7GkKgXk"
      },
      "source": [
        "**Generating RSIS datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7CixLwqKbCB"
      },
      "source": [
        "def generate_datasets(path,crime_type,TransactionDF,matrixdf,day=True,shift=False):\n",
        "    df=pd.read_csv(path)\n",
        "    if day:\n",
        "        names=['Segment_ID',str(crime_type+'_RSIS_WD'),str(crime_type+'_RSIS_WE')]\n",
        "        tchi_bus=df[names]\n",
        "        a=tchi_bus.groupby('Segment_ID')\n",
        "        bus_Segment=a.sum().transpose()\n",
        "        bus_Segment.columns = bus_Segment.columns.astype(str)\n",
        "        bus_Segment.index=[0,1]\n",
        "        wdf=pd.DataFrame(columns=['st1','st2','influence'])\n",
        "        wef=pd.DataFrame(columns=['st1','st2','influence'])\n",
        "    if shift:\n",
        "        names=['Segment_ID',str(crime_type+'_RSIS_FS'),str(crime_type+'_RSIS_SS'),str(crime_type+'_RSIS_TS')]\n",
        "        tchi_bus=df[names]\n",
        "        a=tchi_bus.groupby('Segment_ID')\n",
        "        bus_Segment=a.sum().transpose()\n",
        "        bus_Segment.columns = bus_Segment.columns.astype(str)\n",
        "        bus_Segment.index=[0,1,2]\n",
        "        wfs=pd.DataFrame(columns=['st1','st2','influence'])\n",
        "        wss=pd.DataFrame(columns=['st1','st2','influence'])\n",
        "        wts=pd.DataFrame(columns=['st1','st2','influence'])\n",
        "\n",
        "    for keys,values in matrixdf.items():\n",
        "        if keys in bus_Segment.columns:\n",
        "            for key,value in values.items():\n",
        "                for k,v in value.items():\n",
        "                    if day:\n",
        "                        wdi=bus_Segment.loc[0,keys]*v\n",
        "                        wei=bus_Segment.loc[1,keys]*v\n",
        "                        wddct={'st1':keys,'st2':key,'influence':wdi}\n",
        "                        wedct={'st1':keys,'st2':key,'influence':wei}\n",
        "                        wdf=wdf.append(wddct,ignore_index=True)\n",
        "                        wef=wef.append(wedct,ignore_index=True)\n",
        "                    if shift:\n",
        "                        wfsi=bus_Segment.loc[0,keys]*v\n",
        "                        wssi=bus_Segment.loc[1,keys]*v\n",
        "                        wtsi=bus_Segment.loc[2,keys]*v\n",
        "                        wfsdct={'st1':keys,'st2':key,'influence':wfsi}\n",
        "                        wssdct={'st1':keys,'st2':key,'influence':wssi}\n",
        "                        wtsdct={'st1':keys,'st2':key,'influence':wtsi}\n",
        "                        wfs=wfs.append(wfsdct,ignore_index=True)\n",
        "                        wss=wss.append(wssdct,ignore_index=True)\n",
        "                        wts=wts.append(wtsdct,ignore_index=True)\n",
        "    if day:\n",
        "        df_list=[wdf,wef]\n",
        "        lst1=[]\n",
        "        for i in df_list:\n",
        "            b=i.groupby('st2')\n",
        "            c=b.sum().transpose()\n",
        "            lst1.append(c)\n",
        "        df1=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        b=np.zeros(2288)\n",
        "        df1=df1.append(pd.DataFrame(b.reshape(2,-1), columns=TransactionDF.columns), ignore_index=True)\n",
        "        df2 = pd.concat(lst1)\n",
        "        df2.columns = df2.columns.astype(str)\n",
        "        df2.index=[0,1]\n",
        "        for index, row in df2.iterrows():\n",
        "            for i in list(df2.columns):\n",
        "                df1.loc[index,i]=df2.loc[index,i]\n",
        "        df3=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        b=np.zeros(2288)\n",
        "        df3=df3.append(pd.DataFrame(b.reshape(2,-1), columns=TransactionDF.columns), ignore_index=True)\n",
        "    elif shift:\n",
        "        df_list=[wfs,wss,wts]\n",
        "        lst1=[]\n",
        "        for i in df_list:\n",
        "            b=i.groupby('st2')\n",
        "            c=b.sum().transpose()\n",
        "            lst1.append(c)\n",
        "        df1=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        b=np.zeros(3432)\n",
        "        df1=df1.append(pd.DataFrame(b.reshape(3,-1), columns=TransactionDF.columns), ignore_index=True)\n",
        "        df2 = pd.concat(lst1)\n",
        "        df2.columns = df2.columns.astype(str)\n",
        "        df2.index=[0,1,2]\n",
        "        for index, row in df2.iterrows():\n",
        "            for i in list(df2.columns):\n",
        "                df1.loc[index,i]=df2.loc[index,i]\n",
        "        df3=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        b=np.zeros(3432)\n",
        "        df3=df3.append(pd.DataFrame(b.reshape(3,-1), columns=TransactionDF.columns), ignore_index=True)\n",
        "    for index, row in bus_Segment.iterrows():\n",
        "            for i in list(bus_Segment.columns):\n",
        "                df3.loc[index,i]=bus_Segment.loc[index,i]\n",
        "    df4= df1.add(df3, fill_value=0)\n",
        "    #Normalize the dataframe\n",
        "    column_maxes = df4.max()\n",
        "    df_max = column_maxes.max()\n",
        "    normalized_df4 = df4 / df_max\n",
        "    if day:\n",
        "        a= pd.date_range(start ='1-1-2016',end ='31-12-2017', freq ='D')\n",
        "        df5=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        df6=pd.DataFrame(columns=TransactionDF.columns)\n",
        "#Weekend/Weekday distinction\n",
        "        for i in a:\n",
        "            if i.dayofweek>4:\n",
        "                df5=df5.append(pd.Series(df4.values[1],index=df5.columns),ignore_index=True)\n",
        "                df6=df6.append(pd.Series(normalized_df4.values[1],index=df6.columns),ignore_index=True)\n",
        "            else:\n",
        "                df5=df5.append(pd.Series(df4.values[1],index=df5.columns),ignore_index=True)\n",
        "                df6=df6.append(pd.Series(normalized_df4.values[0],index=df6.columns),ignore_index=True)\n",
        "    if shift:\n",
        "        a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "        df5=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        df6=pd.DataFrame(columns=TransactionDF.columns)\n",
        "        for i in a:\n",
        "            if i.hour==0:\n",
        "                df5=df5.append(pd.Series(normalized_df4.values[0],index=df5.columns),ignore_index=True)\n",
        "                df6=df6.append(pd.Series(normalized_df4.values[0],index=df6.columns),ignore_index=True)\n",
        "            elif i.hour==8:\n",
        "                df5=df5.append(pd.Series(normalized_df4.values[1],index=df5.columns),ignore_index=True)\n",
        "                df6=df6.append(pd.Series(normalized_df4.values[1],index=df6.columns),ignore_index=True)\n",
        "            else:\n",
        "                df5=df5.append(pd.Series(normalized_df4.values[2],index=df5.columns),ignore_index=True)\n",
        "                df6=df6.append(pd.Series(normalized_df4.values[2],index=df6.columns),ignore_index=True)\n",
        "    return df5,df6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAlTOkZ-jgca"
      },
      "source": [
        "**Writing the files to csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jzkPuCTsStq"
      },
      "source": [
        "crime_list=['Robbery','Theft']\n",
        "order=[[True,False],[False,True]]\n",
        "path='C:/Users/Tugrul/Desktop/MatchedFeatures8.csv'\n",
        "for i in crime_list:\n",
        "    for j in order:\n",
        "        if j[0]:\n",
        "            a,b=generate_datasets(path,i,Theft_Transactions8_Shift,matrix8,day=j[0],shift=j[1])\n",
        "            name1=str(i+'day.csv')\n",
        "            name2=str(i+'normalized_day.csv')\n",
        "            a.to_csv('/content/gdrive/MyDrive/MainFiles/RSIS/ %s' % (name1))\n",
        "            b.to_csv('/content/gdrive/MyDrive/MainFiles/RSIS/ %s' % (name2))\n",
        "        if j[1]:\n",
        "            a,b=generate_datasets(path,i,Theft_Transactions8_Shift,matrix8,day=j[0],shift=j[1])\n",
        "            name3=str(i+'shift.csv')\n",
        "            name4=str(i+'normalized_shift.csv')\n",
        "            a.to_csv('/content/gdrive/MyDrive/MainFiles/RSIS/ %s' % (name3))\n",
        "            b.to_csv('/content/gdrive/MyDrive/MainFiles/RSIS/ %s' % (name4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DyU1Pxg9JBs"
      },
      "source": [
        "**Writing the RSIS counts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiwCxBU4BBb-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "f6454ee0-0092-4653-ff46-2ee77b5bdce7"
      },
      "source": [
        "crime_list=['Robbery','Theft']\n",
        "order=[[True,False],[False,True]]\n",
        "path='/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/matched_features_Center.csv'\n",
        "for i in crime_list:\n",
        "    for j in order:\n",
        "        if j[0]:\n",
        "            a,b=generate_datasets_counts(path,i,TheftTable_daily,day=j[0],shift=j[1])\n",
        "            name1=str(i+'count_day.csv')\n",
        "            name2=str(i+'count_normalized_day.csv')\n",
        "            a.to_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ %s' % (name1))\n",
        "            b.to_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ %s' % (name2))\n",
        "        if j[1]:\n",
        "            a,b=generate_datasets_counts(path,i,TheftTable,day=j[0],shift=j[1])\n",
        "            name3=str(i+'count_shift.csv')\n",
        "            name4=str(i+'count_normalized_shift.csv')\n",
        "            a.to_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ %s' % (name3))\n",
        "            b.to_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ %s' % (name4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-21505fc19dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_datasets_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheftTable_daily\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mname1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'count_day.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mname2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'count_normalized_day.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_datasets_counts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7grfMG4jyGql"
      },
      "source": [
        "**Retrieving RSIS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLqoRdG7rir0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "48462a9b-c185-40bd-f898-7b66c8946436"
      },
      "source": [
        "##Theft\n",
        "#Shift\n",
        "TheftRSIS = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftshift.csv')\n",
        "TheftRSIS=TheftRSIS.drop(columns=['Unnamed: 0'])\n",
        "TheftRSIS_normalized = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftnormalized_shift.csv')\n",
        "TheftRSIS_normalized=TheftRSIS_normalized.drop(columns=['Unnamed: 0'])\n",
        "TheftRSIS_mult_normalized = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftmult_normalized_shift.csv')\n",
        "TheftRSIS_mult_normalized=TheftRSIS_mult_normalized.drop(columns=['Unnamed: 0'])\n",
        "TheftRSIS_count_normalized = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftcount_normalized_shift.csv')\n",
        "TheftRSIS_count_normalized=TheftRSIS_count_normalized.drop(columns=['Unnamed: 0'])\n",
        "#Daily\n",
        "TheftRSIS_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftday.csv')\n",
        "TheftRSIS_daily=TheftRSIS_daily.drop(columns=['Unnamed: 0'])\n",
        "TheftRSIS_normalized_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftnormalized_day.csv')\n",
        "TheftRSIS_normalized_daily=TheftRSIS_normalized_daily.drop(columns=['Unnamed: 0'])\n",
        "TheftRSIS_mult_normalized_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftmult_normalized_day.csv')\n",
        "TheftRSIS_mult_normalized_daily=TheftRSIS_mult_normalized_daily.drop(columns=['Unnamed: 0'])\n",
        "TheftRSIS_count_normalized_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Theftcount_normalized_day.csv')\n",
        "TheftRSIS_count_normalized_daily=TheftRSIS_count_normalized_daily.drop(columns=['Unnamed: 0'])\n",
        "##Robbery\n",
        "#Shift\n",
        "RobberyRSIS = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberyshift.csv')\n",
        "RobberyRSIS=RobberyRSIS.drop(columns=['Unnamed: 0'])\n",
        "RobberyRSIS_normalized = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberynormalized_shift.csv')\n",
        "RobberyRSISS_normalized=RobberyRSIS_normalized.drop(columns=['Unnamed: 0'])\n",
        "RobberyRSIS_mult_normalized = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberymult_normalized_shift.csv')\n",
        "RobberyRSIS_mult_normalized=RobberyRSIS_mult_normalized.drop(columns=['Unnamed: 0'])\n",
        "RobberyRSIS_count_normalized = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberycount_normalized_shift.csv')\n",
        "RobberyRSIS_count_normalized=RobberyRSIS_count_normalized.drop(columns=['Unnamed: 0'])\n",
        "#Daily\n",
        "RobberyRSIS_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberyday.csv')\n",
        "RobberyRSIS_daily=RobberyRSIS_daily.drop(columns=['Unnamed: 0'])\n",
        "RobberyRSIS_normalized_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberynormalized_day.csv')\n",
        "RobberyRSIS_normalized_daily=RobberyRSIS_normalized_daily.drop(columns=['Unnamed: 0'])\n",
        "RobberyRSIS_mult_normalized_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberymult_normalized_day.csv')\n",
        "RobberyRSIS_mult_normalized_daily=RobberyRSIS_mult_normalized_daily.drop(columns=['Unnamed: 0'])\n",
        "RobberyRSIS_count_normalized_daily = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/RSIS/ Robberycount_normalized_day.csv')\n",
        "RobberyRSIS_count_normalized_daily = RobberyRSIS_count_normalized_daily.iloc[: , 1:]\n",
        "RobberyRSIS_count_normalized_daily.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>3</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>28</th>\n",
              "      <th>30</th>\n",
              "      <th>41</th>\n",
              "      <th>55</th>\n",
              "      <th>71</th>\n",
              "      <th>99</th>\n",
              "      <th>103</th>\n",
              "      <th>111</th>\n",
              "      <th>121</th>\n",
              "      <th>125</th>\n",
              "      <th>129</th>\n",
              "      <th>135</th>\n",
              "      <th>152</th>\n",
              "      <th>173</th>\n",
              "      <th>180</th>\n",
              "      <th>187</th>\n",
              "      <th>189</th>\n",
              "      <th>195</th>\n",
              "      <th>202</th>\n",
              "      <th>213</th>\n",
              "      <th>243</th>\n",
              "      <th>318</th>\n",
              "      <th>330</th>\n",
              "      <th>348</th>\n",
              "      <th>357</th>\n",
              "      <th>371</th>\n",
              "      <th>381</th>\n",
              "      <th>402</th>\n",
              "      <th>408</th>\n",
              "      <th>...</th>\n",
              "      <th>55344</th>\n",
              "      <th>55356</th>\n",
              "      <th>55359</th>\n",
              "      <th>55421</th>\n",
              "      <th>55426</th>\n",
              "      <th>55433</th>\n",
              "      <th>55439</th>\n",
              "      <th>55469</th>\n",
              "      <th>55490</th>\n",
              "      <th>55506</th>\n",
              "      <th>55520</th>\n",
              "      <th>55550</th>\n",
              "      <th>55573</th>\n",
              "      <th>55588</th>\n",
              "      <th>55592</th>\n",
              "      <th>55597</th>\n",
              "      <th>55636</th>\n",
              "      <th>55687</th>\n",
              "      <th>55689</th>\n",
              "      <th>55752</th>\n",
              "      <th>55772</th>\n",
              "      <th>55780</th>\n",
              "      <th>55814</th>\n",
              "      <th>55831</th>\n",
              "      <th>55840</th>\n",
              "      <th>55867</th>\n",
              "      <th>55915</th>\n",
              "      <th>55932</th>\n",
              "      <th>55990</th>\n",
              "      <th>56015</th>\n",
              "      <th>56025</th>\n",
              "      <th>56039</th>\n",
              "      <th>56141</th>\n",
              "      <th>56145</th>\n",
              "      <th>56146</th>\n",
              "      <th>56156</th>\n",
              "      <th>56183</th>\n",
              "      <th>56225</th>\n",
              "      <th>56250</th>\n",
              "      <th>56293</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2459 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     1    3    5    6    7    8  ...  56146  56156  56183  56225  56250  56293\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.1    0.0    0.2    0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.1    0.0    0.2    0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.1    0.0    0.2    0.0\n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.1    0.0    0.2    0.0\n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.1    0.0    0.2    0.0\n",
              "\n",
              "[5 rows x 2459 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxwNi0HqqSxJ"
      },
      "source": [
        "**SUBSETTING THE GRAPH**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN7kMoXnqkJN"
      },
      "source": [
        "def create_neighborhood_graph(Cno):\n",
        "    a=gchi_segments['Community_Area']==Cno\n",
        "    #segmentsCno=gchi_segments[gchi_segments.Segment_ID.isin(Cno)]\n",
        "    segmentsCno=gchi_segments[a]\n",
        "    edgesCno=segmentsCno[['i', 'j','Segment_ID','shape_len']]\n",
        "    edgesCno['i']=edgesCno['i'].astype('str')\n",
        "    edgesCno['j']=edgesCno['j'].astype('str')\n",
        "    cgraphCno=nx.Graph()\n",
        "    for i, elrow in edgesCno.iterrows():## iterrows help iterate over DataFrame rows as (index, Series) pairs\n",
        "        cgraphCno.add_edge(elrow[0], elrow[1], attr_dict=elrow[2:].to_dict())\n",
        "    ##Intersections\n",
        "    un1=np.unique(edgesCno['i'])\n",
        "    un2=np.unique(edgesCno['j'])\n",
        "    un3=np.unique(np.concatenate((un1,un2)))\n",
        "    nodesCno=gchi_intersections[gchi_intersections['num'].isin(un3)]\n",
        "    nodesCno['num']=nodesCno['num'].astype('str')\n",
        "    for i, nlrow in nodesCno.iterrows():\n",
        "        cgraphCno.nodes[nlrow['num']].update(nlrow[1:].to_dict())\n",
        "    ##Listing the first 5 edges and nodes\n",
        "    #list(cgraphCno.edges(data=True))[0:5]\n",
        "    #list(cgraphCno.nodes(data=True))[0:5]\n",
        "    ##Finding the number of components\n",
        "    #nx.number_connected_components(cgraphCno)\n",
        "    ##CREATING THE SEGMENT GRAPH FOR COMMUNITY AREA 8\n",
        "    mnsegmentsCno=segmentsCno[['Segment_ID','shape_len','i','j','Community_Area']]\n",
        "    ##Converting Types\n",
        "    mnsegmentsCno['Community_Area']=mnsegmentsCno['Community_Area'].astype('Int64')\n",
        "    mnsegmentsCno['Segment_ID']=mnsegmentsCno['Segment_ID'].astype('str')\n",
        "    mnsegmentsCno['i']=mnsegmentsCno['i'].astype('str')\n",
        "    mnsegmentsCno['j']=mnsegmentsCno['j'].astype('str')\n",
        "    mnsegmentsCno['Label'] = mnsegmentsCno.apply(lambda row: str(str(row.i)+'-'+str(row.j)), axis = 1)\n",
        "    ##Adjacency matrix of maingraph\n",
        "    #neighborsCno= nx.adjacency_matrix(cgraphCno, nodelist=None, weight='weight')\n",
        "    ##Neighbor List\n",
        "    matrixxCno= nx.convert.to_dict_of_dicts(cgraphCno)\n",
        "    ##Find the network distance between neighbors\n",
        "    medge_dict={}\n",
        "    for u,v,data in cgraphCno.edges(data=True):\n",
        "       key= str(str(u)+'-'+str(v))\n",
        "       medge_dict.update({key:data})\n",
        "    ##Converting Segments into Nodes\n",
        "    import itertools\n",
        "    caCno= pd.DataFrame()\n",
        "    for key, value in matrixxCno.items():\n",
        "        keylist=value.keys()\n",
        "        combinations_object = itertools.combinations(keylist, 2)\n",
        "        combinations_list = list(combinations_object)\n",
        "        for i in combinations_list:\n",
        "            nkey1= str(key+'-'+i[0])\n",
        "            rnkey1=str(i[0]+'-'+key)\n",
        "            c1= mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey1) | (mnsegmentsCno['Label'] == rnkey1) ,'Segment_ID'].iloc[0]\n",
        "            d1= (mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey1) | (mnsegmentsCno['Label'] == rnkey1),'shape_len'].iloc[0])/2\n",
        "            nkey2= str(key+'-'+i[1])\n",
        "            rnkey2=rnkey1=str(i[1]+'-'+key)\n",
        "            c2= mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey2) | (mnsegmentsCno['Label'] == rnkey2),'Segment_ID'].iloc[0]\n",
        "            d2= (mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey2) | (mnsegmentsCno['Label'] == rnkey2),'shape_len'].iloc[0])/2\n",
        "            e=(d1+d2)\n",
        "            caCno = caCno.append({'S1': c1, 'S2': c2, 'distance': e}, ignore_index=True)\n",
        "    ##Weigthing the edges using gaussian weighting\n",
        "    import math\n",
        "    caCno['weight'] = caCno.apply(lambda row: (math.exp(-row.distance**0.2)), axis = 1)\n",
        "    ##Creating three columns dataset\n",
        "    #ncaCno=caCno[[['S1','S2','weight']]\n",
        "    #ncaCno=ncaCno[.rename(columns={\"S1\": \"from\", \"S2\": \"to\",\"weight\": \"distance\"})\n",
        "    #ncaCno.to_csv('C:/Users/Tugrul/Desktop/distances.csv',sep=\",\",index=False)\n",
        "    ##Creating Segment graph from the dataframe\n",
        "    graphcaCno=nx.from_pandas_edgelist(caCno,source='S1',target='S2',edge_attr='weight')\n",
        "    #ngraphca25=nx.from_pandas_edgelist(ca25,source='S1',target='S2',edge_attr='distance')\n",
        "    ##creating dict of dicts for the dataframe\n",
        "    #matrixca25= nx.convert.to_dict_of_dicts(graphca25)\n",
        "    k='8'\n",
        "    name= \"graph.pickle\"\n",
        "    ##Saving the graph for CA 8\n",
        "    with open('/content/gdrive/MyDrive/MainFiles/%s%s' % (k,name), 'wb') as handle:\n",
        "        pickle.dump(graphcaCno, handle, protocol=2)\n",
        "    return caCno,graphcaCno\n",
        "gchi_segments=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/segments.pkl')\n",
        "gchi_intersections=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/gintersections.pkl')\n",
        "graphdf8,graph8=create_neighborhood_graph(8)\n",
        "##Saving Graphdf8\n",
        "graphdf8.to_pickle('/content/gdrive/MyDrive/MainFiles/graphdf8.pickle')\n",
        "matrixca8= nx.convert.to_dict_of_dicts(graph8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wsp0c3RqvmJ"
      },
      "source": [
        "**CREATING TRANSACTION TABLE FOR COMMUNITY AREAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG89nOy6q36x"
      },
      "source": [
        "def crime_transactions(sdf,ca):\n",
        "    a=np.array(sdf['S1'],dtype=int)\n",
        "    b=np.array(sdf['S2'],dtype=int)\n",
        "    c=np.append(a,b)\n",
        "    sdf=np.unique(c)\n",
        "    nsdf= np.array2string(sdf)\n",
        "    #np.savetxt('C:/Users/Tugrul/Desktop/graph_sensor_ids.txt', sdf, delimiter=',',fmt='%i')\n",
        "    Columnnames= np.concatenate([['Time_ID'],sdf])\n",
        "    Indexvalues=np.arange(2192,4384,1)\n",
        "    a = np.zeros(shape=(2192,(len(sdf)+1)))\n",
        "    sdfTransactionTable = pd.DataFrame(a)\n",
        "    sdfTransactionTable.columns=Columnnames\n",
        "    sdfTransactionTable['Time_ID']=Indexvalues\n",
        "    sdfTransactionTable=sdfTransactionTable.astype(int)\n",
        "    #Subsetting Robbery and Theft\n",
        "    a=Robbery20162018['Community_Area']==ca\n",
        "    Robberyca=Robbery20162018[a]\n",
        "    a=Theft20162018['Community_Area']==ca\n",
        "    Theftca=Theft20162018[a]\n",
        "    #Robberyca=Robbery20162018[Robbery20162018.Segment_ID.isin(lst)]\n",
        "    #Theftca=Theft20162018[Theft20162018.Segment_ID.isin(lst)]\n",
        "    ## Creating Crosstabs for Selected Dataset\n",
        "    selectedcrimes20162018={'Robbery':Robberyca,'Theft':Theftca}\n",
        "    crimetimeseries={}\n",
        "    for k,v in selectedcrimes20162018.items():\n",
        "        df= pd.crosstab(v['Time_ID'],v['Segment_ID'],dropna=False)\n",
        "        crimetimeseries.update({k:df})\n",
        "    ##Inserting into the Robbery Transactional Table\n",
        "    RobberySerie=crimetimeseries['Robbery']\n",
        "    RobberySerie.dtypes\n",
        "    RobberySerie.columns = RobberySerie.columns.astype(str)\n",
        "    sdfTransactionTableR=sdfTransactionTable.copy()\n",
        "    sdfTransactionTableR.index=sdfTransactionTableR['Time_ID']\n",
        "    sdfTransactionTableR.dtypes\n",
        "    sdfTransactionTableR=sdfTransactionTableR.astype(str)\n",
        "    #type(list(sdfTransactionTableR.columns)[1])==list(sdfTransactionTableR)[1]\n",
        "    sdfTransactionTableR.columns = sdfTransactionTableR.columns.astype(str)\n",
        "    RobberySerie.index = RobberySerie.index.map(str)\n",
        "    sdfTransactionTableR.index = sdfTransactionTableR.index.map(str)\n",
        "    ##Inserting the transactions\n",
        "    for index, row in RobberySerie.iterrows():\n",
        "        for i in list(RobberySerie.columns):\n",
        "            sdfTransactionTableR.loc[index,i]=RobberySerie.loc[index,i]\n",
        "    #Aggregating Robbery at daily level\n",
        "    a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "    sdfTransactionTableR.index=a\n",
        "    sdfTransactionTableR=  sdfTransactionTableR.apply(pd.to_numeric)\n",
        "    sdfTransactionTableR_daily = sdfTransactionTableR.resample('D').sum()\n",
        "    ##Inserting into the Theft Transactional Table\n",
        "    TheftSerie=crimetimeseries['Theft']\n",
        "    TheftSerie.columns = TheftSerie.columns.astype(str)\n",
        "    sdfTransactionTableT=sdfTransactionTable.copy()\n",
        "    sdfTransactionTableT.index=sdfTransactionTableT['Time_ID']\n",
        "    sdfTransactionTableT.dtypes\n",
        "    sdfTransactionTableT=sdfTransactionTableT.astype(str)\n",
        "    sdfTransactionTableT.columns = sdfTransactionTableT.columns.astype(str)\n",
        "    TheftSerie.index = TheftSerie.index.map(str)\n",
        "    sdfTransactionTableT.index = sdfTransactionTableT.index.map(str)\n",
        "    ##Inserting the theft transactions\n",
        "    for index, row in TheftSerie.iterrows():\n",
        "        for i in list(TheftSerie.columns):\n",
        "            sdfTransactionTableT.loc[index,i]=TheftSerie.loc[index,i]\n",
        "    sdfTransactionTableT=sdfTransactionTableT.drop(columns= ['Time_ID'])\n",
        "    ##Aggregating at daily level\n",
        "    a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "    sdfTransactionTableT.index=a\n",
        "    sdfTransactionTableT=  sdfTransactionTableT.apply(pd.to_numeric)\n",
        "    sdfTransactionTableT_daily = sdfTransactionTableT.resample('D').sum()\n",
        "    ##Smoothing the columns\n",
        "    sdfTransactionTableR_Smoothed=sdfTransactionTableR.ewm(alpha=0.1).mean()\n",
        "    sdfTransactionTableT_Smoothed=sdfTransactionTableT.ewm(alpha=0.1).mean()\n",
        "    #return sdfTransactionTableT_daily,sdfTransactionTableT\n",
        "    return sdfTransactionTableT,sdfTransactionTableT_daily\n",
        "#Theft_Transactions8_Daily,Theft_Transactions8_Shift=crime_transactions(graphdf8,8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PdbGvRgrA0Q"
      },
      "source": [
        "**RESTRUCTURING THE NEIGHBORHOOD TRANSACTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YlQ9t8nrJ9w"
      },
      "source": [
        "def neighborhood_transactions(path,sdf,matrixdf):\n",
        "    df= pd.read_csv(path)\n",
        "    df=df.rename(columns={\"y_last\": \"Actual\", \"yhat_last\": \"Predicted\"})\n",
        "    a=np.array(sdf['S1'],dtype=int)\n",
        "    b=np.array(sdf['S2'],dtype=int)\n",
        "    nodelist=np.unique(np.append(a,b))\n",
        "    rep_num = int((len(df)/len(nodelist)))\n",
        "    nodes=np.tile(nodelist,rep_num)\n",
        "    df['Segment']=nodes\n",
        "    dict1={}\n",
        "    for key,value in matrixdf.items():\n",
        "        nlist=[]\n",
        "        nlist.append(key)\n",
        "        for keys,values in value.items():\n",
        "            nlist.append(keys)\n",
        "        a=str(key)+'neighbor'\n",
        "        dict1[a]=nlist\n",
        "    dict2={}\n",
        "    df['Segment']=df['Segment'].astype(str)\n",
        "    for key,value in dict1.items():\n",
        "        a=key\n",
        "        dict2[a]=df[df.Segment.isin(value)]\n",
        "    dict3={}\n",
        "    ind_value=np.arange(1,(rep_num+1),1)\n",
        "    for key,value in dict2.items():\n",
        "        value['Segment'] = pd.to_numeric(value['Segment'])\n",
        "        a=list(sorted(np.unique(value.Segment)))\n",
        "        append_act = '_actual'\n",
        "        append_pre='_predicted'\n",
        "        b1 = [str(sub) + append_act for sub in a]\n",
        "        b2 = [str(sub) + append_pre for sub in a]\n",
        "        b3 = [None]*(len(b1)+len(b2))\n",
        "        b3[::2] = b1\n",
        "        b3[1::2] = b2\n",
        "        ndf=value[['Actual','Predicted']]\n",
        "        e=ndf.values.reshape(rep_num,-1)\n",
        "        nndf=pd.DataFrame(data=e,index=ind_value,columns=b3)\n",
        "        d=key\n",
        "        dict3[d]=nndf\n",
        "    return dict3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_HouaUrrNhJ"
      },
      "source": [
        "**CALCULATING THE MEAN HIT RATE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7HogSNkrTnb"
      },
      "source": [
        "from functools import partial, reduce\n",
        "from statistics import mean\n",
        "import math\n",
        "def calc_MHR(transactions_dict,transaction_df,segments_df):\n",
        "    ##transaction_dict is the output of neighborhood transaction,\n",
        "    ##transaction df is the output of crime_transactions function\n",
        "    ##segments_df is the output of create_neighborhood_graph function\n",
        "    my_reduce = partial(pd.merge, left_index=True, right_index=True)\n",
        "    a=reduce(my_reduce, transactions_dict.values())\n",
        "    d=a.filter(regex=(\"_predicted_x\"))\n",
        "    d.columns = d.columns.str.rstrip('_predicted_x')\n",
        "    d=d.iloc[:,~d.columns.duplicated()]\n",
        "    d.columns=d.columns.astype(int)\n",
        "    e = [int(numeric_string) for numeric_string in np.array(d.columns)]\n",
        "    f=sorted(e)\n",
        "    g=d[f]\n",
        "    actual_values=transaction_df.iloc[-(len(g)):,]\n",
        "    un1=np.unique(segments_df['S1'])\n",
        "    un2=np.unique(segments_df['S2'])\n",
        "    un3=np.unique(np.concatenate((un1,un2)))\n",
        "    gchi_segments['Segment_ID']=gchi_segments['Segment_ID'].astype(str)\n",
        "    un4= gchi_segments[gchi_segments.Segment_ID.isin(un3)]\n",
        "    slengths=un4[['Segment_ID','shape_len']]\n",
        "    pred_dict={}\n",
        "    for (index1, row1),(index2,row2) in zip(g.iterrows(),actual_values.iterrows()):\n",
        "        d=np.array(row1)\n",
        "        e=np.array(row2)\n",
        "        key=str(str(index1)+'.Day')\n",
        "        nname=str(str(index1)+'slengths')\n",
        "        nname=slengths.copy()\n",
        "        nname['prediction']=d\n",
        "        nname['actual']=e\n",
        "        pred_dict[index1]=nname\n",
        "    hrs=0\n",
        "    hr_dict={}\n",
        "    hr_list=[]\n",
        "    for key,value in pred_dict.items():\n",
        "        value['Risk_Rank'] = value['prediction'].rank(ascending = 0)\n",
        "        #value['Risk_Rank'] = pd.to_numeric(value['Risk_Rank'])\n",
        "        value = value.set_index('Risk_Rank')\n",
        "        value = value.sort_index()\n",
        "        pred_segs=[]\n",
        "        a=0\n",
        "        for index, row in value.iterrows():\n",
        "            a+=row.shape_len\n",
        "            if a>(0.20*value['shape_len'].sum()):\n",
        "                break\n",
        "            else:\n",
        "               pred_segs.append(row.Segment_ID)\n",
        "        selected_segments=value[value.Segment_ID.isin(pred_segs)]\n",
        "        #hitsegments=len(selected_segments[(selected_segments['actual']!=0)])\n",
        "        #allcrsegments=(len(value[(value['actual']!=0)]))\n",
        "        hitsegments=selected_segments['actual'].sum()\n",
        "        allcrsegments=value['actual'].sum()\n",
        "        hr=hitsegments/allcrsegments\n",
        "        hr_list.append(hr)\n",
        "        keys=str(str(key)+'.HR')\n",
        "        hr_dict[keys]=hr\n",
        "        hrs+=hr\n",
        "    nhrlist = [x for x in hr_list if math.isnan(x) == False]\n",
        "    mhr=mean(nhrlist)\n",
        "    return hr_dict,mhr,pred_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaba2Xvqrs5g"
      },
      "source": [
        "**WRITING THE RESULTS TO A DICTIONARY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh0arMcPrjyE",
        "outputId": "9892b422-5966-4c7c-bf79-fe1304455298"
      },
      "source": [
        "graph8=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/CenterSide/Graphs/Center01graph.pickle')\n",
        "matrix8= nx.convert.to_dict_of_dicts(graph8)\n",
        "paths=(\"/content/gdrive/MyDrive/Robbery_Daily_experiment\")\n",
        "graphdf8=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/CenterSide/Graphs/graphCdf.pkl')\n",
        "def write_to_dict(path,TransactionDF,segmentsDF,matrix_dict):\n",
        "  import glob\n",
        "  all_files = glob.glob(paths + \"/*.csv\")\n",
        "  mhr_values={}\n",
        "  for m in all_files:\n",
        "      df1=neighborhood_transactions(m,graphdf8,matrix8)\n",
        "      a,b,c=calc_MHR(df1,RobberyTable_daily,graphdf8)\n",
        "      key=str(\"mhr\"+m.split('/')[-1])\n",
        "      mhr_values[key]=b\n",
        "  import csv\n",
        "  a_file = open('/content/gdrive/MyDrive/MainFiles/Baselines/Robbery/STGCN/WithoutEX/Seed Tuning/Daily/Robbery_daily_best.csv', 'w',newline=\"\")\n",
        "  writer = csv.writer(a_file)\n",
        "  for key, value in mhr_values.items():\n",
        "      writer.writerow([key, value])\n",
        "  a_file.close()\n",
        "write_to_dict(paths,RobberyTable_daily,graphdf8,matrix8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ]
    }
  ]
}