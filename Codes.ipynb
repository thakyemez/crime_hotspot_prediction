{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thakyemez/crime_hotspot_prediction/blob/main/Codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OBb6UhJSovE",
        "outputId": "6dfbebe9-752a-4839-864b-9ac25fa6d3ad"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXSe_QlEqOzN"
      },
      "source": [
        "**PREPARING THE DATASETS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD2Jl1Kg4xdb"
      },
      "source": [
        "**Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k-fdL7E43Ev"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "import torch.nn as nn\n",
        "from torch.nn import BatchNorm2d, Conv1d, Conv2d, ModuleList, Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from fastprogress import progress_bar\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install durbango\n",
        "!pip install funcy\n",
        "!pip install py3nvml\n",
        "from durbango import pickle_save\n",
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlooOLin41FN"
      },
      "source": [
        "**Preparing the Crime Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqjHnOUJc6RH"
      },
      "source": [
        "##Theft\n",
        "TheftTable = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/Transactions/ CenterTheftTable.csv')\n",
        "a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "TheftTable.index=a\n",
        "#TheftTable=TheftTable.drop(columns=['Time_ID'])\n",
        "TheftTable=TheftTable.drop(columns=['Unnamed: 0'])\n",
        "TheftTable_Smoothed=TheftTable.ewm(alpha=0.1).mean()\n",
        "##Aggregating at day level\n",
        "TheftTable_daily = TheftTable.resample('D').sum()\n",
        "TheftTable_daily.sum()\n",
        "TheftTable_daily.head()\n",
        "TheftTable_daily.to_csv('/content/gdrive/MyDrive/transactions.csv')\n",
        "TheftTable_daily_Smoothed=TheftTable_daily.ewm(alpha=0.05).mean()\n",
        "TheftTable_daily_Smoothed.head()\n",
        "##Robbery\n",
        "RobberyTable = pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/Transactions/ CenterRobberyTable.csv')\n",
        "a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "RobberyTable.index=a\n",
        "RobberyTable=RobberyTable.drop(columns=['Time_ID'])\n",
        "RobberyTable=RobberyTable.drop(columns=['Unnamed: 0'])\n",
        "RobberyTable_Smoothed=RobberyTable.ewm(alpha=0.1).mean()\n",
        "##Aggregating at day level\n",
        "RobberyTable_daily = RobberyTable.resample('D').sum()\n",
        "RobberyTable_daily.sum()\n",
        "RobberyTable_daily.head()\n",
        "RobberyTable_daily.to_csv('/content/gdrive/MyDrive/transactions.csv')\n",
        "RobberyTable_daily_Smoothed=RobberyTable_daily.ewm(alpha=0.05).mean()\n",
        "RobberyTable_daily_Smoothed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-YYexBA1PSK"
      },
      "source": [
        "**Preparing the adjancency matrices for CS network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l5mxtONwpn7"
      },
      "source": [
        "##Preparing the Center graph\n",
        "import glob\n",
        "paths=(\"/content/gdrive/MyDrive/MainFiles/CenterSide/Graphs\")\n",
        "all_files = glob.glob(paths + \"/*.pickle\")\n",
        "for m in all_files:\n",
        "  Cgraph=pd.read_pickle(m)\n",
        "  nodescaC=np.array(TheftTable_daily_Smoothed.columns)\n",
        "  nodescaC_to_ind=np.arange(0,2459,1)\n",
        "  ntemp=nx.convert_matrix.to_numpy_matrix(Cgraph,nodelist=nodescaC,weight='weight')\n",
        "  name=str(m.split('/')[-1]).split('.')[0]\n",
        "  ext= '.pkl'\n",
        "  with open(\"/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix/%s%s\" %(name,ext),'wb') as f:\n",
        "    pickle.dump([nodescaC,nodescaC_to_ind,ntemp], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxjeDtybqdxG"
      },
      "source": [
        "**GENERATE TRAINING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ7wkNAID_A7"
      },
      "source": [
        "def generate_graph_seq2seq_io_data(\n",
        "        df, x_offsets, y_offsets, add_time_in_day=True, add_day_in_week=False, scaler=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate samples from\n",
        "    :param df:\n",
        "    :param x_offsets:\n",
        "    :param y_offsets:\n",
        "    :param add_time_in_day:\n",
        "    :param add_day_in_week:\n",
        "    :param scaler:\n",
        "    :return:\n",
        "    # x: (epoch_size, input_length, num_nodes, input_dim)\n",
        "    # y: (epoch_size, output_length, num_nodes, output_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples, num_nodes = df.shape\n",
        "    data = np.expand_dims(df.values, axis=-1)\n",
        "    print(\"shape_of_data: \", data.shape)\n",
        "    feature_list = [data]\n",
        "\n",
        "    if add_time_in_day:\n",
        "        time_ind = (df.index.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
        "        time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "        feature_list.append(time_in_day)\n",
        "    if add_day_in_week:\n",
        "        dow = df.index.dayofweek\n",
        "        #Trying Weekend/Weekday distinction\n",
        "        dow2=dow.copy()\n",
        "        dow2=[1 if (x==5 or x==6) else 0  for x in dow2]\n",
        "        dow=np.array(dow2)\n",
        "        dow_tiled = np.tile(dow, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "        feature_list.append(dow_tiled)\n",
        "\n",
        "    ##Adding park events as an external variable\n",
        "    ex_park_events=ParkEvents_Smoothed.values.reshape(num_samples, num_nodes,1)\n",
        "    feature_list.append(ex_park_events)\n",
        "    data = np.concatenate(feature_list, axis=-1)\n",
        "    print(\"shape_of_data: \", data.shape)\n",
        "    x, y = [], []\n",
        "    min_t = abs(min(x_offsets))\n",
        "    print(\"min_t \", min_t)\n",
        "    max_t = abs(num_samples - abs(max(y_offsets)))\n",
        "    print(\"max_t\", max_t)\n",
        "    for t in range(min_t, max_t):  # t is the index of the last observation. The loop iterates between 11 and 2180.\n",
        "        x.append(data[t + x_offsets, ...])\n",
        "        y.append(data[t + y_offsets, ...])\n",
        "    print(\"x before stack \", x[0].shape)\n",
        "    x = np.stack(x, axis=0)\n",
        "    print(\"shape of x \", x.shape)\n",
        "    print(\"y before stack \", y[0].shape)\n",
        "    y = np.stack(y, axis=0)\n",
        "    print(\"shape of y \", y.shape)\n",
        "    return x, y\n",
        "\n",
        "def generate_train_val_test(args):\n",
        "    seq_length_x, seq_length_y = args.seq_length_x, args.seq_length_y\n",
        "    df = args.traffic_df_filename\n",
        "    # 0 is the latest observed sample.\n",
        "    x_offsets = np.sort(np.concatenate((np.arange(-(seq_length_x - 1), 1, 1),)))\n",
        "    print(\"shape of x_offsets \", x_offsets.shape)\n",
        "    print(\"x_offsets \",x_offsets)\n",
        "    # Predict the next one hour\n",
        "    y_offsets = np.sort(np.arange(args.y_start, (seq_length_y + 1), 1))\n",
        "    print(\"shape of y_offsets \", y_offsets.shape)\n",
        "    print(\"y_offsets \",y_offsets)\n",
        "    # x: (num_samples, input_length, num_nodes, input_dim)\n",
        "    # y: (num_samples, output_length, num_nodes, output_dim)\n",
        "    x, y = generate_graph_seq2seq_io_data(\n",
        "        df,\n",
        "        x_offsets=x_offsets,\n",
        "        y_offsets=y_offsets,\n",
        "        add_time_in_day=False,\n",
        "        add_day_in_week=args.dow,\n",
        "    )\n",
        "    print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
        "    # Write the data into npz file.\n",
        "    num_samples = x.shape[0]\n",
        "    num_test = round(num_samples * 0.2)\n",
        "    num_train = round(num_samples * 0.7)\n",
        "    num_val = num_samples - num_test - num_train\n",
        "    x_train, y_train = x[:num_train], y[:num_train]\n",
        "    x_val, y_val = (\n",
        "        x[num_train: num_train + num_val],\n",
        "        y[num_train: num_train + num_val],\n",
        "    )\n",
        "    x_test, y_test = x[-num_test:], y[-num_test:]\n",
        "\n",
        "    for cat in [\"train\", \"val\", \"test\"]:\n",
        "        _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
        "        print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
        "        np.savez_compressed(\n",
        "            os.path.join(args.output_dir, f\"{cat}.npz\"),\n",
        "            x=_x,\n",
        "            y=_y,\n",
        "            x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
        "            y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
        "        )\n",
        "    print(\"shape of x_offsets \", x_offsets.shape)\n",
        "    print(\"shape of y_offsets \", y_offsets.shape)\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"/content/gdrive/MyDrive/Robbery_Daily_data/\", help=\"Output directory.\")\n",
        "    parser.add_argument(\"--traffic_df_filename\", type=str, default=RobberyTable_daily_Smoothed, help=\" Daily crime risk scores\",)\n",
        "    parser.add_argument(\"--seq_length_x\", type=int, default=42, help=\"Sequence Length.\",)\n",
        "    parser.add_argument(\"--seq_length_y\", type=int, default=1, help=\"Sequence Length.\",)\n",
        "    parser.add_argument(\"--y_start\", type=int, default=1, help=\"Y pred start\", )\n",
        "    parser.add_argument(\"--dow\", action='store_true',)\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    if os.path.exists(args.output_dir):\n",
        "        reply = str(input(f'{args.output_dir} exists. Do you want to overwrite it? (y/n)')).lower().strip()\n",
        "        if reply[0] != 'y': exit\n",
        "    else:\n",
        "        os.makedirs(args.output_dir)\n",
        "    generate_train_val_test(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRAPHWAVENET**"
      ],
      "metadata": {
        "id": "j3eYF0qQZRxC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgw10fGuDvS"
      },
      "source": [
        "**TRAINING TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w88fsxvYuIWK"
      },
      "source": [
        "hdir=\"/content/gdrive/MyDrive/\"\n",
        "best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "def main(args, **model_kwargs):\n",
        "    device = torch.device(args.device)\n",
        "    data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "    scaler = data['scaler']\n",
        "    aptinit, supports = make_graph_inputs(args, device)\n",
        "    torch.manual_seed(7)\n",
        "    model = GWNet.from_args(args, device, supports, aptinit, **model_kwargs)\n",
        "    if args.checkpoint:\n",
        "\n",
        "      model.load_checkpoint(torch.load(args.checkpoint))\n",
        "    model.to(device)\n",
        "    engine = Trainer.from_args(model, scaler, args)\n",
        "    metrics = []\n",
        "    best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "    lowest_mae_yet = 100  # high value, will get overwritten\n",
        "    mb = progress_bar(list(range(1, args.epochs + 1)))\n",
        "    epochs_since_best_mae = 0\n",
        "    for _ in mb:\n",
        "        train_loss, train_mape, train_rmse = [], [], []\n",
        "        data['train_loader'].shuffle()\n",
        "        for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\n",
        "          trainx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "          trainy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "          yspeed = trainy[:, 0, :, :]\n",
        "          if yspeed.max() == 0: continue\n",
        "              #print('tra覺nx',trainx.shape)\n",
        "              #print('tra覺ny',trainy.shape)\n",
        "              #print('yspeed',yspeed.shape)\n",
        "          mae, mape, rmse = engine.train(trainx, yspeed)\n",
        "          train_loss.append(mae)\n",
        "          train_mape.append(mape)\n",
        "          train_rmse.append(rmse)\n",
        "          if args.n_iters is not None and iter >= args.n_iters:\n",
        "              break\n",
        "        engine.scheduler.step()\n",
        "        _, valid_loss, valid_mape, valid_rmse = eval_(data['val_loader'], device, engine)\n",
        "        m = dict(train_loss=np.mean(train_loss), train_mape=np.mean(train_mape),\n",
        "                      train_rmse=np.mean(train_rmse), valid_loss=np.mean(valid_loss),\n",
        "                        valid_mape=np.mean(valid_mape), valid_rmse=np.mean(valid_rmse))\n",
        "\n",
        "        m = pd.Series(m)\n",
        "        metrics.append(m)\n",
        "        if m.valid_loss < lowest_mae_yet:\n",
        "            torch.save(engine.model.state_dict(), best_model_save_path)\n",
        "            lowest_mae_yet = m.valid_loss\n",
        "            epochs_since_best_mae = 0\n",
        "        else:\n",
        "            epochs_since_best_mae += 1\n",
        "        met_df = pd.DataFrame(metrics)\n",
        "        mb.comment = f'best val_loss: {met_df.valid_loss.min(): .3f}, current val_loss: {m.valid_loss:.3f}, current train loss: {m.train_loss: .3f}'\n",
        "        met_df.round(6).to_csv(f'{hdir}/metrics.csv')\n",
        "        if epochs_since_best_mae >= args.es_patience: break\n",
        "        # Metrics on test data\n",
        "    with open(best_model_save_path, 'rb') as f:\n",
        "      buffer = io.BytesIO(f.read())\n",
        "    engine.model.load_state_dict(torch.load(buffer))\n",
        "    realy = torch.Tensor(data['y_test']).transpose(1, 3)[:, 0, :, :].to(device)\n",
        "    test_met_df, yhat = calc_tstep_metrics(engine.model, device, data['test_loader'], scaler, realy, args.seq_length)\n",
        "    test_met_df.round(12).to_csv(os.path.join(hdir, 'test_metrics.csv'))\n",
        "\n",
        "def eval_(ds, device, engine):\n",
        "    \"\"\"Run validation.\"\"\"\n",
        "    valid_loss = []\n",
        "    valid_mape = []\n",
        "    valid_rmse = []\n",
        "    s1 = time.time()\n",
        "    for (x, y) in ds.get_iterator():\n",
        "        testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "        testy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "        metrics = engine.eval(testx, testy[:, 0, :, :])\n",
        "        valid_loss.append(metrics[0])\n",
        "        valid_mape.append(metrics[1])\n",
        "        valid_rmse.append(metrics[2])\n",
        "    total_time = time.time() - s1\n",
        "    return total_time, valid_loss, valid_mape, valid_rmse\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser =get_shared_arg_parser()\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='')\n",
        "    parser.add_argument('--clip', type=int, default=3, help='Gradient Clipping')\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.005, help='weight decay rate')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
        "    parser.add_argument('--lr_decay_rate', type=float, default=0.935, help='learning rate')\n",
        "    parser.add_argument('--save', type=str, default='Robbery_Daily_experiment', help='save path')\n",
        "    parser.add_argument('--n_iters', default=None, help='quit after this many iterations')\n",
        "    parser.add_argument('--es_patience', type=int, default=20, help='quit if no improvement after this many iterations')\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    t1 = time.time()\n",
        "    if not os.path.exists(hdir):\n",
        "        os.mkdir(hdir)\n",
        "    pickle_save(args,'/content/gdrive/MyDrive/args.pkl')\n",
        "    main(args)\n",
        "    t2 = time.time()\n",
        "    mins = (t2 - t1) / 60\n",
        "    print(f\"Total time spent: {mins:.2f} seconds\")\n",
        "    #pref=m.split('/')[-1]\n",
        "    #pp=p.split('/')[-1].split('.')[0].split('graph')[0].split('Center')[1]\n",
        "    #key=str(\"experiment:\"+\"-\"+ str(k)+\"-\"+str(l)+\"-\"+str(n)+\".csv\")\n",
        "    key=str(\"Robbery_Daily_experiment:\"+\".csv\")\n",
        "\n",
        "    epath=('/content/gdrive/MyDrive/Robbery_Daily_experiment/%s'%(key))\n",
        "def main(args, save_pred_path=epath, save_metrics_path='last_test_metrics_Robbery_preds_10day_dayca8_smoothingcoef05.csv', loader='test', **model_kwargs):\n",
        "    args.checkpoint= best_model_save_path\n",
        "    device = torch.device(args.device)\n",
        "    adjinit, supports = make_graph_inputs(args, device)\n",
        "    model = GWNet.from_args(args, device, supports, adjinit,**model_kwargs)\n",
        "    model.load_state_dict(torch.load(args.checkpoint))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print('model loaded successfully')\n",
        "    data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "    scaler = data['scaler']\n",
        "    realy = torch.Tensor(data[f'y_{loader}']).to(device)\n",
        "    realy = realy.transpose(1,3)[:,0,:,:]\n",
        "    met_df, yhat = calc_tstep_metrics(model, device, data[f'{loader}_loader'], scaler, realy, args.seq_length)\n",
        "    df2 = make_pred_df(realy, yhat, scaler, args.seq_length)\n",
        "    met_df.to_csv(save_metrics_path)\n",
        "    df2.to_csv(save_pred_path, index=False)\n",
        "    if args.plotheatmap:\n",
        "      plot_learned_adj_matrix(model)\n",
        "    return met_df, df2\n",
        "\n",
        "def plot_learned_adj_matrix(model):\n",
        "    adp = F.softmax(F.relu(torch.mm(model.nodevec1, model.nodevec2)), dim=1)\n",
        "    adp = adp.cpu().detach().numpy()\n",
        "    adp = adp / np.max(adp)\n",
        "    df = pd.DataFrame(adp)\n",
        "    sns.heatmap(df, cmap=\"RdYlBu\")\n",
        "    plt.savefig(\"/content/gdrive/MyDrive/heatmap.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = get_shared_arg_parser()\n",
        "    parser.add_argument('--checkpoints', type=str, help='')\n",
        "    parser.add_argument('--plotheatmap', action='store_true')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBmTw98sqj8T"
      },
      "source": [
        "**UTIL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgX34OkAUa3w"
      },
      "source": [
        "DEFAULT_DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "        if pad_with_last_sample:\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        #print(\"number of batches\",self.num_batch)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                yield (x_i, y_i)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "\n",
        "class StandardScaler():\n",
        "\n",
        "    def __init__(self, mean, std, fill_zeroes=True):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.fill_zeroes = fill_zeroes\n",
        "\n",
        "    def transform(self, data):\n",
        "        if self.fill_zeroes:\n",
        "            mask = (data == 0)\n",
        "            data[mask] = self.mean\n",
        "        return (data - self.mean) / self.std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "\n",
        "\n",
        "def sym_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n",
        "\n",
        "def asym_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "def calculate_normalized_laplacian(adj):\n",
        "    \"\"\"\n",
        "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
        "    # D = diag(A 1)\n",
        "    :param adj:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    d = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "    return normalized_laplacian\n",
        "\n",
        "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
        "    if undirected:\n",
        "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
        "    L = calculate_normalized_laplacian(adj_mx)\n",
        "    if lambda_max is None:\n",
        "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
        "        lambda_max = lambda_max[0]\n",
        "    L = sp.csr_matrix(L)\n",
        "    M, _ = L.shape\n",
        "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
        "    L = (2 / lambda_max * L) - I\n",
        "    return L.astype(np.float32).todense()\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        #print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "ADJ_CHOICES = ['scalap', 'normlap', 'symnadj', 'transition', 'identity']\n",
        "def load_adj(pkl_filename, adjtype):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n",
        "    if adjtype == \"scalap\":\n",
        "        adj = [calculate_scaled_laplacian(adj_mx)]\n",
        "    elif adjtype == \"normlap\":\n",
        "        adj = [calculate_normalized_laplacian(adj_mx).astype(np.float32).todense()]\n",
        "    elif adjtype == \"symnadj\":\n",
        "        #print('sym')\n",
        "        adj = [sym_adj(adj_mx)]\n",
        "    elif adjtype == \"transition\":\n",
        "        adj = [asym_adj(adj_mx)]\n",
        "    elif adjtype == \"doubletransition\":\n",
        "        adj = [asym_adj(adj_mx), asym_adj(np.transpose(adj_mx))]\n",
        "    elif adjtype == \"identity\":\n",
        "        adj = [np.diag(np.ones(adj_mx.shape[0])).astype(np.float32)]\n",
        "    else:\n",
        "        error = 0\n",
        "        assert error, \"adj type not defined\"\n",
        "    #print(\"sensor_ids\",sensor_ids)\n",
        "    #print(\"sensor_id_to_ind\",sensor_id_to_ind)\n",
        "    return sensor_ids, sensor_id_to_ind, adj\n",
        "\n",
        "\n",
        "def load_dataset(dataset_dir, batch_size, valid_batch_size=None, test_batch_size=None, n_obs=None, fill_zeroes=True):\n",
        "    data = {}\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
        "        data['x_' + category] = cat_data['x']\n",
        "        data['y_' + category] = cat_data['y']\n",
        "        if n_obs is not None:\n",
        "            data['x_' + category] = data['x_' + category][:n_obs]\n",
        "            data['y_' + category] = data['y_' + category][:n_obs]\n",
        "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std(), fill_zeroes=fill_zeroes)\n",
        "    # Data format\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "    data['train_loader'] = DataLoader(data['x_train'], data['y_train'], batch_size)\n",
        "    data['val_loader'] = DataLoader(data['x_val'], data['y_val'], valid_batch_size)\n",
        "    data['test_loader'] = DataLoader(data['x_test'], data['y_test'], test_batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "def calc_metrics(preds, labels,device='cuda:0', null_val=0.):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels != null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean(mask)\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    #print(\"mask shape\",mask.shape)\n",
        "    mse = (preds - labels) ** 2\n",
        "    #print(\"preds.shape\",preds.shape)\n",
        "    #print(\"labels.shape\",preds.shape)\n",
        "    #print(\"mse shape\",mse.shape)\n",
        "    #print(\"mse\",mse)\n",
        "    #mae = (torch.abs(preds-labels)) # original mae\n",
        "    #print(\"mae shape before mask\",mae.shape)\n",
        "    #print(\"mae type before mask\", type(mae.shape))\n",
        "    ##adapted mae\n",
        "    if preds.shape[0]==args.batch_size:\n",
        "      fpreds=torch.flatten(preds).to(device).cpu().data.numpy()\n",
        "      flabels=torch.flatten(labels).to(device).cpu().data.numpy()\n",
        "      #print(\"flabels.shape\",fpreds.shape)\n",
        "      #c=[abs(x-y)*y if y >0 else abs(x-y)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Trying MSE  ##Trying MSE yielded inferior performance\n",
        "      ##c=[((x-y)**2)*y if y >0 else ((x-y)**2)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Quantile loss\n",
        "      c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds,flabels)]\n",
        "      a=np.array(c)\n",
        "      b=a.reshape(-1,2459,1)\n",
        "      mae=torch.tensor([b],requires_grad=True).to(device)\n",
        "    else:\n",
        "      d=preds.shape[0]\n",
        "      fpreds=torch.flatten(preds).to(device).cpu().data.numpy()\n",
        "      flabels=torch.flatten(labels).to(device).cpu().data.numpy()\n",
        "      #print(\"flabels.shape\",fpreds.shape)\n",
        "      #c=[abs(x-y)*y if y >0 else abs(x-y)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Trying MSE yielded inferior performance\n",
        "      #c=[((x-y)**2)*y if y >0 else ((x-y)**2)*(10**-100)  for x,y in zip(fpreds,flabels)]\n",
        "      ##Quantile loss\n",
        "      c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds,flabels)]\n",
        "      a=np.array(c)\n",
        "      b=a.reshape(d,2459)\n",
        "      mae=torch.tensor([b],requires_grad=True).to(device)\n",
        "    #print(\"mae train\",mae)\n",
        "    #print(\"preds shape in calc_metrics\",preds.shape)\n",
        "    #print(\"labels shape in calc_metrics\",labels.shape)\n",
        "    mape = mae / labels\n",
        "    #print(\"mae before mask\",mae)\n",
        "    #print(\"mae mean before mask\",mae.mean)\n",
        "    mae, mape, mse = [mask_and_fillna(l, mask) for l in [mae, mape, mse]]\n",
        "    #print(\"mae after mask\",mae)\n",
        "    rmse = torch.sqrt(mse)\n",
        "    return mae, mape, rmse\n",
        "\n",
        "\n",
        "def mask_and_fillna(loss, mask):\n",
        "    loss = loss * mask\n",
        "    #print(\"shape of loss\", loss.shape)\n",
        "    #print(\"shape of mask\", mask.shape)\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "\n",
        "def calc_tstep_metrics(model, device, test_loader, scaler, realy, seq_length) -> pd.DataFrame:\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for _, (x, __) in enumerate(test_loader.get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "        #print(\"testx.shape\",testx.shape)\n",
        "        with torch.no_grad():\n",
        "            preds = model(testx).transpose(1, 3)\n",
        "            #print(\"preds.shape\",preds.shape)\n",
        "        outputs.append(preds.squeeze(1))\n",
        "    yhat = torch.cat(outputs, dim=0)[:realy.size(0), ...]\n",
        "    #print(\"yhat.shape in calc tstep metrics\",yhat.shape)\n",
        "    test_met = []\n",
        "\n",
        "    for i in range(seq_length):\n",
        "        pred = scaler.inverse_transform(yhat[:, :, i])\n",
        "        pred = torch.clamp(pred, min=0., max=70.)\n",
        "        real = realy[:, :, i]\n",
        "        test_met.append([x.item() for x in calc_metrics(pred, real)])\n",
        "    test_met_df = pd.DataFrame(test_met, columns=['mae', 'mape', 'rmse']).rename_axis('t')\n",
        "    #print(\"test_met_df.shape\",test_met_df.shape)\n",
        "    return test_met_df, yhat\n",
        "\n",
        "\n",
        "def _to_ser(arr):\n",
        "    return pd.DataFrame(arr.cpu().detach().numpy()).stack().rename_axis(['obs', 'sensor_id'])\n",
        "\n",
        "\n",
        "def make_pred_df(realy, yhat, scaler, seq_length):\n",
        "    #print(\"realy.shape\",realy.shape)\n",
        "    #print(\"yhat.shape\",yhat.shape)\n",
        "    df = pd.DataFrame(dict(y_last=_to_ser(realy[:, :, seq_length - 1]),\n",
        "                           yhat_last=_to_ser(scaler.inverse_transform(yhat[:, :, seq_length - 1]))))\n",
        "                           #y_3=_to_ser(realy[:, :, 2]),\n",
        "                           #yhat_3=_to_ser(scaler.inverse_transform(yhat[:, :, 2]))))\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_graph_inputs(args, device):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_adj(args.adjdata, args.adjtype)\n",
        "    supports = [torch.tensor(i).to(device) for i in adj_mx]\n",
        "    aptinit = None if args.randomadj else supports[0]  # ignored without do_graph_conv and add_apt_adj\n",
        "    if args.aptonly:\n",
        "        if not args.addaptadj and args.do_graph_conv: raise ValueError(\n",
        "            'WARNING: not using adjacency matrix')\n",
        "        supports = None\n",
        "    return aptinit, supports\n",
        "def get_shared_arg_parser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--device', type=str, default='cuda:0' if torch.cuda.is_available() else 'cpu', help='')\n",
        "    parser.add_argument('--data', type=str, default=\"/content/gdrive/MyDrive/Robbery_Daily_data\", help='data path')\n",
        "    parser.add_argument('--adjdata', type=str, default='/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix/Center01graph.pkl',help='adj data path')\n",
        "    ##for grid search\n",
        "    #parser.add_argument('--data', type=str, default=m, help='data path')\n",
        "    #parser.add_argument('--adjdata', type=str, default=p,help='adj data path')\n",
        "    parser.add_argument('--adjtype', type=str, default='symnadj', help='adj type', choices=ADJ_CHOICES)\n",
        "    parser.add_argument('--do_graph_conv', action='store_true',\n",
        "                        help='whether to add graph convolution layer')\n",
        "    parser.add_argument('--aptonly', default=False, help='whether only adaptive adj')\n",
        "    parser.add_argument('--addaptadj', action='store_true', help='whether add adaptive adj')\n",
        "    parser.add_argument('--randomadj', action='store_true',\n",
        "                        help='whether random initialize adaptive adj')\n",
        "    parser.add_argument('--seq_length', type=int, default=1, help='')\n",
        "    parser.add_argument('--nhid', type=int, default=8, help='Number of channels for internal conv')\n",
        "    parser.add_argument('--in_dim', type=int, default=1, help='inputs dimension')\n",
        "    parser.add_argument('--num_nodes', type=int, default=2459, help='number of nodes')\n",
        "    parser.add_argument('--batch_size', type=int, default=16, help='batch size')\n",
        "    parser.add_argument('--dropout', type=float, default=0.3, help='dropout rate')\n",
        "    parser.add_argument('--n_obs', default=None, help='Only use this many observations. For unit testing.')\n",
        "    parser.add_argument('--apt_size', default=10, type=int)\n",
        "    parser.add_argument('--cat_feat_gc', action='store_true')\n",
        "    parser.add_argument('--fill_zeroes', action='store_true')\n",
        "    parser.add_argument('--checkpoint', type=str, help='')\n",
        "    return parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5l9QKP7rv5d"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8gmPuYtrznt"
      },
      "source": [
        "def nconv(x, A):\n",
        "    \"\"\"Multiply x by adjacency matrix along source node axis\"\"\"\n",
        "    return torch.einsum('ncvl,vw->ncwl', (x, A)).contiguous()\n",
        "\n",
        "class GraphConvNet(nn.Module):\n",
        "    def __init__(self, c_in, c_out, dropout, support_len=3, order=2):\n",
        "        super().__init__()\n",
        "        c_in = (order * support_len + 1) * c_in\n",
        "        self.final_conv = Conv2d(c_in, c_out, (1, 1), padding=(0, 0), stride=(1, 1), bias=True)\n",
        "        self.dropout = dropout\n",
        "        self.order = order\n",
        "\n",
        "    def forward(self, x, support: list):\n",
        "        out = [x]\n",
        "        #print(\"x shape in forward\",x.shape)\n",
        "        for a in support:\n",
        "            x1 = nconv(x, a)\n",
        "            out.append(x1)\n",
        "            for k in range(2, self.order + 1):\n",
        "                x2 = nconv(x1, a)\n",
        "                out.append(x2)\n",
        "                x1 = x2\n",
        "\n",
        "        h = torch.cat(out, dim=1)\n",
        "        h = self.final_conv(h)\n",
        "        h = F.dropout(h, self.dropout, training=self.training)\n",
        "        #print(\"hidden layer shape\",h.shape)\n",
        "        return h\n",
        "\n",
        "\n",
        "class GWNet(nn.Module):\n",
        "    def __init__(self, device, num_nodes,supports, dropout=0.3, do_graph_conv=True,\n",
        "                 addaptadj=True,aptinit=None, in_dim=1, out_dim=1,\n",
        "                 residual_channels=32, dilation_channels=32, cat_feat_gc=False,\n",
        "                 skip_channels=256, end_channels=512, kernel_size=2, blocks=7, layers=3,\n",
        "                 apt_size=10): #here we had to delete =none statement for supports and aptinit argument\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.blocks = blocks\n",
        "        self.layers = layers\n",
        "        self.do_graph_conv = do_graph_conv\n",
        "        self.cat_feat_gc = cat_feat_gc\n",
        "        self.addaptadj = addaptadj\n",
        "\n",
        "\n",
        "        if self.cat_feat_gc:\n",
        "            self.start_conv = nn.Conv2d(in_channels=1,  # hard code to avoid errors\n",
        "                                        out_channels=residual_channels,\n",
        "                                        kernel_size=(1, 1))\n",
        "\n",
        "            self.cat_feature_conv = nn.Conv2d(in_channels=in_dim - 1,\n",
        "                                              out_channels=residual_channels,\n",
        "                                              kernel_size=(1, 1))\n",
        "        else:\n",
        "            self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
        "                                        out_channels=residual_channels,\n",
        "                                        kernel_size=(1, 1))\n",
        "\n",
        "        self.fixed_supports = supports or []\n",
        "        receptive_field = 1\n",
        "\n",
        "        self.supports_len = len(self.fixed_supports)\n",
        "        if do_graph_conv and addaptadj:\n",
        "            if aptinit is None:\n",
        "                #print(\"randomnodevecs\")\n",
        "                nodevecs = torch.randn(num_nodes, apt_size), torch.randn(apt_size, num_nodes)\n",
        "            else:\n",
        "                #print(\"not random vecs\")\n",
        "                nodevecs = self.svd_init(apt_size, aptinit)\n",
        "            #print(\"shape nodevecs\",nodevecs.shape)\n",
        "            self.supports_len += 1\n",
        "            self.nodevec1, self.nodevec2 = [Parameter(n.to(device), requires_grad=True) for n in nodevecs]\n",
        "\n",
        "        depth = list(range(blocks * layers))\n",
        "\n",
        "        # 1x1 convolution for residual and skip connections (slightly different see docstring)\n",
        "        self.residual_convs = ModuleList([Conv1d(dilation_channels, residual_channels, (1, 1)) for _ in depth])\n",
        "        self.skip_convs = ModuleList([Conv1d(dilation_channels, skip_channels, (1, 1)) for _ in depth])\n",
        "        self.bn = ModuleList([BatchNorm2d(residual_channels) for _ in depth])\n",
        "        self.graph_convs = ModuleList([GraphConvNet(dilation_channels, residual_channels, dropout, support_len=self.supports_len)\n",
        "                                              for _ in depth])\n",
        "\n",
        "        self.filter_convs = ModuleList()\n",
        "        self.gate_convs = ModuleList()\n",
        "        for b in range(blocks):\n",
        "            additional_scope = kernel_size - 1\n",
        "            D = 1 # dilation\n",
        "            for i in range(layers):\n",
        "                # dilated convolutions\n",
        "                self.filter_convs.append(Conv2d(residual_channels, dilation_channels, (1, kernel_size), dilation=D))\n",
        "                self.gate_convs.append(Conv1d(residual_channels, dilation_channels, (1, kernel_size), dilation=D))\n",
        "                D *= 2\n",
        "                receptive_field += additional_scope\n",
        "                additional_scope *= 2\n",
        "        self.receptive_field = receptive_field\n",
        "        self.end_conv_1 = Conv2d(skip_channels, end_channels, (1, 1), bias=True)\n",
        "        self.end_conv_2 = Conv2d(end_channels, out_dim, (1, 1), bias=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def svd_init(apt_size, aptinit):\n",
        "        m, p, n = torch.linalg.svd(aptinit)\n",
        "        nodevec1 = torch.mm(m[:, :apt_size], torch.diag(p[:apt_size] ** 0.5))\n",
        "        nodevec2 = torch.mm(torch.diag(p[:apt_size] ** 0.5), n[:, :apt_size].t())\n",
        "        #print(\"nodevec1 shape\",nodevec1.shape)\n",
        "        #print(\"nodevec2 shape\",nodevec2.shape)\n",
        "        return nodevec1, nodevec2\n",
        "\n",
        "    @classmethod\n",
        "    def from_args(cls, args, device, supports, aptinit, **kwargs):\n",
        "        defaults = dict(dropout=args.dropout, supports=supports,\n",
        "                        do_graph_conv=args.do_graph_conv, addaptadj=args.addaptadj, aptinit=aptinit,\n",
        "                        in_dim=args.in_dim, apt_size=args.apt_size, out_dim=args.seq_length,\n",
        "                        residual_channels=args.nhid, dilation_channels=args.nhid,\n",
        "                        skip_channels=args.nhid * 8, end_channels=args.nhid * 16,\n",
        "                        cat_feat_gc=args.cat_feat_gc)\n",
        "        defaults.update(**kwargs)\n",
        "        model = cls(device, args.num_nodes, **defaults)\n",
        "        return model\n",
        "\n",
        "    def load_checkpoint(self, state_dict):\n",
        "        \"\"\"It is assumed that ckpt was trained to predict a subset of timesteps.\"\"\"\n",
        "        bk, wk = ['end_conv_2.bias', 'end_conv_2.weight']  # only weights that depend on seq_length\n",
        "        b, w = state_dict.pop(bk), state_dict.pop(wk)\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "        cur_state_dict = self.state_dict()\n",
        "        cur_state_dict[bk][:b.shape[0]] = b\n",
        "        cur_state_dict[wk][:w.shape[0]] = w\n",
        "        self.load_state_dict(cur_state_dict)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('forward覺n bas覺', x.shape)\n",
        "        # Input shape is (bs, features, n_nodes, n_timesteps)\n",
        "        in_len = x.size(3)\n",
        "        if in_len < self.receptive_field:\n",
        "            x = nn.functional.pad(x, (self.receptive_field - in_len, 0, 0, 0))\n",
        "            #print('if receptive_field',x.shape)\n",
        "        if self.cat_feat_gc:\n",
        "            f1, f2 = x[:, [0]], x[:, 1:]\n",
        "            x1 = self.start_conv(f1)\n",
        "            x2 = F.leaky_relu(self.cat_feature_conv(f2))\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            x = self.start_conv(x)\n",
        "            #print('if start conv',x.shape)\n",
        "        skip = 0\n",
        "        adjacency_matrices = self.fixed_supports\n",
        "        # calculate the current adaptive adj matrix once per iteration\n",
        "        if self.addaptadj:\n",
        "            adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)\n",
        "            adjacency_matrices = self.fixed_supports + [adp]\n",
        "\n",
        "        # WaveNet layers\n",
        "        for i in range(self.blocks * self.layers):\n",
        "            # EACH BLOCK\n",
        "\n",
        "            #            |----------------------------------------|     *residual*\n",
        "            #            |                                        |\n",
        "            #            |   |-dil_conv -- tanh --|                |\n",
        "            #         ---|                  * ----|-- 1x1 -- + -->\t*x_in*\n",
        "            #                |-dil_conv -- sigm --|    |\n",
        "            #                                         1x1\n",
        "            #                                          |\n",
        "            # ---------------------------------------> + ------------->\t*skip*\n",
        "            residual = x\n",
        "            # dilated convolution\n",
        "            filter = torch.tanh(self.filter_convs[i](residual))\n",
        "            gate = torch.sigmoid(self.gate_convs[i](residual))\n",
        "            x = filter * gate\n",
        "            #print('filter*gate',x.shape)\n",
        "            # parametrized skip connection\n",
        "            s = self.skip_convs[i](x)  # what are we skipping??\n",
        "            try:  # if i > 0 this works\n",
        "                skip = skip[:, :, :,  -s.size(3):]  # TODO(SS): Mean/Max Pool?\n",
        "            except:\n",
        "                skip = 0\n",
        "            skip = s + skip\n",
        "            if i == (self.blocks * self.layers - 1):  # last X getting ignored anyway\n",
        "                break\n",
        "\n",
        "            if self.do_graph_conv:\n",
        "                graph_out = self.graph_convs[i](x, adjacency_matrices)\n",
        "                x = x + graph_out if self.cat_feat_gc else graph_out\n",
        "                #print('if do graph conv',x.shape)\n",
        "            else:\n",
        "                x = self.residual_convs[i](x)\n",
        "                #print('if residual convs',x.shape)\n",
        "            x = x + residual[:, :, :, -x.size(3):]  # TODO(SS): Mean/Max Pool?\n",
        "            #print('x + residual[:, :, :, -x.size(3):]',x.shape)\n",
        "            x = self.bn[i](x)\n",
        "            #print('self.bn[i](x)',x.shape)\n",
        "\n",
        "        x = F.relu(skip)  # ignore last X?\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        #print('relu and end.conv1',x.shape)\n",
        "        x = self.end_conv_2(x)  # downsample to (bs, seq_length, 1804, nfeatures)\n",
        "        #print(\"x in forward\",x.shape)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OIzqVZ7rb9y"
      },
      "source": [
        "**ENGINE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HCuMURWrj4L"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model: GWNet, scaler, lrate, wdecay, clip=3, lr_decay_rate=.97):\n",
        "        self.model = model\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "        self.scaler = scaler\n",
        "        self.clip = clip\n",
        "        self.scheduler = optim.lr_scheduler.LambdaLR(\n",
        "            self.optimizer, lr_lambda=lambda epoch: lr_decay_rate ** epoch)\n",
        "\n",
        "    @classmethod\n",
        "    def from_args(cls, model, scaler, args):\n",
        "        return cls(model, scaler, args.learning_rate, args.weight_decay, clip=args.clip,\n",
        "                   lr_decay_rate=args.lr_decay_rate)\n",
        "\n",
        "    def train(self, input, real_val):\n",
        "       # print('eng覺ne tra覺n bas覺',input.shape)\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        input = nn.functional.pad(input,(1,0,0,0))\n",
        "        #print(\"inputshape in engine.train\",input.shape)\n",
        "        output = self.model(input).transpose(1,3)  # now, output = [batch_size,1,num_nodes, seq_length]\n",
        "        #print(\"outputshape in engine.train\",output.shape)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        #print(\"predictshape in engine.train\",predict.shape)\n",
        "        assert predict.shape[1] == 1\n",
        "        mae, mape, rmse = calc_metrics(predict.squeeze(1), real_val, null_val=0.0)\n",
        "        #print(\"mae after calc_metrics\",mae)\n",
        "        #print(\"predict.squeeze shape\",predict.squeeze(1).shape)\n",
        "        #print(\"real val shape\",real_val.shape)\n",
        "        mae.backward()\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "        self.optimizer.step()\n",
        "        #print(\"mae.item().shape\",mae.item())\n",
        "        return mae.item(),mape.item(),rmse.item()\n",
        "\n",
        "    def eval(self, input, real_val):\n",
        "        self.model.eval()\n",
        "        input = nn.functional.pad(input,(1,0,0,0))\n",
        "        output = self.model(input).transpose(1,3) #  [batch_size,seq_length,num_nodes,1]\n",
        "       # print(\"outputshape in engine.eval\",output.shape)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        #print(\"real shape in engine.eval\",real.shape)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        predict = torch.clamp(predict, min=0., max=70.)\n",
        "        #print(\"predict shape in engine.eval\",predict.shape)\n",
        "        mae, mape, rmse = [x.item() for x in calc_metrics(predict, real, null_val=0.0)]\n",
        "        return mae, mape, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQlwya6Q1agt"
      },
      "source": [
        "**EXP RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xcdm3e1gQZ"
      },
      "source": [
        "def summary(d):\n",
        "    try:\n",
        "        tr_val = pd.read_csv(f'{d}/metrics.csv', index_col=0)\n",
        "        tr_ser = tr_val.loc[tr_val.valid_loss.idxmin()]\n",
        "        tr_ser['best_epoch'] = tr_val.valid_loss.idxmin()\n",
        "        tr_ser['min_train_loss'] = tr_val.train_loss.min()\n",
        "    except FileNotFoundError:\n",
        "        tr_ser = pd.Series()\n",
        "    try:\n",
        "        tmet = pd.read_csv(f'{d}/test_metrics.csv', index_col=0)\n",
        "        tmean = tmet.add_prefix('test_').mean()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        tmean = pd.Series()\n",
        "    tab = pd.concat([tr_ser, tmean]).round(3)\n",
        "    return tab\n",
        "\n",
        "def loss_curve(d):\n",
        "    if 'logs' not in d: d =  f'logs/{d}'\n",
        "    tr_val = pd.read_csv(f'{d}/metrics.csv', index_col=0)\n",
        "    return tr_val[['train_loss', 'valid_loss']]\n",
        "\n",
        "\n",
        "def plot_loss_curve(log_dir):\n",
        "    d = loss_curve(log_dir)\n",
        "    ax = d.plot()\n",
        "    plt.axhline(d.valid_loss.min())\n",
        "    print(d.valid_loss.idxmin())\n",
        "\n",
        "def make_results_table():\n",
        "    return pd.DataFrame({os.path.basename(c): summary(c) for c in glob('logs/*')}).T.sort_values('valid_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N81p5H8qytSJ",
        "outputId": "ea3bb59f-de35-455b-c452-1053fbaf0dfd"
      },
      "source": [
        "hdir=\"/content/gdrive/MyDrive/\"\n",
        "best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "best_model_save_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/best_model.pth'"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-qUeEDeINYk"
      },
      "source": [
        "**GRID SEARCH FOR HYPERPARAMETER TUNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwm7ZjT6IT7Z"
      },
      "source": [
        "#Iterating over generated train&test datasets\n",
        "import os\n",
        "import itertools\n",
        "directory_list = list()\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Robbery_Daily_data\", topdown=False):\n",
        "    for name in dirs:\n",
        "        directory_list.append(os.path.join(root, name))\n",
        "import glob\n",
        "paths=(\"/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix\")\n",
        "all_files = glob.glob(paths + \"/*.pkl\")\n",
        "##Separate parameters\n",
        "lr=[0.001,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]\n",
        "weight_decay=[0.0001,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01]\n",
        "lr_decay_rate=[0.97,0.94,0.9375,0.9350,0.9325,0.93,0.9275,0.9250,0.9225,0.92,0.9175]\n",
        "for k,l,n in zip(lr,weight_decay,lr_decay_rate):\n",
        "    hdir=\"/content/gdrive/MyDrive/\"\n",
        "    best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "    def main(args, **model_kwargs):\n",
        "        device = torch.device(args.device)\n",
        "        data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "        scaler = data['scaler']\n",
        "        aptinit, supports = make_graph_inputs(args, device)\n",
        "        torch.manual_seed(0)\n",
        "        model = GWNet.from_args(args, device, supports, aptinit, **model_kwargs)\n",
        "        if args.checkpoint:\n",
        "          model.load_checkpoint(torch.load(args.checkpoint))\n",
        "        model.to(device)\n",
        "        engine = Trainer.from_args(model, scaler, args)\n",
        "        metrics = []\n",
        "        best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "        lowest_mae_yet = 100  # high value, will get overwritten\n",
        "        mb = progress_bar(list(range(1, args.epochs + 1)))\n",
        "        epochs_since_best_mae = 0\n",
        "        for _ in mb:\n",
        "            train_loss, train_mape, train_rmse = [], [], []\n",
        "            data['train_loader'].shuffle()\n",
        "            for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\n",
        "              trainx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "              trainy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "              yspeed = trainy[:, 0, :, :]\n",
        "              if yspeed.max() == 0: continue\n",
        "                  #print('tra覺nx',trainx.shape)\n",
        "                  #print('tra覺ny',trainy.shape)\n",
        "                  #print('yspeed',yspeed.shape)\n",
        "              mae, mape, rmse = engine.train(trainx, yspeed)\n",
        "              train_loss.append(mae)\n",
        "              train_mape.append(mape)\n",
        "              train_rmse.append(rmse)\n",
        "              if args.n_iters is not None and iter >= args.n_iters:\n",
        "                  break\n",
        "            engine.scheduler.step()\n",
        "            _, valid_loss, valid_mape, valid_rmse = eval_(data['val_loader'], device, engine)\n",
        "            m = dict(train_loss=np.mean(train_loss), train_mape=np.mean(train_mape),\n",
        "                      train_rmse=np.mean(train_rmse), valid_loss=np.mean(valid_loss),\n",
        "                        valid_mape=np.mean(valid_mape), valid_rmse=np.mean(valid_rmse))\n",
        "\n",
        "            m = pd.Series(m)\n",
        "            metrics.append(m)\n",
        "            if m.valid_loss < lowest_mae_yet:\n",
        "                torch.save(engine.model.state_dict(), best_model_save_path)\n",
        "                lowest_mae_yet = m.valid_loss\n",
        "                epochs_since_best_mae = 0\n",
        "            else:\n",
        "                epochs_since_best_mae += 1\n",
        "            met_df = pd.DataFrame(metrics)\n",
        "            mb.comment = f'best val_loss: {met_df.valid_loss.min(): .3f}, current val_loss: {m.valid_loss:.3f}, current train loss: {m.train_loss: .3f}'\n",
        "            met_df.round(6).to_csv(f'{hdir}/metrics.csv')\n",
        "            if epochs_since_best_mae >= args.es_patience: break\n",
        "            # Metrics on test data\n",
        "        with open(best_model_save_path, 'rb') as f:\n",
        "          buffer = io.BytesIO(f.read())\n",
        "        engine.model.load_state_dict(torch.load(buffer))\n",
        "        realy = torch.Tensor(data['y_test']).transpose(1, 3)[:, 0, :, :].to(device)\n",
        "        test_met_df, yhat = calc_tstep_metrics(engine.model, device, data['test_loader'], scaler, realy, args.seq_length)\n",
        "        test_met_df.round(6).to_csv(os.path.join(hdir, 'test_metrics.csv'))\n",
        "\n",
        "    def eval_(ds, device, engine):\n",
        "        \"\"\"Run validation.\"\"\"\n",
        "        valid_loss = []\n",
        "        valid_mape = []\n",
        "        valid_rmse = []\n",
        "        s1 = time.time()\n",
        "        for (x, y) in ds.get_iterator():\n",
        "            testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "            testy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "            metrics = engine.eval(testx, testy[:, 0, :, :])\n",
        "            valid_loss.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "        total_time = time.time() - s1\n",
        "        return total_time, valid_loss, valid_mape, valid_rmse\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        parser =get_shared_arg_parser()\n",
        "        parser.add_argument('--epochs', type=int, default=100, help='')\n",
        "        parser.add_argument('--clip', type=int, default=3, help='Gradient Clipping')\n",
        "        parser.add_argument('--weight_decay', type=float, default=l, help='weight decay rate')\n",
        "        parser.add_argument('--learning_rate', type=float, default=k, help='learning rate')\n",
        "        parser.add_argument('--lr_decay_rate', type=float, default=n, help='learning rate')\n",
        "        parser.add_argument('--save', type=str, default='Robbery_Daily_experiment', help='save path')\n",
        "        parser.add_argument('--n_iters', default=None, help='quit after this many iterations')\n",
        "        parser.add_argument('--es_patience', type=int, default=25, help='quit if no improvement after this many iterations')\n",
        "\n",
        "        args, unknown = parser.parse_known_args()\n",
        "        t1 = time.time()\n",
        "        if not os.path.exists(hdir):\n",
        "            os.mkdir(hdir)\n",
        "        pickle_save(args,'/content/gdrive/MyDrive/args.pkl')\n",
        "        main(args)\n",
        "        t2 = time.time()\n",
        "        mins = (t2 - t1) / 60\n",
        "        print(f\"Total time spent: {mins:.2f} seconds\")\n",
        "        #pref=m.split('/')[-1]\n",
        "        #pp=p.split('/')[-1].split('.')[0].split('graph')[0].split('Center')[1]\n",
        "        key=str(\"experiment:\"+\"-\"+ str(k)+\"-\"+str(l)+\"-\"+str(n)+\".csv\")\n",
        "        epath=('/content/gdrive/MyDrive/Robbery_Daily_experiment/%s'%(key))\n",
        "    def main(args, save_pred_path=epath, save_metrics_path='last_test_metrics_Robbery_preds_10day_dayca8_smoothingcoef05.csv', loader='test', **model_kwargs):\n",
        "        args.checkpoint= best_model_save_path\n",
        "        device = torch.device(args.device)\n",
        "        adjinit, supports = make_graph_inputs(args, device)\n",
        "        model = GWNet.from_args(args, device, supports, adjinit,**model_kwargs)\n",
        "        model.load_state_dict(torch.load(args.checkpoint))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print('model loaded successfully')\n",
        "        data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "        scaler = data['scaler']\n",
        "        realy = torch.Tensor(data[f'y_{loader}']).to(device)\n",
        "        realy = realy.transpose(1,3)[:,0,:,:]\n",
        "        met_df, yhat = calc_tstep_metrics(model, device, data[f'{loader}_loader'], scaler, realy, args.seq_length)\n",
        "        df2 = make_pred_df(realy, yhat, scaler, args.seq_length)\n",
        "        met_df.to_csv(save_metrics_path)\n",
        "        df2.to_csv(save_pred_path, index=False)\n",
        "        if args.plotheatmap: plot_learned_adj_matrix(model)\n",
        "        return met_df, df2\n",
        "\n",
        "    def plot_learned_adj_matrix(model):\n",
        "        adp = F.softmax(F.relu(torch.mm(model.nodevec1, model.nodevec2)), dim=1)\n",
        "        adp = adp.cpu().detach().numpy()\n",
        "        adp = adp / np.max(adp)\n",
        "        df = pd.DataFrame(adp)\n",
        "        sns.heatmap(df, cmap=\"RdYlBu\")\n",
        "        plt.savefig(\"/content/gdrive/MyDrive/heatmap.png\")\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        parser = get_shared_arg_parser()\n",
        "        parser.add_argument('--checkpoints', type=str, help='')\n",
        "        parser.add_argument('--plotheatmap', action='store_true')\n",
        "        args, unknown = parser.parse_known_args()\n",
        "        main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-8ZdmtYa_OL"
      },
      "source": [
        "**GRID SEARCH-GENERATE TRAIN DATASET**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "HjB9ds1obCwo"
      },
      "source": [
        "#@title Default title text\n",
        "#Robberynormalized_shift=pd.read_csv('/content/gdrive/MyDrive/MainFiles/RSIS/ Robberynormalized_shift.csv')\n",
        "#Robberynormalized_shift=Robberynormalized_shift.drop(columns=('Unnamed: 0'))\n",
        "train_window= [3,21,30,42]\n",
        "smoothing_coef=[0.05,0.1,0.5,0.9]\n",
        "for i in train_window:\n",
        "  for j in smoothing_coef:\n",
        "    #TheftTable_Smoothed=TheftTable.ewm(alpha=j).mean()\n",
        "    #RobberyTable_daily_Smoothed = RobberyTable_daily.ewm(alpha=j).mean()\n",
        "    RobberyTable_Smoothed = RobberyTable.ewm(alpha=j).mean()\n",
        "    #Total311_Smoothed=Total311.ewm(alpha=j).mean()\n",
        "    #for k in smoothing_coef:\n",
        "      #RobberyTable_daily_Smoothed = RobberyTable.ewm(alpha=k).mean()\n",
        "    def generate_graph_seq2seq_io_data(df, x_offsets, y_offsets, add_time_in_day=False, add_day_in_week=False, scaler=None):\n",
        "          \"\"\"\n",
        "          Generate samples from\n",
        "          :param df:\n",
        "          :param x_offsets:\n",
        "          :param y_offsets:\n",
        "          :param add_time_in_day:\n",
        "          :param add_day_in_week:\n",
        "          :param scaler:\n",
        "          :return:\n",
        "          # x: (epoch_size, input_length, num_nodes, input_dim)\n",
        "          # y: (epoch_size, output_length, num_nodes, output_dim)\"\"\"\n",
        "          num_samples, num_nodes = df.shape\n",
        "          data = np.expand_dims(df.values, axis=-1)\n",
        "          print(\"shape_of_data: \", data.shape)\n",
        "          feature_list = [data]\n",
        "          #if add_time_in_day:\n",
        "            #time_ind = (df.index.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
        "            #time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "            #feature_list.append(time_in_day)\n",
        "          #if add_day_in_week:\n",
        "            #dow = df.index.dayofweek\n",
        "            #dow_tiled = np.tile(dow, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "            #feature_list.append(dow_tiled)\n",
        "          #Trying Weekend/Weekday distinction\n",
        "            #dow2=dow.copy()\n",
        "            #dow2=[1 if (x==5 or x==6) else 0  for x in dow2]\n",
        "            #dow=np.array(dow2)\n",
        "            #dow_tiled = np.tile(dow, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
        "            #feature_list.append(dow_tiled)\n",
        "          #data = np.concatenate(feature_list, axis=-1)\n",
        "          ##Adding RSIS values\n",
        "          #Robberynormalized_shifts=Robberynormalized_shift.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(Robberynormalized_shifts)\n",
        "          #Adding Robbery\n",
        "          #TheftTable_Smootheds=TheftTable_Smoothed.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(TheftTable_Smootheds)\n",
        "          #Adding 311\n",
        "          #Total311_Smootheds=Total311.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(Total311_Smootheds)\n",
        "          #Adding Park events\n",
        "          #Parkevent_Smootheds=ParkEvents_Smoothed.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(Parkevent_Smootheds)\n",
        "          #Adding RSIS\n",
        "          #RobberyRSIS_Smootheds=RobberyRSIS_daily.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(RobberyRSIS_Smootheds)\n",
        "          #Adding RSIS normalized\n",
        "          #RobberyRSIS_Smootheds=RobberyRSIS_normalized_daily.values.reshape(num_samples, num_nodes,1)\n",
        "          #RobberyRSIS_Smootheds=RobberyRSISS_normalized.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(RobberyRSIS_Smootheds)\n",
        "          ##Adding Normalized mult RSIS as an external variable\n",
        "          #RSISRobbery=RobberyRSIS_mult_normalized.values.reshape(num_samples, num_nodes,1)\n",
        "          #feature_list.append(RSISRobbery)\n",
        "          ##Adding Normalized count RSIS as an external variable\n",
        "          RSISRobbery=RobberyRSIS_count_normalized.values.reshape(num_samples, num_nodes,1)\n",
        "          feature_list.append(RSISRobbery)\n",
        "          data = np.concatenate(feature_list, axis=-1)\n",
        "          print(\"shape_of_data: \", data.shape)\n",
        "          x, y = [], []\n",
        "          min_t = abs(min(x_offsets))\n",
        "          print(\"min_t \", min_t)\n",
        "          max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
        "          print(\"max_t\", max_t)\n",
        "          for t in range(min_t, max_t):# t is the index of the last observation. The loop iterates between 11 and 2180.\n",
        "            x.append(data[t + x_offsets, ...])\n",
        "            y.append(data[t + y_offsets, ...])\n",
        "          print(\"x before stack \", x[0].shape)\n",
        "          x = np.stack(x, axis=0)\n",
        "          print(\"shape of x \", x.shape)\n",
        "          print(\"y before stack \", y[0].shape)\n",
        "          y = np.stack(y, axis=0)\n",
        "          print(\"shape of y \", y.shape)\n",
        "          return x, y\n",
        "    def generate_train_val_test(args):\n",
        "      seq_length_x, seq_length_y = args.seq_length_x, args.seq_length_y\n",
        "      df = args.traffic_df_filename\n",
        "      # 0 is the latest observed sample.\n",
        "      x_offsets = np.sort(np.concatenate((np.arange(-(seq_length_x - 1), 1, 1),)))\n",
        "      print(\"shape of x_offsets \", x_offsets.shape)\n",
        "      print(\"x_offsets \",x_offsets)\n",
        "      # Predict the next one hour\n",
        "      y_offsets = np.sort(np.arange(args.y_start, (seq_length_y + 1), 1))\n",
        "      print(\"shape of y_offsets \", y_offsets.shape)\n",
        "      print(\"y_offsets \",y_offsets)\n",
        "      # x: (num_samples, input_length, num_nodes, input_dim)\n",
        "      # y: (num_samples, output_length, num_nodes, output_dim)\n",
        "      x, y = generate_graph_seq2seq_io_data(\n",
        "          df,\n",
        "          x_offsets=x_offsets,\n",
        "          y_offsets=y_offsets,\n",
        "          add_time_in_day=False,\n",
        "          add_day_in_week=args.dow,\n",
        "          )\n",
        "      print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
        "      # Write the data into npz file.\n",
        "      num_samples = x.shape[0]\n",
        "      num_test = round(num_samples * 0.2)\n",
        "      num_train = round(num_samples * 0.7)\n",
        "      num_val = num_samples - num_test - num_train\n",
        "      x_train, y_train = x[:num_train], y[:num_train]\n",
        "      x_val, y_val = (\n",
        "          x[num_train: num_train + num_val],\n",
        "          y[num_train: num_train + num_val],\n",
        "          )\n",
        "      x_test, y_test = x[-num_test:], y[-num_test:]\n",
        "      for cat in [\"train\", \"val\", \"test\"]:\n",
        "        _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
        "        print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
        "        np.savez_compressed(\n",
        "            os.path.join(args.output_dir, f\"{cat}.npz\"),\n",
        "            x=_x,\n",
        "            y=_y,\n",
        "            x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
        "            y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
        "            )\n",
        "      print(\"shape of x_offsets \", x_offsets.shape)\n",
        "      print(\"shape of y_offsets \", y_offsets.shape)\n",
        "    key1=str(\"data:\"+str(i)+\"-\"+str(j))\n",
        "    if __name__ == \"__main__\":\n",
        "          parser = argparse.ArgumentParser()\n",
        "          parser.add_argument(\"--output_dir\", type=str, default=\"/content/gdrive/MyDrive/Robbery_Daily_data/%s\" %(key1), help=\"Output directory.\")\n",
        "          parser.add_argument(\"--traffic_df_filename\", type=str, default=RobberyTable_Smoothed, help=\" Daily crime risk scores\",)\n",
        "          parser.add_argument(\"--seq_length_x\", type=int, default=i, help=\"Sequence Length.\",)\n",
        "          parser.add_argument(\"--seq_length_y\", type=int, default=1, help=\"Sequence Length.\",)\n",
        "          parser.add_argument(\"--y_start\", type=int, default=1, help=\"Y pred start\", )\n",
        "          parser.add_argument(\"--dow\", action='store_true',)\n",
        "          args, unknown = parser.parse_known_args()\n",
        "    if os.path.exists(args.output_dir):\n",
        "      reply = str(input(f'{args.output_dir} exists. Do you want to overwrite it? (y/n)')).lower().strip()\n",
        "      if reply[0] != 'y': exit\n",
        "    else:\n",
        "      os.makedirs(args.output_dir)\n",
        "    generate_train_val_test(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZAU-rBb_UlP"
      },
      "source": [
        "# **GRID SEARCH TRAIN-TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28oYWDSV_ZXc"
      },
      "source": [
        "#Iterating over generated train&test datasets\n",
        "import os\n",
        "directory_list = list()\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Robbery_Daily_data\", topdown=False):\n",
        "    for name in dirs:\n",
        "        directory_list.append(os.path.join(root, name))\n",
        "import glob\n",
        "paths=(\"/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix\")\n",
        "all_files = glob.glob(paths + \"/*.pkl\")\n",
        "\n",
        "for m in directory_list:\n",
        "  for p in all_files:\n",
        "      import itertools\n",
        "      hdir=\"/content/gdrive/MyDrive/\"\n",
        "      best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "      def main(args, **model_kwargs):\n",
        "          device = torch.device(args.device)\n",
        "          data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "          scaler = data['scaler']\n",
        "          aptinit, supports = make_graph_inputs(args, device)\n",
        "          torch.manual_seed(0)\n",
        "          model = GWNet.from_args(args, device, supports, aptinit, **model_kwargs)\n",
        "          if args.checkpoint:\n",
        "            model.load_checkpoint(torch.load(args.checkpoint))\n",
        "          model.to(device)\n",
        "          engine = Trainer.from_args(model, scaler, args)\n",
        "          metrics = []\n",
        "          best_model_save_path = os.path.join(hdir, 'best_model.pth')\n",
        "          lowest_mae_yet = 100  # high value, will get overwritten\n",
        "          mb = progress_bar(list(range(1, args.epochs + 1)))\n",
        "          epochs_since_best_mae = 0\n",
        "          for _ in mb:\n",
        "              train_loss, train_mape, train_rmse = [], [], []\n",
        "              data['train_loader'].shuffle()\n",
        "              for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\n",
        "                trainx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "                trainy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "                yspeed = trainy[:, 0, :, :]\n",
        "                if yspeed.max() == 0: continue\n",
        "                    #print('tra覺nx',trainx.shape)\n",
        "                    #print('tra覺ny',trainy.shape)\n",
        "                    #print('yspeed',yspeed.shape)\n",
        "                mae, mape, rmse = engine.train(trainx, yspeed)\n",
        "                train_loss.append(mae)\n",
        "                train_mape.append(mape)\n",
        "                train_rmse.append(rmse)\n",
        "                if args.n_iters is not None and iter >= args.n_iters:\n",
        "                    break\n",
        "              engine.scheduler.step()\n",
        "              _, valid_loss, valid_mape, valid_rmse = eval_(data['val_loader'], device, engine)\n",
        "              m = dict(train_loss=np.mean(train_loss), train_mape=np.mean(train_mape),\n",
        "                        train_rmse=np.mean(train_rmse), valid_loss=np.mean(valid_loss),\n",
        "                          valid_mape=np.mean(valid_mape), valid_rmse=np.mean(valid_rmse))\n",
        "\n",
        "              m = pd.Series(m)\n",
        "              metrics.append(m)\n",
        "              if m.valid_loss < lowest_mae_yet:\n",
        "                  torch.save(engine.model.state_dict(), best_model_save_path)\n",
        "                  lowest_mae_yet = m.valid_loss\n",
        "                  epochs_since_best_mae = 0\n",
        "              else:\n",
        "                  epochs_since_best_mae += 1\n",
        "              met_df = pd.DataFrame(metrics)\n",
        "              mb.comment = f'best val_loss: {met_df.valid_loss.min(): .3f}, current val_loss: {m.valid_loss:.3f}, current train loss: {m.train_loss: .3f}'\n",
        "              met_df.round(6).to_csv(f'{hdir}/metrics.csv')\n",
        "              if epochs_since_best_mae >= args.es_patience: break\n",
        "              # Metrics on test data\n",
        "          with open(best_model_save_path, 'rb') as f:\n",
        "            buffer = io.BytesIO(f.read())\n",
        "          engine.model.load_state_dict(torch.load(buffer))\n",
        "          realy = torch.Tensor(data['y_test']).transpose(1, 3)[:, 0, :, :].to(device)\n",
        "          test_met_df, yhat = calc_tstep_metrics(engine.model, device, data['test_loader'], scaler, realy, args.seq_length)\n",
        "          test_met_df.round(6).to_csv(os.path.join(hdir, 'test_metrics.csv'))\n",
        "\n",
        "      def eval_(ds, device, engine):\n",
        "          \"\"\"Run validation.\"\"\"\n",
        "          valid_loss = []\n",
        "          valid_mape = []\n",
        "          valid_rmse = []\n",
        "          s1 = time.time()\n",
        "          for (x, y) in ds.get_iterator():\n",
        "              testx = torch.Tensor(x).to(device).transpose(1, 3)\n",
        "              testy = torch.Tensor(y).to(device).transpose(1, 3)\n",
        "              metrics = engine.eval(testx, testy[:, 0, :, :])\n",
        "              valid_loss.append(metrics[0])\n",
        "              valid_mape.append(metrics[1])\n",
        "              valid_rmse.append(metrics[2])\n",
        "          total_time = time.time() - s1\n",
        "          return total_time, valid_loss, valid_mape, valid_rmse\n",
        "\n",
        "\n",
        "      if __name__ == \"__main__\":\n",
        "          parser =get_shared_arg_parser()\n",
        "          parser.add_argument('--epochs', type=int, default=100, help='')\n",
        "          parser.add_argument('--clip', type=int, default=3, help='Gradient Clipping')\n",
        "          parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay rate')\n",
        "          parser.add_argument('--learning_rate', type=float, default=0.01, help='learning rate')\n",
        "          parser.add_argument('--lr_decay_rate', type=float, default=0.97, help='learning rate')\n",
        "          parser.add_argument('--save', type=str, default='experiment', help='save path')\n",
        "          parser.add_argument('--n_iters', default=None, help='quit after this many iterations')\n",
        "          parser.add_argument('--es_patience', type=int, default=25, help='quit if no improvement after this many iterations')\n",
        "\n",
        "          args, unknown = parser.parse_known_args()\n",
        "          t1 = time.time()\n",
        "          if not os.path.exists(hdir):\n",
        "              os.mkdir(hdir)\n",
        "          pickle_save(args,'/content/gdrive/MyDrive/args.pkl')\n",
        "          main(args)\n",
        "          t2 = time.time()\n",
        "          mins = (t2 - t1) / 60\n",
        "          print(f\"Total time spent: {mins:.2f} seconds\")\n",
        "          pref=m.split('/')[-1]\n",
        "          pp=p.split('/')[-1].split('.')[0].split('graph')[0].split('Center')[1]\n",
        "          key=str(\"experiment:\"+pref+pp+\"-\"+\".csv\")\n",
        "          epath=('/content/gdrive/MyDrive/Robbery_Daily_experiment/%s'%(key))\n",
        "      def main(args, save_pred_path=epath, save_metrics_path='last_test_metrics_Robbery_preds_10day_dayca8_smoothingcoef05.csv', loader='test', **model_kwargs):\n",
        "          args.checkpoint= best_model_save_path\n",
        "          device = torch.device(args.device)\n",
        "          adjinit, supports = make_graph_inputs(args, device)\n",
        "          model = GWNet.from_args(args, device, supports, adjinit,**model_kwargs)\n",
        "          model.load_state_dict(torch.load(args.checkpoint))\n",
        "          model.to(device)\n",
        "          model.eval()\n",
        "          print('model loaded successfully')\n",
        "          data = load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\n",
        "          scaler = data['scaler']\n",
        "          realy = torch.Tensor(data[f'y_{loader}']).to(device)\n",
        "          realy = realy.transpose(1,3)[:,0,:,:]\n",
        "          met_df, yhat = calc_tstep_metrics(model, device, data[f'{loader}_loader'], scaler, realy, args.seq_length)\n",
        "          df2 = make_pred_df(realy, yhat, scaler, args.seq_length)\n",
        "          met_df.to_csv(save_metrics_path)\n",
        "          df2.to_csv(save_pred_path, index=False)\n",
        "          if args.plotheatmap: plot_learned_adj_matrix(model)\n",
        "          return met_df, df2\n",
        "\n",
        "      def plot_learned_adj_matrix(model):\n",
        "          adp = F.softmax(F.relu(torch.mm(model.nodevec1, model.nodevec2)), dim=1)\n",
        "          adp = adp.cpu().detach().numpy()\n",
        "          adp = adp / np.max(adp)\n",
        "          df = pd.DataFrame(adp)\n",
        "          sns.heatmap(df, cmap=\"RdYlBu\")\n",
        "          plt.savefig(\"/content/gdrive/MyDrive/heatmap.png\")\n",
        "\n",
        "\n",
        "      if __name__ == \"__main__\":\n",
        "          parser = get_shared_arg_parser()\n",
        "          parser.add_argument('--checkpoints', type=str, help='')\n",
        "          parser.add_argument('--plotheatmap', action='store_true')\n",
        "          args, unknown = parser.parse_known_args()\n",
        "          main(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPATIOTEMPORAL GRAPH CONVOLUTIONAL NEURAL NETWORK**"
      ],
      "metadata": {
        "id": "fg1Gic7oZm_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare the datasets**"
      ],
      "metadata": {
        "id": "o9rYk0lvacfK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWVyOYGGNk4O"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as utils\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5GF7crFCX-l"
      },
      "outputs": [],
      "source": [
        "def mask_and_fillna(loss, mask):\n",
        "    loss = loss * mask\n",
        "    #print(\"shape of loss\", loss.shape)\n",
        "    #print(\"shape of mask\", mask.shape)\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        #print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "def load_metr_la_data():\n",
        "    #if (not os.path.isfile(\"data/adj_mat.npy\")\n",
        "     #       or not os.path.isfile(\"data/node_values.npy\")):\n",
        "      #  with zipfile.ZipFile(\"data/METR-LA.zip\", 'r') as zip_ref:\n",
        "       #     zip_ref.extractall(\"data/\")\n",
        "\n",
        "    b,c,A = load_pickle('/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix/Center01graph.pkl')\n",
        "    TheftTable_daily_Smoothed=TheftTable_daily.ewm(alpha=0.05).mean()\n",
        "    #Grid Search\n",
        "    #b,c,A = load_pickle(p)\n",
        "    #TheftTable_Smoothed=TheftTable.ewm(alpha=j).mean()\n",
        "    X=np.expand_dims(TheftTable_Smoothed.values, axis=-1).transpose((1, 2, 0))\n",
        "    X = X.astype(np.float32)\n",
        "    print(\"FirstXshape\"+str(X.shape))\n",
        "    #feature_list = [X]\n",
        "    #Add rRSIS\n",
        "    #RSISTheft=TheftRSIS_count_normalized.values.reshape(X.shape[0], X.shape[1],X.shape[2])\n",
        "    #print(RSISTheft.shape)\n",
        "    #Add 311\n",
        "    #RSISTheft=Total311.values.reshape(X.shape[0], X.shape[1],X.shape[2])\n",
        "    #Add Robbery\n",
        "    #RobberyTable_Smoothed=RobberyTable.ewm(alpha=0.5).mean()\n",
        "    #RSISTheft=RobberyTable_Smoothed.values.reshape(X.shape[0], X.shape[1],X.shape[2])\n",
        "    #Add Park events\n",
        "    #Parkevents_daily_Smoothed=ParkEvents_daily.ewm(span=2).mean()\n",
        "    #RSISTheft=Parkevents_daily_Smoothed.values.reshape(X.shape[0], X.shape[1],X.shape[2])\n",
        "    #feature_list.append(RSISTheft)\n",
        "    #X = np.concatenate(feature_list, axis=-1)\n",
        "    #X = np.concatenate(feature_list, axis=1)\n",
        "    #print(\"xconcat\"+str(X.shape))\n",
        "    # Normalization using Z-score method\n",
        "    means = np.mean(X, axis=(0, 2))\n",
        "    X = X - means.reshape(1, -1, 1)\n",
        "    stds = np.std(X, axis=(0, 2))\n",
        "    X = X / stds.reshape(1, -1, 1)\n",
        "    #print(\"final_shape\"+str(X.shape))\n",
        "    return A, X, means, stds\n",
        "\n",
        "\n",
        "def get_normalized_adj(A):\n",
        "    \"\"\"\n",
        "    Returns the degree normalized adjacency matrix.\n",
        "    \"\"\"\n",
        "    A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
        "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
        "    D[D <= 10e-5] = 10e-5    # Prevent infs\n",
        "    diag = np.reciprocal(np.sqrt(D))\n",
        "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A),\n",
        "                         diag.reshape((1, -1)))\n",
        "    return A_wave\n",
        "\n",
        "\n",
        "def generate_dataset(X, num_timesteps_input, num_timesteps_output):\n",
        "    \"\"\"\n",
        "    Takes node features for the graph and divides them into multiple samples\n",
        "    along the time-axis by sliding a window of size (num_timesteps_input+\n",
        "    num_timesteps_output) across it in steps of 1.\n",
        "    :param X: Node features of shape (num_vertices, num_features,\n",
        "    num_timesteps)\n",
        "    :return:\n",
        "        - Node features divided into multiple samples. Shape is\n",
        "          (num_samples, num_vertices, num_features, num_timesteps_input).\n",
        "        - Node targets for the samples. Shape is\n",
        "          (num_samples, num_vertices, num_features, num_timesteps_output).\n",
        "    \"\"\"\n",
        "    # Generate the beginning index and the ending index of a sample, which\n",
        "    # contains (num_points_for_training + num_points_for_predicting) points\n",
        "    indices = [(i, i + (num_timesteps_input + num_timesteps_output)) for i\n",
        "               in range(X.shape[2] - (num_timesteps_input + num_timesteps_output) + 1)]\n",
        "    print(indices)\n",
        "\n",
        "    # Save samples\n",
        "    features, target = [], []\n",
        "    for i, j in indices:\n",
        "        features.append(\n",
        "            X[:, :, i: i + num_timesteps_input].transpose((0, 2, 1)))\n",
        "        print(X[:, :, i: i + num_timesteps_input].transpose((0, 2, 1)).shape)\n",
        "        target.append(X[:, 0, i + num_timesteps_input: j])\n",
        "    print(\"fshape\"+str(torch.from_numpy(np.array(features)).shape))\n",
        "    print(\"tshape\"+str(torch.from_numpy(np.array(target)).shape))\n",
        "    return torch.from_numpy(np.array(features)), \\\n",
        "           torch.from_numpy(np.array(target))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        #print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "b,c,A = load_pickle('/content/gdrive/MyDrive/MainFiles/CenterSide/AdjMatrix/Center03graph.pkl')\n",
        "A"
      ],
      "metadata": {
        "id": "mRCdgk9LR5S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STGCN**"
      ],
      "metadata": {
        "id": "xb6Bbe7Za1PR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoANy4n4OQFx"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TimeBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network block that applies a temporal convolution to each node of\n",
        "    a graph in isolation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of input features at each node in each time\n",
        "        step.\n",
        "        :param out_channels: Desired number of output channels at each node in\n",
        "        each time step.\n",
        "        :param kernel_size: Size of the 1D temporal kernel.\n",
        "        \"\"\"\n",
        "        super(TimeBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
        "        self.conv3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        :param X: Input data of shape (batch_size, num_nodes, num_timesteps,\n",
        "        num_features=in_channels)\n",
        "        :return: Output data of shape (batch_size, num_nodes,\n",
        "        num_timesteps_out, num_features_out=out_channels)\n",
        "        \"\"\"\n",
        "        # Convert into NCHW format for pytorch to perform convolutions.\n",
        "        X = X.permute(0, 3, 1, 2)\n",
        "        temp = self.conv1(X) + torch.sigmoid(self.conv2(X))\n",
        "        out = F.relu(temp + self.conv3(X))\n",
        "        # Convert back from NCHW to NHWC\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class STGCNBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network block that applies a temporal convolution on each node in\n",
        "    isolation, followed by a graph convolution, followed by another temporal\n",
        "    convolution on each node.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, spatial_channels, out_channels,\n",
        "                 num_nodes):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of input features at each node in each time\n",
        "        step.\n",
        "        :param spatial_channels: Number of output channels of the graph\n",
        "        convolutional, spatial sub-block.\n",
        "        :param out_channels: Desired number of output features at each node in\n",
        "        each time step.\n",
        "        :param num_nodes: Number of nodes in the graph.\n",
        "        \"\"\"\n",
        "        super(STGCNBlock, self).__init__()\n",
        "        self.temporal1 = TimeBlock(in_channels=in_channels,\n",
        "                                   out_channels=out_channels)\n",
        "        self.Theta1 = nn.Parameter(torch.FloatTensor(out_channels,\n",
        "                                                     spatial_channels))\n",
        "        self.temporal2 = TimeBlock(in_channels=spatial_channels,\n",
        "                                   out_channels=out_channels)\n",
        "        self.batch_norm = nn.BatchNorm2d(num_nodes)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.Theta1.shape[1])\n",
        "        self.Theta1.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, X, A_hat):\n",
        "        \"\"\"\n",
        "        :param X: Input data of shape (batch_size, num_nodes, num_timesteps,\n",
        "        num_features=in_channels).\n",
        "        :param A_hat: Normalized adjacency matrix.\n",
        "        :return: Output data of shape (batch_size, num_nodes,\n",
        "        num_timesteps_out, num_features=out_channels).\n",
        "        \"\"\"\n",
        "        t = self.temporal1(X)\n",
        "        lfs = torch.einsum(\"ij,jklm->kilm\", [A_hat, t.permute(1, 0, 2, 3)])\n",
        "        # t2 = F.relu(torch.einsum(\"ijkl,lp->ijkp\", [lfs, self.Theta1]))\n",
        "        t2 = F.relu(torch.matmul(lfs, self.Theta1))\n",
        "        t3 = self.temporal2(t2)\n",
        "        return self.batch_norm(t3)\n",
        "        # return t3\n",
        "\n",
        "\n",
        "class STGCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatio-temporal graph convolutional network as described in\n",
        "    https://arxiv.org/abs/1709.04875v3 by Yu et al.\n",
        "    Input should have shape (batch_size, num_nodes, num_input_time_steps,\n",
        "    num_features).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_nodes, num_features, num_timesteps_input,\n",
        "                 num_timesteps_output):\n",
        "        \"\"\"\n",
        "        :param num_nodes: Number of nodes in the graph.\n",
        "        :param num_features: Number of features at each node in each time step.\n",
        "        :param num_timesteps_input: Number of past time steps fed into the\n",
        "        network.\n",
        "        :param num_timesteps_output: Desired number of future time steps\n",
        "        output by the network.\n",
        "        \"\"\"\n",
        "        super(STGCN, self).__init__()\n",
        "        self.block1 = STGCNBlock(in_channels=num_features, out_channels=8,\n",
        "                                 spatial_channels=2, num_nodes=num_nodes)\n",
        "        self.block2 = STGCNBlock(in_channels=8, out_channels=8,\n",
        "                                 spatial_channels=2, num_nodes=num_nodes)\n",
        "        self.last_temporal = TimeBlock(in_channels=8, out_channels=8)\n",
        "        self.fully = nn.Linear((num_timesteps_input - 2 * 5) * 8,\n",
        "                               num_timesteps_output)\n",
        "\n",
        "    def forward(self, A_hat, X):\n",
        "        \"\"\"\n",
        "        :param X: Input data of shape (batch_size, num_nodes, num_timesteps,\n",
        "        num_features=in_channels).\n",
        "        :param A_hat: Normalized adjacency matrix.\n",
        "        \"\"\"\n",
        "        out1 = self.block1(X, A_hat)\n",
        "        out2 = self.block2(out1, A_hat)\n",
        "        out3 = self.last_temporal(out2)\n",
        "        out4 = self.fully(out3.reshape((out3.shape[0], out3.shape[1], -1)))\n",
        "        return out4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "bNt8fSnxbA2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare the dataset**"
      ],
      "metadata": {
        "id": "dnrgCZawbd0g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYa95ulEL678"
      },
      "source": [
        "def PrepareDataset(speed_matrix, seq_len, BATCH_SIZE = 16, pred_len = 1, train_propotion = 0.7, valid_propotion = 0.1):\n",
        "    \"\"\" Prepare training and testing datasets and dataloaders.\n",
        "\n",
        "    Convert speed/volume/occupancy matrix to training and testing dataset.\n",
        "    The vertical axis of speed_matrix is the time axis and the horizontal axis\n",
        "    is the spatial axis.\n",
        "\n",
        "    Args:\n",
        "        speed_matrix: a Matrix containing spatial-temporal speed data for a network\n",
        "        seq_len: length of input sequence\n",
        "        pred_len: length of predicted sequence\n",
        "    Returns:\n",
        "        Training dataloader\n",
        "        Testing dataloader\n",
        "    \"\"\"\n",
        "    time_len, num_nodes = speed_matrix.shape\n",
        "    print(\"shape_of_speed_matrix: \", speed_matrix.shape)\n",
        "    max_speed = speed_matrix.max().max()\n",
        "    speed_matrix =  speed_matrix / max_speed\n",
        "    data = np.expand_dims(speed_matrix.values, axis=-1)\n",
        "    #print(\"shape_of_data: \", data.shape)\n",
        "    feature_list = [data]\n",
        "    #Adding Theft RSIS\n",
        "    #TheftRSIS_mult_normalized_daily.values.reshape(time_len, num_nodes,1)\n",
        "    RSISTheft=TheftRSIS_mult_normalized_daily.values.reshape(time_len, num_nodes,1)\n",
        "    #print(\"RSISTheft shape\"+str(RSISTheft.shape))\n",
        "    feature_list.append(RSISTheft)\n",
        "    #data = np.concatenate(feature_list, axis=-1)\n",
        "    whole_matrix = np.concatenate(feature_list, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "    #Adding Park events\n",
        "    PE=ParkEvents.ewm(span=6).mean()\n",
        "    max_speed = PE.max().max()\n",
        "    PE =  PE / max_speed\n",
        "    PE.index=speed_matrix.index\n",
        "    whole_matrix= pd.merge(speed_matrix, TheftRSIS_count_normalized, left_index=True, right_index=True)\n",
        "    print(\"shape of whole:\"+ str(whole_matrix.shape))\n",
        "    speed_sequences, speed_labels = [], []\n",
        "    for i in range(time_len - seq_len - pred_len):\n",
        "\n",
        "        speed_sequences.append(whole_matrix[i:i+seq_len,:])\n",
        "        speed_labels.append(whole_matrix[i+seq_len:i+seq_len+pred_len])\n",
        "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
        "    print(speed_sequences.shape)\n",
        "    print(speed_labels.shape)\n",
        "    sample_size = speed_sequences.shape[0]\n",
        "    index = np.arange(sample_size, dtype = int)\n",
        "\n",
        "    train_index = int(np.floor(sample_size * train_propotion))\n",
        "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
        "    train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
        "    print(train_data.shape)\n",
        "    valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
        "    print(valid_label.shape)\n",
        "    test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "\n",
        "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "    train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=False, drop_last = True)\n",
        "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=False, drop_last = True)\n",
        "    test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False, drop_last = True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader, max_speed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODULES**"
      ],
      "metadata": {
        "id": "5BLiWq4TcD_r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeTJFHzQ3n6z"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "class FilterLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
        "        '''\n",
        "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
        "        '''\n",
        "        super(FilterLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        self.filter_square_matrix = None\n",
        "        if use_gpu:\n",
        "            self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
        "        else:\n",
        "            self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "#         print(self.weight.data)\n",
        "#         print(self.bias.data)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.filter_square_matrix.matmul(self.weight), self.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) \\\n",
        "            + ', bias=' + str(self.bias is not None) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAIN MODEL**"
      ],
      "metadata": {
        "id": "jqIwkRtjcNzE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_eQCoQGNFuJ"
      },
      "source": [
        "def TrainModel(model, train_dataloader, valid_dataloader,learning_rate,clip=5.0,num_epochs = 100, patience = 25, min_delta = 1e-100):\n",
        "\n",
        "    inputs, labels = next(iter(train_dataloader))\n",
        "    [batch_size, step_size,fea_size] = inputs.size()\n",
        "    input_dim = fea_size\n",
        "    hidden_dim = fea_size\n",
        "    output_dim = labels.shape[2]\n",
        "    device='cuda:0'\n",
        "    model.cuda()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "    clip=0.1\n",
        "    learning_rate = 0.001\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate, weight_decay=0.0001)\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    interval = 100\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "    losses_epochs_train = []\n",
        "    losses_epochs_valid = []\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    # Variables for Early Stopping\n",
        "    is_best_model = 0\n",
        "    patient_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "      print('-' * 10)\n",
        "      trained_number = 0\n",
        "\n",
        "      valid_dataloader_iter = iter(valid_dataloader)\n",
        "\n",
        "      losses_epoch_train = []\n",
        "      losses_epoch_valid = []\n",
        "\n",
        "      for data in train_dataloader:\n",
        "          inputs, labels = data\n",
        "\n",
        "          if inputs.shape[0] != batch_size:\n",
        "              continue\n",
        "\n",
        "          if use_gpu:\n",
        "              inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "          else:\n",
        "              inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "\n",
        "          model.zero_grad()\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          fpreds=torch.flatten(outputs).to(device).cpu().data.numpy()\n",
        "          flabels=torch.flatten(torch.squeeze(labels)).to(device).cpu().data.numpy()\n",
        "          print(\"fp\"+str(fpreds.shape))\n",
        "          print(\"fl\"+str(flabels.shape))\n",
        "          c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds,flabels)]\n",
        "          a=np.array(c)\n",
        "          b=sum(a)\n",
        "          loss_train=torch.tensor(b,requires_grad=True).to(device)\n",
        "          print(\"lt_shape\"+ str(loss_train.data))\n",
        "          losses_train.append(loss_train.data)\n",
        "          losses_epoch_train.append(loss_train.data)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss_train.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "          optimizer.step()\n",
        "\n",
        "          # validation\n",
        "          try:\n",
        "            inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "          except StopIteration:\n",
        "            valid_dataloader_iter = iter(valid_dataloader)\n",
        "            inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "\n",
        "          if use_gpu:\n",
        "              inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
        "          else:\n",
        "              inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
        "\n",
        "          outputs_val= model(inputs_val)\n",
        "          fpreds_val=torch.flatten(outputs_val).to(device).cpu().data.numpy()\n",
        "          flabels_val=torch.flatten(torch.squeeze(labels_val)).to(device).cpu().data.numpy()\n",
        "          print(\"fp_val\"+str(fpreds_val.shape))\n",
        "          print(\"fl_val\"+str(flabels_val.shape))\n",
        "          c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds_val,flabels_val)]\n",
        "          a=np.array(c)\n",
        "          b=sum(a)\n",
        "          loss_valid=torch.tensor(b,requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          #loss_valid = loss_MSE(outputs_val, torch.squeeze(labels_val))\n",
        "          print(\"lv\"+str(loss_valid.data))\n",
        "          losses_valid.append(loss_valid.data)\n",
        "          losses_epoch_valid.append(loss_valid.data)\n",
        "\n",
        "          # output\n",
        "          trained_number += 1\n",
        "\n",
        "      avg_losses_epoch_train = sum(losses_epoch_train) / float(len(losses_epoch_train))\n",
        "      avg_losses_epoch_valid = sum(losses_epoch_valid) / float(len(losses_epoch_valid))\n",
        "      losses_epochs_train.append(avg_losses_epoch_train)\n",
        "      losses_epochs_valid.append(avg_losses_epoch_valid)\n",
        "\n",
        "      # Early Stopping\n",
        "      if epoch == 0:\n",
        "          is_best_model = 1\n",
        "          best_model = model\n",
        "          min_loss_epoch_valid = 10000.0\n",
        "          if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
        "              min_loss_epoch_valid = avg_losses_epoch_valid\n",
        "      else:\n",
        "          if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
        "              is_best_model = 1\n",
        "              best_model = model\n",
        "              min_loss_epoch_valid = avg_losses_epoch_valid\n",
        "              patient_epoch = 0\n",
        "          else:\n",
        "              is_best_model = 0\n",
        "              patient_epoch += 1\n",
        "              if patient_epoch >= patience:\n",
        "                  print('Early Stopped at Epoch:', epoch)\n",
        "                  break\n",
        "\n",
        "      # Print training parameters\n",
        "      cur_time = time.time()\n",
        "      print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format(epoch,\n",
        "                    np.around(avg_losses_epoch_train.cpu(), decimals=8),\n",
        "                    np.around(avg_losses_epoch_valid.cpu(), decimals=8),\n",
        "                    np.around([cur_time - pre_time] , decimals=2),\n",
        "                    is_best_model) )\n",
        "      pre_time = cur_time\n",
        "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST MODEL**"
      ],
      "metadata": {
        "id": "NC3FMzErcUjs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih5LV6PTNkYl"
      },
      "source": [
        "def TestModel(model, test_dataloader, max_speed):\n",
        "\n",
        "    inputs, labels = next(iter(test_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.MSELoss()\n",
        "\n",
        "    tested_batch = 0\n",
        "\n",
        "    losses_mse = []\n",
        "    losses_l1 = []\n",
        "    predict_array=[]\n",
        "    actual_array=[]\n",
        "    for data in test_dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        if inputs.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        # rnn.loop()\n",
        "        hidden = model.initHidden(batch_size)\n",
        "\n",
        "        outputs = None\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        fpreds=torch.flatten(outputs).to(device).cpu().data.numpy()\n",
        "        flabels=torch.flatten(torch.squeeze(labels)).to(device).cpu().data.numpy()\n",
        "        predict_array.append(fpreds)\n",
        "        actual_array.append(flabels)\n",
        "        c=[((0.95-1)*((x-y)**2)) if y==0 else ((0.95)*((x-y)**2)) for x,y in zip(fpreds,flabels)]\n",
        "        a=np.array(c)\n",
        "        b=sum(a)\n",
        "        loss_mse=torch.tensor(b,requires_grad=True).to(device)\n",
        "\n",
        "        #loss_MSE = torch.nn.MSELoss()\n",
        "        loss_L1 = torch.nn.L1Loss()\n",
        "        #loss_mse = loss_MSE(outputs, torch.squeeze(labels))\n",
        "        loss_l1 = loss_L1(outputs, torch.squeeze(labels))\n",
        "        losses_mse.append(loss_mse.cpu().data.numpy())\n",
        "        losses_l1.append(loss_l1.cpu().data.numpy())\n",
        "\n",
        "        tested_batch += 1\n",
        "\n",
        "        if tested_batch % 1000 == 0:\n",
        "            cur_time = time.time()\n",
        "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format( \\\n",
        "                  tested_batch * batch_size, \\\n",
        "                  np.around([loss_l1.data[0]], decimals=8), \\\n",
        "                  np.around([loss_mse.data[0]], decimals=8), \\\n",
        "                  np.around([cur_time - pre_time], decimals=2) ) )\n",
        "            pre_time = cur_time\n",
        "    a1=np.concatenate(predict_array).ravel().tolist()\n",
        "    a2=np.concatenate(actual_array).ravel().tolist()\n",
        "    df = pd.DataFrame(list(zip(a2, a1)),\n",
        "               columns =['y_last', 'yhat_last'])\n",
        "    key=str(\"Theft_shift_experiment:\"+\"-\"+str(i)+str(j)+\"LSTM\"+\".csv\")\n",
        "\n",
        "    df.to_csv('/content/gdrive/MyDrive/Theft_Daily_experiment//%s'%(key),index=False)\n",
        "    losses_l1 = np.array(losses_l1)\n",
        "    losses_mse = np.array(losses_mse)\n",
        "    mean_l1 = np.mean(losses_l1) * max_speed\n",
        "    std_l1 = np.std(losses_l1) * max_speed\n",
        "\n",
        "    print('Tested: L1_mean: {}, L1_std : {}'.format(mean_l1, std_l1))\n",
        "    return [losses_l1, losses_mse, mean_l1, std_l1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "T453eRpwdp23"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdY1Ym5I2PN5"
      },
      "source": [
        "for i in range(0,10):\n",
        "  TheftTable_Smoothed=TheftTable.ewm(alpha=0.5).mean()\n",
        "  train_dataloader, valid_dataloader, test_dataloader, max_speed = PrepareDataset(TheftTable_Smoothed,3)\n",
        "  inputs, labels = next(iter(train_dataloader))\n",
        "  [batch_size, step_size, fea_size] = inputs.size()\n",
        "  input_dim = fea_size\n",
        "  hidden_dim = fea_size\n",
        "  output_dim = labels.shape[2]\n",
        "  device='cuda:0'\n",
        "  lstm = LSTM(input_dim, hidden_dim, output_dim, output_last = True)\n",
        "  lstm, lstm_loss = TrainModel(lstm, train_dataloader, valid_dataloader,0.001, num_epochs = 100)\n",
        "  lstm_test = TestModel(lstm, test_dataloader, max_speed )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoOqtwVi63ja"
      },
      "source": [
        "**PREPARING THE EXTERNAL DATASETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuVEhyim66Rm"
      },
      "source": [
        "import geopandas as gpd\n",
        "cl_gchi_core_sn=pd.read_pickle(\"/content/gdrive/MyDrive/MainFiles/snappedcorecrimes.pkl\")\n",
        "##Subsetting the crime dataset\n",
        "Robbery=['312','313','031A','031B','320','325','326','330','331','334','337','033A','033B','340']\n",
        "Theft=['810','820','850','860','865','870','880','890','895']\n",
        "core20162018=cl_gchi_core_sn.loc[(cl_gchi_core_sn['Time_ID'] > 2191) & (cl_gchi_core_sn['Time_ID'] <= 4383)]\n",
        "Robbery20162018=core20162018.loc[(core20162018.IUCR.isin(Robbery))]\n",
        "Theft20162018=core20162018.loc[(core20162018.IUCR.isin(Theft))]\n",
        "##Retrieving the segments\n",
        "gchi_segments=pd.read_pickle(\"/content/gdrive/MyDrive/MainFiles/segments.pkl\")\n",
        "gchi_intersections=pd.read_pickle(\"/content/gdrive/MyDrive/MainFiles/gintersections.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY7d0GOJzIG-"
      },
      "source": [
        "**PARK EVENTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJZmQpFZxfBP"
      },
      "source": [
        "ParkEvents=pd.read_csv('/content/gdrive/MyDrive/MainFiles/CenterSide/Park Events/event_transactions.csv')\n",
        "ParkEvents=ParkEvents.drop(columns=(['Time_ID']))\n",
        "a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "ParkEvents.index=a\n",
        "ParkEvents_Smoothed=ParkEvents.ewm(span=6).mean()\n",
        "##Aggregating at day level\n",
        "ParkEvents_daily = ParkEvents.resample('D').sum()\n",
        "ParkEvents_daily_Smoothed=ParkEvents_daily.ewm(span=2).mean()\n",
        "ParkEvents_daily.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING GRAPHS FOR COMMUNITY AREAS**"
      ],
      "metadata": {
        "id": "OoY-ytOqX_iV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN7kMoXnqkJN"
      },
      "source": [
        "def create_neighborhood_graph(Cno):\n",
        "    a=gchi_segments['Community_Area']==Cno\n",
        "    #segmentsCno=gchi_segments[gchi_segments.Segment_ID.isin(Cno)]\n",
        "    segmentsCno=gchi_segments[a]\n",
        "    edgesCno=segmentsCno[['i', 'j','Segment_ID','shape_len']]\n",
        "    edgesCno['i']=edgesCno['i'].astype('str')\n",
        "    edgesCno['j']=edgesCno['j'].astype('str')\n",
        "    cgraphCno=nx.Graph()\n",
        "    for i, elrow in edgesCno.iterrows():## iterrows help iterate over DataFrame rows as (index, Series) pairs\n",
        "        cgraphCno.add_edge(elrow[0], elrow[1], attr_dict=elrow[2:].to_dict())\n",
        "    ##Intersections\n",
        "    un1=np.unique(edgesCno['i'])\n",
        "    un2=np.unique(edgesCno['j'])\n",
        "    un3=np.unique(np.concatenate((un1,un2)))\n",
        "    nodesCno=gchi_intersections[gchi_intersections['num'].isin(un3)]\n",
        "    nodesCno['num']=nodesCno['num'].astype('str')\n",
        "    for i, nlrow in nodesCno.iterrows():\n",
        "        cgraphCno.nodes[nlrow['num']].update(nlrow[1:].to_dict())\n",
        "\n",
        "    ##CREATING THE SEGMENT GRAPH FOR THE CENTER SIDE\n",
        "    mnsegmentsCno=segmentsCno[['Segment_ID','shape_len','i','j','Community_Area']]\n",
        "    ##Converting Types\n",
        "    mnsegmentsCno['Community_Area']=mnsegmentsCno['Community_Area'].astype('Int64')\n",
        "    mnsegmentsCno['Segment_ID']=mnsegmentsCno['Segment_ID'].astype('str')\n",
        "    mnsegmentsCno['i']=mnsegmentsCno['i'].astype('str')\n",
        "    mnsegmentsCno['j']=mnsegmentsCno['j'].astype('str')\n",
        "    mnsegmentsCno['Label'] = mnsegmentsCno.apply(lambda row: str(str(row.i)+'-'+str(row.j)), axis = 1)\n",
        "    ##Adjacency matrix of maingraph\n",
        "\n",
        "    matrixxCno= nx.convert.to_dict_of_dicts(cgraphCno)\n",
        "    ##Find the network distance between neighbors\n",
        "    medge_dict={}\n",
        "    for u,v,data in cgraphCno.edges(data=True):\n",
        "       key= str(str(u)+'-'+str(v))\n",
        "       medge_dict.update({key:data})\n",
        "    ##Converting Segments into Nodes\n",
        "    import itertools\n",
        "    caCno= pd.DataFrame()\n",
        "    for key, value in matrixxCno.items():\n",
        "        keylist=value.keys()\n",
        "        combinations_object = itertools.combinations(keylist, 2)\n",
        "        combinations_list = list(combinations_object)\n",
        "        for i in combinations_list:\n",
        "            nkey1= str(key+'-'+i[0])\n",
        "            rnkey1=str(i[0]+'-'+key)\n",
        "            c1= mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey1) | (mnsegmentsCno['Label'] == rnkey1) ,'Segment_ID'].iloc[0]\n",
        "            d1= (mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey1) | (mnsegmentsCno['Label'] == rnkey1),'shape_len'].iloc[0])/2\n",
        "            nkey2= str(key+'-'+i[1])\n",
        "            rnkey2=rnkey1=str(i[1]+'-'+key)\n",
        "            c2= mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey2) | (mnsegmentsCno['Label'] == rnkey2),'Segment_ID'].iloc[0]\n",
        "            d2= (mnsegmentsCno.loc[(mnsegmentsCno['Label'] == nkey2) | (mnsegmentsCno['Label'] == rnkey2),'shape_len'].iloc[0])/2\n",
        "            e=(d1+d2)\n",
        "            caCno = caCno.append({'S1': c1, 'S2': c2, 'distance': e}, ignore_index=True)\n",
        "    ##Weigthing the edges using gaussian weighting\n",
        "    import math\n",
        "    caCno['weight'] = caCno.apply(lambda row: (math.exp(-row.distance**0.2)), axis = 1)\n",
        "\n",
        "    ##Creating Segment graph from the dataframe\n",
        "    graphcaCno=nx.from_pandas_edgelist(caCno,source='S1',target='S2',edge_attr='weight')\n",
        "\n",
        "    ##creating dict of dicts for the dataframe\n",
        "    k='8'\n",
        "    name= \"graph.pickle\"\n",
        "    ##Saving the graph for CA 8\n",
        "    with open('/content/gdrive/MyDrive/MainFiles/%s%s' % (k,name), 'wb') as handle:\n",
        "        pickle.dump(graphcaCno, handle, protocol=2)\n",
        "    return caCno,graphcaCno\n",
        "gchi_segments=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/segments.pkl')\n",
        "gchi_intersections=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/gintersections.pkl')\n",
        "graphdf8,graph8=create_neighborhood_graph(8)\n",
        "##Saving Graphdf8\n",
        "graphdf8.to_pickle('/content/gdrive/MyDrive/MainFiles/graphdf8.pickle')\n",
        "matrixca8= nx.convert.to_dict_of_dicts(graph8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wsp0c3RqvmJ"
      },
      "source": [
        "**CREATING TRANSACTION TABLE FOR COMMUNITY AREAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG89nOy6q36x"
      },
      "source": [
        "def crime_transactions(sdf,ca):\n",
        "    a=np.array(sdf['S1'],dtype=int)\n",
        "    b=np.array(sdf['S2'],dtype=int)\n",
        "    c=np.append(a,b)\n",
        "    sdf=np.unique(c)\n",
        "    nsdf= np.array2string(sdf)\n",
        "    #np.savetxt('C:/Users/Tugrul/Desktop/graph_sensor_ids.txt', sdf, delimiter=',',fmt='%i')\n",
        "    Columnnames= np.concatenate([['Time_ID'],sdf])\n",
        "    Indexvalues=np.arange(2192,4384,1)\n",
        "    a = np.zeros(shape=(2192,(len(sdf)+1)))\n",
        "    sdfTransactionTable = pd.DataFrame(a)\n",
        "    sdfTransactionTable.columns=Columnnames\n",
        "    sdfTransactionTable['Time_ID']=Indexvalues\n",
        "    sdfTransactionTable=sdfTransactionTable.astype(int)\n",
        "    #Subsetting Robbery and Theft\n",
        "    a=Robbery20162018['Community_Area']==ca\n",
        "    Robberyca=Robbery20162018[a]\n",
        "    a=Theft20162018['Community_Area']==ca\n",
        "    Theftca=Theft20162018[a]\n",
        "    #Robberyca=Robbery20162018[Robbery20162018.Segment_ID.isin(lst)]\n",
        "    #Theftca=Theft20162018[Theft20162018.Segment_ID.isin(lst)]\n",
        "    ## Creating Crosstabs for Selected Dataset\n",
        "    selectedcrimes20162018={'Robbery':Robberyca,'Theft':Theftca}\n",
        "    crimetimeseries={}\n",
        "    for k,v in selectedcrimes20162018.items():\n",
        "        df= pd.crosstab(v['Time_ID'],v['Segment_ID'],dropna=False)\n",
        "        crimetimeseries.update({k:df})\n",
        "    ##Inserting into the Robbery Transactional Table\n",
        "    RobberySerie=crimetimeseries['Robbery']\n",
        "    RobberySerie.dtypes\n",
        "    RobberySerie.columns = RobberySerie.columns.astype(str)\n",
        "    sdfTransactionTableR=sdfTransactionTable.copy()\n",
        "    sdfTransactionTableR.index=sdfTransactionTableR['Time_ID']\n",
        "    sdfTransactionTableR.dtypes\n",
        "    sdfTransactionTableR=sdfTransactionTableR.astype(str)\n",
        "    #type(list(sdfTransactionTableR.columns)[1])==list(sdfTransactionTableR)[1]\n",
        "    sdfTransactionTableR.columns = sdfTransactionTableR.columns.astype(str)\n",
        "    RobberySerie.index = RobberySerie.index.map(str)\n",
        "    sdfTransactionTableR.index = sdfTransactionTableR.index.map(str)\n",
        "    ##Inserting the transactions\n",
        "    for index, row in RobberySerie.iterrows():\n",
        "        for i in list(RobberySerie.columns):\n",
        "            sdfTransactionTableR.loc[index,i]=RobberySerie.loc[index,i]\n",
        "    #Aggregating Robbery at daily level\n",
        "    a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "    sdfTransactionTableR.index=a\n",
        "    sdfTransactionTableR=  sdfTransactionTableR.apply(pd.to_numeric)\n",
        "    sdfTransactionTableR_daily = sdfTransactionTableR.resample('D').sum()\n",
        "    ##Inserting into the Theft Transactional Table\n",
        "    TheftSerie=crimetimeseries['Theft']\n",
        "    TheftSerie.columns = TheftSerie.columns.astype(str)\n",
        "    sdfTransactionTableT=sdfTransactionTable.copy()\n",
        "    sdfTransactionTableT.index=sdfTransactionTableT['Time_ID']\n",
        "    sdfTransactionTableT.dtypes\n",
        "    sdfTransactionTableT=sdfTransactionTableT.astype(str)\n",
        "    sdfTransactionTableT.columns = sdfTransactionTableT.columns.astype(str)\n",
        "    TheftSerie.index = TheftSerie.index.map(str)\n",
        "    sdfTransactionTableT.index = sdfTransactionTableT.index.map(str)\n",
        "    ##Inserting the theft transactions\n",
        "    for index, row in TheftSerie.iterrows():\n",
        "        for i in list(TheftSerie.columns):\n",
        "            sdfTransactionTableT.loc[index,i]=TheftSerie.loc[index,i]\n",
        "    sdfTransactionTableT=sdfTransactionTableT.drop(columns= ['Time_ID'])\n",
        "    ##Aggregating at daily level\n",
        "    a= pd.date_range(start ='1-1-2016 00:00:00',end ='31-12-2017 08:00:00', freq ='8H')\n",
        "    sdfTransactionTableT.index=a\n",
        "    sdfTransactionTableT=  sdfTransactionTableT.apply(pd.to_numeric)\n",
        "    sdfTransactionTableT_daily = sdfTransactionTableT.resample('D').sum()\n",
        "    ##Smoothing the columns\n",
        "    sdfTransactionTableR_Smoothed=sdfTransactionTableR.ewm(alpha=0.1).mean()\n",
        "    sdfTransactionTableT_Smoothed=sdfTransactionTableT.ewm(alpha=0.1).mean()\n",
        "    #return sdfTransactionTableT_daily,sdfTransactionTableT\n",
        "    return sdfTransactionTableT,sdfTransactionTableT_daily\n",
        "#Theft_Transactions8_Daily,Theft_Transactions8_Shift=crime_transactions(graphdf8,8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PdbGvRgrA0Q"
      },
      "source": [
        "**RESTRUCTURING THE NEIGHBORHOOD TRANSACTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YlQ9t8nrJ9w"
      },
      "source": [
        "def neighborhood_transactions(path,sdf,matrixdf):\n",
        "    df= pd.read_csv(path)\n",
        "    df=df.rename(columns={\"y_last\": \"Actual\", \"yhat_last\": \"Predicted\"})\n",
        "    a=np.array(sdf['S1'],dtype=int)\n",
        "    b=np.array(sdf['S2'],dtype=int)\n",
        "    nodelist=np.unique(np.append(a,b))\n",
        "    rep_num = int((len(df)/len(nodelist)))\n",
        "    nodes=np.tile(nodelist,rep_num)\n",
        "    df['Segment']=nodes\n",
        "    dict1={}\n",
        "    for key,value in matrixdf.items():\n",
        "        nlist=[]\n",
        "        nlist.append(key)\n",
        "        for keys,values in value.items():\n",
        "            nlist.append(keys)\n",
        "        a=str(key)+'neighbor'\n",
        "        dict1[a]=nlist\n",
        "    dict2={}\n",
        "    df['Segment']=df['Segment'].astype(str)\n",
        "    for key,value in dict1.items():\n",
        "        a=key\n",
        "        dict2[a]=df[df.Segment.isin(value)]\n",
        "    dict3={}\n",
        "    ind_value=np.arange(1,(rep_num+1),1)\n",
        "    for key,value in dict2.items():\n",
        "        value['Segment'] = pd.to_numeric(value['Segment'])\n",
        "        a=list(sorted(np.unique(value.Segment)))\n",
        "        append_act = '_actual'\n",
        "        append_pre='_predicted'\n",
        "        b1 = [str(sub) + append_act for sub in a]\n",
        "        b2 = [str(sub) + append_pre for sub in a]\n",
        "        b3 = [None]*(len(b1)+len(b2))\n",
        "        b3[::2] = b1\n",
        "        b3[1::2] = b2\n",
        "        ndf=value[['Actual','Predicted']]\n",
        "        e=ndf.values.reshape(rep_num,-1)\n",
        "        nndf=pd.DataFrame(data=e,index=ind_value,columns=b3)\n",
        "        d=key\n",
        "        dict3[d]=nndf\n",
        "    return dict3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_HouaUrrNhJ"
      },
      "source": [
        "**CALCULATING THE MEAN HIT RATE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7HogSNkrTnb"
      },
      "source": [
        "from functools import partial, reduce\n",
        "from statistics import mean\n",
        "import math\n",
        "def calc_MHR(transactions_dict,transaction_df,segments_df):\n",
        "    ##transaction_dict is the output of neighborhood transaction,\n",
        "    ##transaction df is the output of crime_transactions function\n",
        "    ##segments_df is the output of create_neighborhood_graph function\n",
        "    my_reduce = partial(pd.merge, left_index=True, right_index=True)\n",
        "    a=reduce(my_reduce, transactions_dict.values())\n",
        "    d=a.filter(regex=(\"_predicted_x\"))\n",
        "    d.columns = d.columns.str.rstrip('_predicted_x')\n",
        "    d=d.iloc[:,~d.columns.duplicated()]\n",
        "    d.columns=d.columns.astype(int)\n",
        "    e = [int(numeric_string) for numeric_string in np.array(d.columns)]\n",
        "    f=sorted(e)\n",
        "    g=d[f]\n",
        "    actual_values=transaction_df.iloc[-(len(g)):,]\n",
        "    un1=np.unique(segments_df['S1'])\n",
        "    un2=np.unique(segments_df['S2'])\n",
        "    un3=np.unique(np.concatenate((un1,un2)))\n",
        "    gchi_segments['Segment_ID']=gchi_segments['Segment_ID'].astype(str)\n",
        "    un4= gchi_segments[gchi_segments.Segment_ID.isin(un3)]\n",
        "    slengths=un4[['Segment_ID','shape_len']]\n",
        "    pred_dict={}\n",
        "    for (index1, row1),(index2,row2) in zip(g.iterrows(),actual_values.iterrows()):\n",
        "        d=np.array(row1)\n",
        "        e=np.array(row2)\n",
        "        key=str(str(index1)+'.Day')\n",
        "        nname=str(str(index1)+'slengths')\n",
        "        nname=slengths.copy()\n",
        "        nname['prediction']=d\n",
        "        nname['actual']=e\n",
        "        pred_dict[index1]=nname\n",
        "    hrs=0\n",
        "    hr_dict={}\n",
        "    hr_list=[]\n",
        "    for key,value in pred_dict.items():\n",
        "        value['Risk_Rank'] = value['prediction'].rank(ascending = 0)\n",
        "        #value['Risk_Rank'] = pd.to_numeric(value['Risk_Rank'])\n",
        "        value = value.set_index('Risk_Rank')\n",
        "        value = value.sort_index()\n",
        "        pred_segs=[]\n",
        "        a=0\n",
        "        for index, row in value.iterrows():\n",
        "            a+=row.shape_len\n",
        "            if a>(0.20*value['shape_len'].sum()):\n",
        "                break\n",
        "            else:\n",
        "               pred_segs.append(row.Segment_ID)\n",
        "        selected_segments=value[value.Segment_ID.isin(pred_segs)]\n",
        "        #hitsegments=len(selected_segments[(selected_segments['actual']!=0)])\n",
        "        #allcrsegments=(len(value[(value['actual']!=0)]))\n",
        "        hitsegments=selected_segments['actual'].sum()\n",
        "        allcrsegments=value['actual'].sum()\n",
        "        hr=hitsegments/allcrsegments\n",
        "        hr_list.append(hr)\n",
        "        keys=str(str(key)+'.HR')\n",
        "        hr_dict[keys]=hr\n",
        "        hrs+=hr\n",
        "    nhrlist = [x for x in hr_list if math.isnan(x) == False]\n",
        "    mhr=mean(nhrlist)\n",
        "    return hr_dict,mhr,pred_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaba2Xvqrs5g"
      },
      "source": [
        "**WRITING THE RESULTS TO A DICTIONARY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh0arMcPrjyE"
      },
      "source": [
        "graphCS=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/CenterSide/Graphs/Center01graph.pickle')\n",
        "matrixCS= nx.convert.to_dict_of_dicts(graphCS)\n",
        "paths=(\"/content/gdrive/MyDrive/Robbery_Daily_experiment\")\n",
        "graphdfCS=pd.read_pickle('/content/gdrive/MyDrive/MainFiles/CenterSide/Graphs/graphCdf.pkl')\n",
        "def write_to_dict(path,TransactionDF,segmentsDF,matrix_dict):\n",
        "  import glob\n",
        "  all_files = glob.glob(paths + \"/*.csv\")\n",
        "  mhr_values={}\n",
        "  for m in all_files:\n",
        "      df1=neighborhood_transactions(m,graphdfCS,matrixCS)\n",
        "      a,b,c=calc_MHR(df1,RobberyTable_daily,graphdfCS)\n",
        "      key=str(\"mhr\"+m.split('/')[-1])\n",
        "      mhr_values[key]=b\n",
        "  import csv\n",
        "  a_file = open('/content/gdrive/MyDrive/MainFiles/Baselines/Robbery/STGCN/WithoutEX/Seed Tuning/Daily/Robbery_daily_best.csv', 'w',newline=\"\")\n",
        "  writer = csv.writer(a_file)\n",
        "  for key, value in mhr_values.items():\n",
        "      writer.writerow([key, value])\n",
        "  a_file.close()\n",
        "write_to_dict(paths,RobberyTable_daily,graphdfCS,matrixCS)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}